<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Chapter 1 Bayesian statistics &amp; MCMC | Bayesian Analysis of Capture-Recapture Data with Hidden Markov Models – Theory and Case Studies in R</title>
<meta name="author" content="Olivier Gimenez">
<meta name="description" content="1.1 Introduction In this first chapter, you will learn what the Bayesian theory is, and how you may use it with a simple example. You will also see how to implement simulation algorithms to...">
<meta name="generator" content="bookdown 0.23 with bs4_book()">
<meta property="og:title" content="Chapter 1 Bayesian statistics &amp; MCMC | Bayesian Analysis of Capture-Recapture Data with Hidden Markov Models – Theory and Case Studies in R">
<meta property="og:type" content="book">
<meta property="og:url" content="https://oliviergimenez.github.io/bayesian-cr-workshop/crashcourse.html">
<meta property="og:description" content="1.1 Introduction In this first chapter, you will learn what the Bayesian theory is, and how you may use it with a simple example. You will also see how to implement simulation algorithms to...">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Chapter 1 Bayesian statistics &amp; MCMC | Bayesian Analysis of Capture-Recapture Data with Hidden Markov Models – Theory and Case Studies in R">
<meta name="twitter:description" content="1.1 Introduction In this first chapter, you will learn what the Bayesian theory is, and how you may use it with a simple example. You will also see how to implement simulation algorithms to...">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/header-attrs-2.10/header-attrs.js"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.2.5.1/tabs.js"></script><script src="libs/bs3compat-0.2.5.1/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- CSS --><link rel="stylesheet" href="bs4_style.css">
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">Bayesian Analysis of Capture-Recapture Data with Hidden Markov Models – Theory and Case Studies in R</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html">Welcome</a></li>
<li><a class="" href="preface.html">Preface</a></li>
<li><a class="" href="about-the-author.html">About the Author</a></li>
<li class="book-part">Theory</li>
<li><a class="active" href="crashcourse.html"><span class="header-section-number">1</span> Bayesian statistics &amp; MCMC</a></li>
</ul>

        <div class="book-extra">
          <p><a id="book-repo" href="https://github.com/oliviergimenez/banana-book">View book source <i class="fab fa-github"></i></a></p>
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="crashcourse" class="section level1" number="1">
<h1>
<span class="header-section-number">1</span> Bayesian statistics &amp; MCMC<a class="anchor" aria-label="anchor" href="#crashcourse"><i class="fas fa-link"></i></a>
</h1>
<div id="introduction" class="section level2" number="1.1">
<h2>
<span class="header-section-number">1.1</span> Introduction<a class="anchor" aria-label="anchor" href="#introduction"><i class="fas fa-link"></i></a>
</h2>
<p>In this first chapter, you will learn what the Bayesian theory is, and how you may use it with a simple example. You will also see how to implement simulation algorithms to implement the Bayesian method for more complex analyses. This is not an exhaustive treatment of Bayesian statistics, but you should get what you need to navigate through the rest of the book.</p>
</div>
<div id="bayes-theorem" class="section level2" number="1.2">
<h2>
<span class="header-section-number">1.2</span> Bayes’ theorem<a class="anchor" aria-label="anchor" href="#bayes-theorem"><i class="fas fa-link"></i></a>
</h2>
<p>Let’s not wait any longer and jump into it. Bayesian statistics relies on the Bayes’ theorem (or law, or rule, whatever you prefer) named after Reverend Thomas Bayes (Figure <a href="crashcourse.html#fig:revbayes">1.1</a>). This theorem was published in 1763 two years after Bayes’ death thanks to his friend’s efforts Richard Price, and was independently rediscovered by Pierre-Simon Laplace <span class="citation">(<a href="crashcourse.html#ref-mcgrayne2011" role="doc-biblioref">McGrayne 2011</a>)</span>.</p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:revbayes"></span>
<img src="images/amazing-thomas-bayes-illustration.jpg" alt="Cartoon of Thomas Bayes with Bayes' theorem in background. Source: [James Kulich](https://www.elmhurst.edu/blog/thomas-bayes/)" width="70%"><p class="caption">
Figure 1.1: Cartoon of Thomas Bayes with Bayes’ theorem in background. Source: <a href="https://www.elmhurst.edu/blog/thomas-bayes/">James Kulich</a>
</p>
</div>
<p>As we will see in a minute, Bayes’ theorem is all about conditional probabilities, which are somehow tricky to understand. Conditional probability of outcome or event A given event B, which we denote <span class="math inline">\(\Pr(A \mid B)\)</span>, is the probability that A occurs, revised by considering the additional information that event B has definitely occurred.<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content='&lt;p&gt;For example, a friend of yours rolls a fair dice and asks you the probability that the outcome was a six (event A). Your answer is 1/6 because each side of the dice is equally likely to come up. Now imagine that you’re told the number rolled was even (event B) before you answer your friend’s question. Because there are only three even numbers, one of which is six, you may revise your answer for the probability that a six was rolled from 1/6 to &lt;span class="math inline"&gt;\(\Pr(A \mid B) = 1/3\)&lt;/span&gt;.&lt;/p&gt;'><sup>1</sup></a> The order in which A and B appear is important, make sure you do not confuse <span class="math inline">\(\Pr(A \mid B)\)</span> and <span class="math inline">\(\Pr(B \mid A)\)</span>.</p>
<p>Bayes’ theorem (Figure <a href="crashcourse.html#fig:bayestheorem">1.2</a>) gives you <span class="math inline">\(\Pr(A \mid B)\)</span> using marginal probabilities <span class="math inline">\(\Pr(A)\)</span> and <span class="math inline">\(\Pr(B)\)</span> and <span class="math inline">\(\Pr(B \mid A)\)</span>:
<span class="math display">\[\Pr(A \mid B) = \displaystyle{\frac{ \Pr(B \mid A) \; \Pr(A)}{\Pr(B)}}.\]</span>
Originally, Bayes theorem was seen as a mean to infer an unkown cause A of a particular effect B, knowing the probability of effect B given cause A. Think for example of a situation where a medical diagnosis is needed, with A an unkown disease and B symptoms, the doctor knows P(symptoms|disease) and wants to derive P(disease|symptoms). This way of reversing <span class="math inline">\(\Pr(B \mid A)\)</span> into <span class="math inline">\(\Pr(A \mid B)\)</span> explains why Bayesian thinking used to be referred to as ‘inverse probability.’</p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:bayestheorem"></span>
<img src="images/bayes_neon.jpeg" alt="Bayes' theorem spelt out in blue neon at the offices of Autonomy in Cambridge. Source: [Wikipedia](https://en.wikipedia.org/wiki/Bayes%27_theorem)" width="400"><p class="caption">
Figure 1.2: Bayes’ theorem spelt out in blue neon at the offices of Autonomy in Cambridge. Source: <a href="https://en.wikipedia.org/wiki/Bayes%27_theorem">Wikipedia</a>
</p>
</div>
<p>I don’t know about you, but I need to think twice for not messing the letters around. I find it easier to remember Bayes’ theorem written like this<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content='&lt;p&gt;When teaching Bayes’ theorem, I am very much inspired by Tristan Mahr’s slides from his introduction to Bayesian regression &lt;a href="https://www.tjmahr.com/bayes-intro-lecture-slides-2017/" class="uri"&gt;https://www.tjmahr.com/bayes-intro-lecture-slides-2017/&lt;/a&gt;&lt;/p&gt;'><sup>2</sup></a>:</p>
<p><span class="math display">\[ \Pr(\text{hypothesis} \mid \text{data}) = \frac{ \Pr(\text{data} \mid \text{hypothesis}) \; \Pr(\text{hypothesis})}{\Pr(\text{data})} \]</span>
The <em>hypothesis</em> is a working assumption about which you want to learn using <em>data</em>. In capture-recapture analyses, the hypothesis might be a parameter like detection probability, or regression parameters in a relationship between survival probability and a covariate. Bayes’ theorem tells us how to obtain the probability of a hypothesis given the data we have. This is great because think about it, this is exactly what the scientific method is! We’d like to know how plausible is some hypothesis based on some data we collected, and possibly compare several hypotheses among them. In that respect, the Bayesian reasoning matches the scientific reasoning, which probably explains why we feel like the Bayesian framework is so intuitive for doing and understanding statistics.</p>
<p>You might ask then, why is Bayesian statistics not the default in statistics? Clearly, because of futile wars between male statisticians (including Ronald Fisher, Jerzy Neyman and Egon Sharpe Pearson among others), little progress was made for over two centuries. Also, until recently, there were practical problems to implement the Bayesian approach. Recent advances in computational power coupled with the development of new algorithms have led to a great increase in the application of Bayesian methods within the last three decades.</p>
</div>
<div id="what-is-the-bayesian-approach" class="section level2" number="1.3">
<h2>
<span class="header-section-number">1.3</span> What is the Bayesian approach?<a class="anchor" aria-label="anchor" href="#what-is-the-bayesian-approach"><i class="fas fa-link"></i></a>
</h2>
<p>Typical statistical problems involve estimating parameter(s) <span class="math inline">\(\theta\)</span> with available data. To do so, you might be more used to the frequentist rather than the Bayesian method. The frequentist approach, and in particular maximum likelihood estimation (MLE), assumes that the parameters are fixed, and have unknown values to be estimated. Therefore classical estimates are generally point estimates of the parameters of interest. In contrast, the Bayesian approach assumes that the parameters are not fixed but have some fixed unknown distribution<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;A probability distribution is a mathematical expression that gives the probability for a random variable to take particular values. A probability distribution may be either discrete (e.g., the Bernoulli, Binomial or Poisson distribution) or continuous (e.g., the Gaussian distribution also known as the normal distribution)&lt;/p&gt;"><sup>3</sup></a> – a distribution for the parameter.</p>
<p>The Bayesian approach is based upon the idea that you, the experimenter, begin with some prior beliefs about the system. Then you collect data and update your prior beliefs on the basis of observations. These observations might arise from field work or lab work. This updating process is based upon the Bayes’ theorem which we’ve seen earlier. Loosely, let’s say <span class="math inline">\(A = \theta\)</span> and <span class="math inline">\(B = \text{data}\)</span>, then the Bayes’ theorem gives you a way to estimate parameter <span class="math inline">\(\theta\)</span> given the data you have. Bayes’ theorem becomes:</p>
<p><span class="math display">\[{\color{red}{\Pr(\theta \mid \text{data})}} = \frac{\color{blue}{\Pr(\text{data} \mid \theta)} \times \color{green}{\Pr(\theta)}}{\color{orange}{\Pr(\text{data})}}.\]</span>
Let’s spend some time going through each quantity in this formula.</p>
<p>On the left-hand side is the <span class="math inline">\(\color{red}{\text{posterior distribution}}\)</span>. It represents what you know after having seen the data. This is the basis for inference and clearly what you’re after, a distribution, possibly multivariate if you have more than one parameter.</p>
<p>On the right-hand side, there is the <span class="math inline">\(\color{blue}{\text{likelihood}}\)</span>. This quantity is the same as in the MLE approach. It captures the information you have in your data, given a model parameterized with <span class="math inline">\(\theta\)</span>.</p>
<p>Then we have the <span class="math inline">\(\color{green}{\text{prior distribution}}\)</span>. This quantity represents what you know before seeing the data. This is the source of much discussion about the Bayesian approach. It may be vague if you don’t know anything about <span class="math inline">\(\theta\)</span>. Usually however, you never start from scratch, and you’d like your prior to be informed by information you have. We’ll get back to it at length later on.</p>
<p>Last, we have <span class="math inline">\(\color{orange}{\Pr(\text{data})}\)</span> which is sometimes called the average likelihood. The likelihood is averaged over the prior <span class="math inline">\(\color{orange}{\Pr(\text{data}) = \int{L(\text{data} \mid \theta)\Pr(\theta) d\theta}}\)</span> so that the posterior is standardized, that is it integrates or sums up to one for the posterior to be a distribution. The average likelihood is a <span class="math inline">\(N\)</span>-dimensional integral if you have <span class="math inline">\(N\)</span> parameters to estimate <span class="math inline">\(\theta = \theta_1, \ldots, \theta_N\)</span>. This quantity is difficult, if not impossible, to calculate. This is one of the reasons why the Bayesian method wasn’t used until recently, and why we need algorithms to estimate posterior distributions as I illustrate in the next section.</p>
</div>
<div id="numerical-approx" class="section level2" number="1.4">
<h2>
<span class="header-section-number">1.4</span> Approximating posterior distributions via numerical integration<a class="anchor" aria-label="anchor" href="#numerical-approx"><i class="fas fa-link"></i></a>
</h2>
<p>Let’s take an example. Say we capture, mark and release <span class="math inline">\(n = 57\)</span> animals at the beginning of a winter, out of which we recapture <span class="math inline">\(y = 19\)</span> animals alive<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content='&lt;p&gt;We used a similar example in &lt;span class="citation"&gt;&lt;a href="crashcourse.html#ref-king_bayesian_2009" role="doc-biblioref"&gt;King et al.&lt;/a&gt; (&lt;a href="crashcourse.html#ref-king_bayesian_2009" role="doc-biblioref"&gt;2009&lt;/a&gt;)&lt;/span&gt;&lt;/p&gt;'><sup>4</sup></a>. We’d like to estimate winter survival <span class="math inline">\(\theta\)</span>.</p>
<div class="sourceCode" id="cb2"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">y</span> <span class="op">&lt;-</span> <span class="fl">19</span> <span class="co"># nb of success</span>
<span class="va">n</span> <span class="op">&lt;-</span> <span class="fl">57</span> <span class="co"># nb of attempts</span></code></pre></div>
<p>We build our model first. Assuming all animals are independent of each other and have the same survival probability, then the number of alive animals at the end of the winter is a binomial distribution<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content='&lt;p&gt;I follow &lt;span class="citation"&gt;&lt;a href="crashcourse.html#ref-mcelreathbook" role="doc-biblioref"&gt;McElreath&lt;/a&gt; (&lt;a href="crashcourse.html#ref-mcelreathbook" role="doc-biblioref"&gt;2016&lt;/a&gt;)&lt;/span&gt; and use labels on the right to help remember what each line is about.&lt;/p&gt;'><sup>5</sup></a>:</p>
<p><span class="math display">\[\begin{align*}
y &amp;\sim \text{Binomial}(n, \theta) &amp;\text{[likelihood]}
\end{align*}\]</span></p>
<p>In the Bayesian approach, priors are part of the model. For a parameter that is a probability, the one thing we know is that the prior should be a continuous random variable that lies between 0 and 1. To reflect that, we often go for the uniform distribution <span class="math inline">\(U(0,1)\)</span> to imply <em>vague</em> priors. Here vague means that survival has, before we see the data, the same probability of falling between 0.1 and 0.2 and between 0.8 and 0.9, for example.</p>
<p><span class="math display">\[\begin{align*}
\theta &amp;\sim \text{Uniform}(0, 1) &amp;\text{[prior for }\theta \text{]}
\end{align*}\]</span></p>
<p>Now we apply the Bayes’ theorem. We write a <code>R</code> function that computes the product of the likelihood times the prior, or the numerator in the formula of the Bayes’ theorem: <span class="math inline">\(\Pr(\text{data} \mid \theta) \times \Pr(\theta)\)</span></p>
<div class="sourceCode" id="cb3"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">numerator</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">theta</span><span class="op">)</span> <span class="fu"><a href="https://rdrr.io/r/stats/Binomial.html">dbinom</a></span><span class="op">(</span><span class="va">y</span>, <span class="va">n</span>, <span class="va">theta</span><span class="op">)</span> <span class="op">*</span> <span class="fu"><a href="https://rdrr.io/r/stats/Uniform.html">dunif</a></span><span class="op">(</span><span class="va">theta</span>, <span class="fl">0</span>, <span class="fl">1</span><span class="op">)</span></code></pre></div>
<p>We write another function that calculates the denominator, the average likelihood: <span class="math inline">\(\Pr(\text{data}) = \int{L(\theta \mid \text{data}) \Pr(\theta) d\theta}\)</span></p>
<div class="sourceCode" id="cb4"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">denominator</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/integrate.html">integrate</a></span><span class="op">(</span><span class="va">numerator</span>,<span class="fl">0</span>,<span class="fl">1</span><span class="op">)</span><span class="op">$</span><span class="va">value</span></code></pre></div>
<p>Then we get the posterior via numerical integration as in Figure <a href="crashcourse.html#fig:numapprox">1.3</a>.</p>
<div class="sourceCode" id="cb5"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">grid</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/seq.html">seq</a></span><span class="op">(</span><span class="fl">0</span>, <span class="fl">1</span>, <span class="fl">0.01</span><span class="op">)</span> <span class="co"># a grid of values for theta</span>
<span class="va">numerical_posterior</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/data.frame.html">data.frame</a></span><span class="op">(</span>survival <span class="op">=</span> <span class="va">grid</span>, 
                                  posterior <span class="op">=</span> <span class="fu">numerator</span><span class="op">(</span><span class="va">grid</span><span class="op">)</span><span class="op">/</span><span class="va">denominator</span><span class="op">)</span> 
<span class="va">numerical_posterior</span> <span class="op">%&gt;%</span>
  <span class="fu">ggplot</span><span class="op">(</span><span class="op">)</span> <span class="op">+</span> 
  <span class="fu">geom_line</span><span class="op">(</span><span class="fu">aes</span><span class="op">(</span>x <span class="op">=</span> <span class="va">survival</span>, y <span class="op">=</span> <span class="va">posterior</span><span class="op">)</span>, 
            size <span class="op">=</span> <span class="fl">1.5</span><span class="op">)</span></code></pre></div>
<div class="figure">
<span style="display:block;" id="fig:numapprox"></span>
<img src="banana-book_files/figure-html/numapprox-1.png" alt="Numerical approximation of winter survival posterior distribution." width="672"><p class="caption">
Figure 1.3: Numerical approximation of winter survival posterior distribution.
</p>
</div>
<p>How good is our numerical approximation of the survival posterior distribution? Ideally, we would want to compare the approximation to the true posterior distribution. Although a closed-form expression for the posterior distribution is in general intractable, when you combine a binomial likelihood together with a beta distribution as a prior, then the posterior distribution is also a beta distribution, which makes it amenable to all sorts of exact calculations<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;We say that the beta distribution is the conjugate prior distribution for the binomial distribution.&lt;/p&gt;"><sup>6</sup></a>. The beta distribution is continuous between 0 and 1, and extends the uniform distribution to situations where not all outcomes are equally likely. It has two parameters <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span> that control its shape (Figure <a href="crashcourse.html#fig:betadistribution">1.4</a>).</p>

<div class="figure">
<span style="display:block;" id="fig:betadistribution"></span>
<img src="banana-book_files/figure-html/betadistribution-1.png" alt="The distribution beta(\(\alpha\),\(\beta\)) for different values of \(\alpha\) and \(\beta\). Note that for \(\alpha = \beta = 1\), we get the uniform distribution between 0 and 1, see top left panel." width="672"><p class="caption">
Figure 1.4: The distribution beta(<span class="math inline">\(\alpha\)</span>,<span class="math inline">\(\beta\)</span>) for different values of <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span>. Note that for <span class="math inline">\(\alpha = \beta = 1\)</span>, we get the uniform distribution between 0 and 1, see top left panel.
</p>
</div>
It can be shown that if the likelihood of the data <span class="math inline">\(y\)</span> is binomial <span class="math inline">\(B(n,p)\)</span>, and the prior is a beta distribution with parameters <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span>, then the posterior is a beta distribution with parameters <span class="math inline">\(\alpha + y\)</span> and <span class="math inline">\(\beta + n - y\)</span>. In our example, survival has a beta posterior distribution with parameters 20 and 39. In Figure <a href="crashcourse.html#fig:compar">1.5</a>, we superimpose the exact posterior and its numerical approximation. Clearly, the two distributions are indistinguishable, suggesting that the numerical approximation is more than fine.
<div class="figure">
<span style="display:block;" id="fig:compar"></span>
<img src="banana-book_files/figure-html/compar-1.png" alt="Comparison of exact (dashed line) vs. numerical approximation (continuous line) of winter survival posterior distribution." width="672"><p class="caption">
Figure 1.5: Comparison of exact (dashed line) vs. numerical approximation (continuous line) of winter survival posterior distribution.
</p>
</div>
<!-- To finish up, let's add the prior.  -->
<!-- ```{r, echo = FALSE} -->
<!-- ggplot() +  -->
<!--   geom_line(data = numerical_posterior,  -->
<!--             aes(x = survival, y = posterior),  -->
<!--             size = 1.5,  -->
<!--             col = wesanderson::wes_palettes$Royal1[2],  -->
<!--             alpha = 0.5) +  -->
<!--   geom_line(data = dfexpposterior,  -->
<!--             aes(x = survival, y = explicit_posterior), -->
<!--             col = wesanderson::wes_palettes$Royal1[3],  -->
<!--             size = 1.5,  -->
<!--             linetype = "dashed") +  -->
<!--   geom_line(data = dfprior, -->
<!--             aes(x = survival, y = prior), -->
<!--             col = wesanderson::wes_palettes$Royal1[1], -->
<!--             size = 1.5) -->
<!-- ``` -->
<p>The posterior distribution contains all the information we need about the parameter to be estimated. When dealing with many parameters however, you may want to summarise posterior results by calculating numerical summaries. The simplest numerical summary is the mean or the median of the posterior distribution. These can simply be calculated in <code>R</code>:</p>
<div class="sourceCode" id="cb6"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">sample_from_posterior</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Beta.html">rbeta</a></span><span class="op">(</span>n <span class="op">=</span> <span class="fl">1000</span>, <span class="fl">1</span> <span class="op">+</span> <span class="fl">19</span>, <span class="fl">1</span> <span class="op">+</span> <span class="fl">57</span> <span class="op">-</span> <span class="fl">19</span><span class="op">)</span> <span class="co"># draw 1000 values from posterior distribution of survival beta(20,39)</span>
<span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">sample_from_posterior</span><span class="op">)</span> <span class="co"># compute mean</span>
<span class="co">## [1] 0.3368</span>
<span class="fu"><a href="https://rdrr.io/r/stats/median.html">median</a></span><span class="op">(</span><span class="va">sample_from_posterior</span><span class="op">)</span> <span class="co"># compute median</span>
<span class="co">## [1] 0.3315</span></code></pre></div>
<p>Because the posterior distribution is rather symetric, mean and median are very similar but this is not necessarily the case. We may also check that the mean we calculate empirically matches the expectation of a beta distribution<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content='&lt;p&gt;&lt;span class="math inline"&gt;\(E(\text{beta}(\alpha, \beta)) = \displaystyle{\frac{\alpha}{\alpha + \beta}}\)&lt;/span&gt;&lt;/p&gt;'><sup>7</sup></a>:</p>
<div class="sourceCode" id="cb7"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">sample_from_posterior</span><span class="op">)</span> <span class="co"># compute mean</span>
<span class="co">## [1] 0.3368</span>
<span class="fl">20</span><span class="op">/</span><span class="op">(</span><span class="fl">20</span><span class="op">+</span><span class="fl">39</span><span class="op">)</span>
<span class="co">## [1] 0.339</span></code></pre></div>
<p>Another useful numerical summary is the credible interval within which our parameter falls with some probability, usually 0.95 hence a 95<span class="math inline">\(\%\)</span> credible interval. This can be obtained in <code>R</code> with:</p>
<div class="sourceCode" id="cb8"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://rdrr.io/r/stats/quantile.html">quantile</a></span><span class="op">(</span><span class="va">sample_from_posterior</span>, probs <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">2.5</span><span class="op">/</span><span class="fl">100</span>, <span class="fl">97.5</span><span class="op">/</span><span class="fl">100</span><span class="op">)</span><span class="op">)</span>
<span class="co">##   2.5%  97.5% </span>
<span class="co">## 0.2251 0.4727</span></code></pre></div>
<p>In our example, we have a single parameter to estimate, winter survival. This means dealing with a one-dimensional integral in the denominator which is pretty easy with a quadrature scheme and the <code>R</code> function <code><a href="https://rdrr.io/r/stats/integrate.html">integrate()</a></code>. Now what if we had multiple parameters? For example, let’s imagine you’d like to fit a capture-recapture model with detection probability <span class="math inline">\(p\)</span> and regression parameters <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span> for the intercept and slope of a relationship between survival probability and a covariate, then Bayes’ theorem gives you the posterior distribution of all three parameters together:</p>
<p><span class="math display">\[ P(\alpha, \beta, p \mid \text{data}) = \frac{ P(\text{data} \mid \alpha, \beta, p) \times P(\alpha, \beta, p)}{\iiint \, P(\text{data} \mid \alpha, \beta, p) P(\alpha, \beta, p) d\alpha d\beta dp} \]</span>
There are two computational challenges with this formula. First, do we really wish to calculate a three-dimensional integral? The answer is no, one-dimensional and two-dimensional integrals are so much further we can go with standard methods. Second, we’re more interested in a posterior distribution for each parameter separately than the joint posterior distribution. The so-called marginal distribution of <span class="math inline">\(p\)</span> for example is obtained by integrating over all the other parameters – a two-dimensional integral in this example. Now imagine with tens or hundreds of parameters to estimate, these integrals become highly multi-dimensional and simply intractable. In the next section, I introduce powerful simulation methods to circumvent this issue.</p>
</div>
<div id="bayesian-computation-with-markov-chain-monte-carlo-mcmc" class="section level2" number="1.5">
<h2>
<span class="header-section-number">1.5</span> Bayesian computation with Markov chain Monte Carlo (MCMC)<a class="anchor" aria-label="anchor" href="#bayesian-computation-with-markov-chain-monte-carlo-mcmc"><i class="fas fa-link"></i></a>
</h2>
<p>In the early 1990s, statisticians rediscovered work from the 1950’s in physics. In a famous paper that would lay the fundations of modern Bayesian statistics (see Figure <a href="crashcourse.html#fig:mcmcpaper">1.6</a>), the authors use simulations to approximate posterior distributions with some precision by drawing large samples. This is a neat trick to avoid explicit calculation of the multi-dimensional integrals we struggle with when using Bayes’ theorem. These simulation algorithms are called Markov chain Monte Carlo (MCMC), and they definitely gave a boost to Bayesian statistics. There are two parts in MCMC, Markov chain and Monte Carlo. Let’s try and make sense of these terms.</p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:mcmcpaper"></span>
<img src="images/metropolis.png" alt="MCMC article cover. Source: [The Journal of Chemical Physics](https://aip.scitation.org/doi/10.1063/1.1699114)" width="582"><p class="caption">
Figure 1.6: MCMC article cover. Source: <a href="https://aip.scitation.org/doi/10.1063/1.1699114">The Journal of Chemical Physics</a>
</p>
</div>
<p>Now what does Monte Carlo stand for? We have actually already met with this method in Section <a href="crashcourse.html#numerical-approx">1.4</a> when we calculated numerical summaries of the survival posterior distribution. Monte Carlo integration is a simulation technique to estimate integrals involving any function of the parameter <span class="math inline">\(\theta\)</span>, in which you draw samples from the posterior distribution, then calculate the sample mean of the values to which we applied the function. <strong>prendre la SD comme exemple?</strong></p>
<p>Now what is a Markov chain? A Markov chain is a random sequence of numbers, in which each number depends only on the previous number. An example for a Markov chain is the weather in my home town in Southern France, in which a sunny day is most likely to be followed by another sunny day. Under certain conditions<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;The Markov chain is irreducible and aperiodic.&lt;/p&gt;"><sup>8</sup></a>, a Markov chain will converge to a unique stationary distribution. The cool thing is that you can build Markov chains with a given stationary distribution set to be the desired posterior distribution. <strong>donner un exemple de Markov chain et stationary distribution</strong></p>
<p>MCMC therefore allows us to construct a sequence of values whose distribution
converges to the posterior distribution of interest (if the chain is run
for long enough). Once the chain has converged to the stationary distribution,
we can then use the sequence of values taken by the chain in order to obtain
empirical (Monte Carlo) estimates of any posterior summaries of interest, such
as posterior means. Note that we need to ensure that the Markov chain has
reached the stationary distribution before we can use the realisations to obtain
our Monte Carlo estimates of the posterior distributions of interest. This
means that we need to discard realisations from the start of the chain and
only use observations once the chain has converged. This initial period of the
chain is referred to as the burn-in. We return to the issue of determining the
length of the burn-in in Section 5.4.2, once we have described the general
MCMC methods. We emphasise that the beauty of MCMC is that the updating
procedure remains relatively simple, no matter how complex the posterior
distribution of interest. Thus we can do the required integration by sampling
from the posterior distribution, and we can sample from the posterior by generating
a Markov chain. We construct a Markov chain using an appropriate
updating scheme, but how do we do these updates? There are several standard
approaches.</p>
<p>This summarizes the core spirit of MCMC algorithms.</p>
<p>There are several ways of constructing these chains: e.g., Metropolis-Hastings, Gibbs sampler. Have a look to <a href="https://github.com/chi-feng/mcmc-demo" class="uri">https://github.com/chi-feng/mcmc-demo</a> for an interactive gallery of MCMC algorithms. Here I will illustrate the Metropolis algorithm and how to implement it in practice. <strong>Différence entre Metropolis et MH</strong></p>
<p>Let’s go back to our example on animal survival estimation. We illustrate sampling from survival posterior distribution. We write functions for likelihood, prior and posterior.</p>
<div class="sourceCode" id="cb9"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co"># 19 animals recaptured alive out of 57 captured, marked and released</span>
<span class="va">survived</span> <span class="op">&lt;-</span> <span class="fl">19</span>
<span class="va">released</span> <span class="op">&lt;-</span> <span class="fl">57</span>

<span class="co"># binomial log-likelihood function</span>
<span class="va">loglikelihood</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">x</span>, <span class="va">p</span><span class="op">)</span><span class="op">{</span>
  <span class="fu"><a href="https://rdrr.io/r/stats/Binomial.html">dbinom</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">x</span>, size <span class="op">=</span> <span class="va">released</span>, prob <span class="op">=</span> <span class="va">p</span>, log <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span>
<span class="op">}</span>

<span class="co"># uniform prior density</span>
<span class="va">logprior</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">p</span><span class="op">)</span><span class="op">{</span>
  <span class="fu"><a href="https://rdrr.io/r/stats/Uniform.html">dunif</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">p</span>, min <span class="op">=</span> <span class="fl">0</span>, max <span class="op">=</span> <span class="fl">1</span>, log <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span>
<span class="op">}</span>

<span class="co"># posterior density function (log scale)</span>
<span class="va">posterior</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">x</span>, <span class="va">p</span><span class="op">)</span><span class="op">{</span>
  <span class="fu">loglikelihood</span><span class="op">(</span><span class="va">x</span>, <span class="va">p</span><span class="op">)</span> <span class="op">+</span> <span class="fu">logprior</span><span class="op">(</span><span class="va">p</span><span class="op">)</span> <span class="co"># - log(Pr(data))</span>
<span class="op">}</span></code></pre></div>
<p>The Metropolis algorithm works as follows: <strong>pour chaque étape, donner l’intuition</strong></p>
<ol style="list-style-type: decimal">
<li><p>We start at any possible value of the parameter to be estimated.</p></li>
<li><p>To decide where to visit next, we propose to move away from the current value of the parameter – this is a <em>candidate</em> value. To do so, we add to the current value some random value from (say) a normal distribution with some variance.</p></li>
<li><p>We compute the ratio of the probabilities at the candidate and current locations <span class="math inline">\(R = \text{posterior(candidate)/posterior(current)}\)</span>. This is where the magic of MCMC happens, in that <span class="math inline">\(\Pr(\text{data})\)</span>, the denominator of the Bayes’ theorem, cancels out and does not need to be calculated.</p></li>
<li><p>We spin a continuous spinner that lands anywhere from 0 to 1 – call it the random spin <span class="math inline">\(X\)</span>. If <span class="math inline">\(X\)</span> is smaller than <span class="math inline">\(R\)</span>, we move to the candidate location, otherwise we remain at the current location.</p></li>
<li><p>We repeat 2-4 a number of times – or <em>steps</em> (many steps).</p></li>
</ol>
<p>Enough of the theory, let’s implement the Metropolis algorithm in <code>R</code>. Let’s start by setting the scene.</p>
<div class="sourceCode" id="cb10"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">steps</span> <span class="op">&lt;-</span> <span class="fl">100</span> <span class="co"># number of steps</span>
<span class="va">theta.post</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/rep.html">rep</a></span><span class="op">(</span><span class="cn">NA</span>, <span class="va">steps</span><span class="op">)</span> <span class="co"># vector to store samples</span>
<span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">1234</span><span class="op">)</span> <span class="co"># for reproducibility</span></code></pre></div>
<p>Now we follow the 5 steps we’ve just described. First, we pick a starting value, and store it (step 1).</p>
<div class="sourceCode" id="cb11"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">inits</span> <span class="op">&lt;-</span> <span class="fl">0.5</span>
<span class="va">theta.post</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="va">inits</span></code></pre></div>
<p>To go to the next steps, we’ll need a function to propose a candidate value. <strong>expliquer le away, on s’en sert plus bas pour autocorrelation, called SD</strong></p>
<div class="sourceCode" id="cb12"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">move</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">x</span>, <span class="va">away</span> <span class="op">=</span> <span class="fl">.2</span><span class="op">)</span><span class="op">{</span> 
  <span class="va">logitx</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">x</span> <span class="op">/</span> <span class="op">(</span><span class="fl">1</span> <span class="op">-</span> <span class="va">x</span><span class="op">)</span><span class="op">)</span>
  <span class="va">logit_candidate</span> <span class="op">&lt;-</span> <span class="va">logitx</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="fl">1</span>, <span class="fl">0</span>, <span class="va">away</span><span class="op">)</span>
  <span class="va">candidate</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Logistic.html">plogis</a></span><span class="op">(</span><span class="va">logit_candidate</span><span class="op">)</span>
  <span class="kw"><a href="https://rdrr.io/r/base/function.html">return</a></span><span class="op">(</span><span class="va">candidate</span><span class="op">)</span>
<span class="op">}</span></code></pre></div>
<p>Now we’re ready for steps 2, 3 and 4. Actually, we will write a look to take care of step 5 as well. Remember we start at initial value 0.5 and run the algorithm for <span class="math inline">\(100\)</span> iterations.</p>
<div class="sourceCode" id="cb13"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="kw">for</span> <span class="op">(</span><span class="va">t</span> <span class="kw">in</span> <span class="fl">2</span><span class="op">:</span><span class="va">steps</span><span class="op">)</span><span class="op">{</span> <span class="co"># repeat steps 2-4 (step 5)</span>
  
  <span class="co"># propose candidate value for prob of success (step 2)</span>
  <span class="va">theta_star</span> <span class="op">&lt;-</span> <span class="fu">move</span><span class="op">(</span><span class="va">theta.post</span><span class="op">[</span><span class="va">t</span><span class="op">-</span><span class="fl">1</span><span class="op">]</span><span class="op">)</span>
  
  <span class="co"># calculate ratio R (step 3)</span>
  <span class="va">pstar</span> <span class="op">&lt;-</span> <span class="fu">posterior</span><span class="op">(</span><span class="va">survived</span>, p <span class="op">=</span> <span class="va">theta_star</span><span class="op">)</span>  
  <span class="va">pprev</span> <span class="op">&lt;-</span> <span class="fu">posterior</span><span class="op">(</span><span class="va">survived</span>, p <span class="op">=</span> <span class="va">theta.post</span><span class="op">[</span><span class="va">t</span><span class="op">-</span><span class="fl">1</span><span class="op">]</span><span class="op">)</span>
  <span class="va">logR</span> <span class="op">&lt;-</span> <span class="va">pstar</span> <span class="op">-</span> <span class="va">pprev</span>
  <span class="va">R</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html">exp</a></span><span class="op">(</span><span class="va">logR</span><span class="op">)</span>
  
  <span class="co"># accept candidate value or keep current value (step 4)</span>
  <span class="va">accept</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Binomial.html">rbinom</a></span><span class="op">(</span><span class="fl">1</span>, <span class="fl">1</span>, prob <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/Extremes.html">min</a></span><span class="op">(</span><span class="va">R</span>, <span class="fl">1</span><span class="op">)</span><span class="op">)</span>
  <span class="va">theta.post</span><span class="op">[</span><span class="va">t</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/ifelse.html">ifelse</a></span><span class="op">(</span><span class="va">accept</span> <span class="op">==</span> <span class="fl">1</span>, <span class="va">theta_star</span>, <span class="va">theta.post</span><span class="op">[</span><span class="va">t</span><span class="op">-</span><span class="fl">1</span><span class="op">]</span><span class="op">)</span>
<span class="op">}</span></code></pre></div>
<p>We get the following values.</p>
<div class="sourceCode" id="cb14"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://rdrr.io/r/utils/head.html">head</a></span><span class="op">(</span><span class="va">theta.post</span><span class="op">)</span> <span class="co"># first values</span>
<span class="co">## [1] 0.5000 0.4399 0.4399 0.4577 0.4577 0.4577</span>
<span class="fu"><a href="https://rdrr.io/r/utils/head.html">tail</a></span><span class="op">(</span><span class="va">theta.post</span><span class="op">)</span> <span class="co"># last values</span>
<span class="co">## [1] 0.4146 0.3772 0.3772 0.3861 0.3899 0.3624</span></code></pre></div>
Visually, you may look at the chain in Figure <a href="crashcourse.html#fig:chain">1.7</a>. <strong>introduire traceplot?</strong>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:chain"></span>
<img src="banana-book_files/figure-html/chain-1.png" alt="Visualisation of a Markov chain, often called traceplot. Starting value is 0.5." width="672"><p class="caption">
Figure 1.7: Visualisation of a Markov chain, often called traceplot. Starting value is 0.5.
</p>
</div>
Can we run another chain and start at initial value 0.2 this time? Yes, just go through the same algorithm again, and visualise the results in Figure <a href="crashcourse.html#fig:twochains">1.8</a>.
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:twochains"></span>
<img src="banana-book_files/figure-html/twochains-1.png" alt="Visualisation of two Markov chains with starting values 0.2 (yellow) and 0.5 (blue)." width="672"><p class="caption">
Figure 1.8: Visualisation of two Markov chains with starting values 0.2 (yellow) and 0.5 (blue).
</p>
</div>
Notice that we do not get the same exact results. <strong>Stochasticity, mais on average, seems to reach same value and vary aournd it – equilibrium? Stationary? posterior of winter survival</strong> Now let’s increase the number of steps and run a chain with 5000 iterations as in Figure <a href="crashcourse.html#fig:longchain">1.9</a>.
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:longchain"></span>
<img src="banana-book_files/figure-html/longchain-1.png" alt="Visualisation of a Markov chains with 5000 iterations." width="672"><p class="caption">
Figure 1.9: Visualisation of a Markov chains with 5000 iterations.
</p>
</div>
<p>We also add two straight lines, one in yellow for the mean of the posterior distribution <strong>dire comment c’est calculé</strong>, and the other in red for the maximum likelihood estimate <strong>c’est quoi ici?</strong>. <strong>expliquer pourquoi, et noter que ça donne la même chose</strong>.</p>
<p>I find it informative to look at the animated version of Figure <a href="crashcourse.html#fig:longchain">1.9</a>, it helps understanding the iterative behavior of the algorithm, and also to realise how the chains converge to their stationary distribution, see Figure <a href="crashcourse.html#fig:animlongchain">1.10</a>.</p>
<img src="images/traceplotMCMC.gif" width="100%" style="display: block; margin: auto;"><div class="figure" style="text-align: center">
<span style="display:block;" id="fig:animlongchain"></span>
<img src="images/histMCMC.gif" alt="Sampling values in survival posterior distribution with a MCMC algorithm. Top panel: traceplot. Bottom panel: histogram." width="100%"><p class="caption">
Figure 1.10: Sampling values in survival posterior distribution with a MCMC algorithm. Top panel: traceplot. Bottom panel: histogram.
</p>
</div>
<p><strong>Introduire l’idée de grass, note de bas de page vers Steve Brooks comm pers, et transition vers next section avec l’idée de comment on regarde la convergence</strong> We discard some realisations of the Markov chain before convergence is achieved. Once the stationary distribution is reached <strong>use stationary or target or limiting</strong>, we can regard the realisations of the chain as a (dependent) sample from the posterior distribution, and obtain Monte Carlo estimates of parameters. In the next section, we consider several important implementation issues. <strong>coder Metropolis d’au-dessus dans Nimble</strong></p>
</div>
<div id="assessing-convergence" class="section level2" number="1.6">
<h2>
<span class="header-section-number">1.6</span> Assessing convergence<a class="anchor" aria-label="anchor" href="#assessing-convergence"><i class="fas fa-link"></i></a>
</h2>
<p>When implementing MCMC, we need to determine how long it takes for our Markov chain to converge to the target distribution, and the number of iterations we need after achieving convergence to get reasonable Monte Carlo estimates of model parameters (numerical summaries). <strong>expliquer numerical summaries quelque part</strong></p>
<div id="burn-in" class="section level3" number="1.6.1">
<h3>
<span class="header-section-number">1.6.1</span> Burn-in<a class="anchor" aria-label="anchor" href="#burn-in"><i class="fas fa-link"></i></a>
</h3>
<p>In practice, we discard observations from the start of the Markov chain and just use observations from the chain once it has converged. The initial observations that we discard are usually referred to as the <strong>burn-in</strong>.</p>
<p>The simplest method to determine the length of the burn-in period is to look at trace plots. Going back to our example, we see from the trace plot in Figure <a href="crashcourse.html#fig:burnin">1.11</a> that we need at least 500 iterations to achieve convergence toward an average survival around 0.3. It is always better to be conservative when specifying the length of the burn-in period, and in this example, we would use 750 or even 1000 iterations as a burn-in. <strong>préciser qu’il faut faire qqs runs préliminaires pour déterminer le burn-in; ajouter des cas pathologiques ou faire des renvois aux sections suiantes, en particuier les minimal locaux ou par redundancy?</strong></p>
<div class="figure">
<span style="display:block;" id="fig:burnin"></span>
<img src="banana-book_files/figure-html/burnin-1.png" alt="Determining the length of the burn-in period." width="672"><p class="caption">
Figure 1.11: Determining the length of the burn-in period.
</p>
</div>
<p>Inspecting the trace plot for a single run of the Markov chain is useful. However, we usually run the Markov chain several times, starting from different over-dispersed points, to check that all replicates achieve the same target distribution. This approach is formalised by using the Brooks-Gelman-Rubin (BGR) statistic <span class="math inline">\(\hat{R}\)</span> which measures the ratio of the total variability combining multiple chains (between-chain plus within-chain) to the within-chain variability. The BGR statistic asks whether there is a chain effect, and is very much alike the <span class="math inline">\(F\)</span> test in an analysis of variance. Values below <span class="math inline">\(1.2\)</span> indicate likely convergence. <strong>check 1.1 or 1.2</strong></p>
<p>Back to our example, we run two replicates of the Markov chain with starting values 0.2 and 0.8 using 100 up to 5000 iterations, and calculate the BGR statistic using half the number of iterations as the length of the burn-in. From Figure <a href="crashcourse.html#fig:bgr">1.12</a>, we get a value of the BGR statistic near 1 by up to 2500 iterations, which suggests that with 2500 iterations as a burn-in, there is no evidence of a lack of convergence.</p>
<div class="figure">
<span style="display:block;" id="fig:bgr"></span>
<img src="banana-book_files/figure-html/bgr-1.png" alt="Brooks-Gelman-Rubin statistic." width="672"><p class="caption">
Figure 1.12: Brooks-Gelman-Rubin statistic.
</p>
</div>
<p>It is important to bear in mind that a value near 1 for the BGR statistic is only a necessary <em>but not sufficient</em> condition for convergence. In other words, this diagnostic cannot tell you for sure that the Markov chain has achieved convergence, only that it has not.</p>
</div>
<div id="chain-length" class="section level3" number="1.6.2">
<h3>
<span class="header-section-number">1.6.2</span> Chain length<a class="anchor" aria-label="anchor" href="#chain-length"><i class="fas fa-link"></i></a>
</h3>
<p>How long of a chain is needed to produce stable parameter estimates? To answer this question, you need to keep in mind that successive steps in a Markov chain are near each other, and are not independent – this is usually referred to as autocorrelation. Ideally, we would like to keep autocorrelation as low as possible. Here again, trace plots are useful to diagnose issues with autocorrelation. Let’s get back to our survival example. Figure (fig:tracechainlength) shows trace plots for samples in survival posterior distribution. Small and big moves provide relatively high correlations between successive observations of the Markov chain, whereas a standard deviation of 0.2 allows efficient exploration of the parameter space. The movement around the parameter space is often referred to as <strong>mixing</strong>. Mixing is bad when the chain makes small and big moves, and good otherwise.</p>
<div class="figure">
<span style="display:block;" id="fig:tracechainlength"></span>
<img src="banana-book_files/figure-html/tracechainlength-1.png" alt="Trace plots for different tuning of the acceptance rate. Left: SD is 0.02, the chain exhibits small moves and mixing is bad. Right: SD is 2, the chain exhibits big moves and mixing is bad. Middle: SD is 0.2, the chain exhibits adequate moves and mixing is good." width="672"><p class="caption">
Figure 1.13: Trace plots for different tuning of the acceptance rate. Left: SD is 0.02, the chain exhibits small moves and mixing is bad. Right: SD is 2, the chain exhibits big moves and mixing is bad. Middle: SD is 0.2, the chain exhibits adequate moves and mixing is good.
</p>
</div>
<p>Besides trace plots, autocorrelation function (ACF) plots are a convenient way of displaying the strength of autocorrelation in a given sample values. ACF plots provide the autocorrelation between successively sampled values separated by <span class="math inline">\(k\)</span> iterations, referred to as lag, (i.e. <span class="math inline">\(\text{cor}(\theta_t, \theta_{t+k})\)</span>) for increasing values of <span class="math inline">\(k\)</span>. Figure (fig:acfchainlength)</p>
<div class="figure">
<span style="display:block;" id="fig:acfchainlength"></span>
<img src="banana-book_files/figure-html/acfchainlength-1.png" alt="Autocorrelation function plots for different tuning of the acceptance rate. Left: SD is 0.02, autocorrelation is strong, decreases slowly with increasing lag and mixing is bad. Right: SD is 2, autocorrelation is strong, decreases slowly with increasing lag and mixing is bad. Middle: SD is 0.2, autocorrelation is weak, decreases rapidly with increasing lag and mixing is good." width="672"><p class="caption">
Figure 1.14: Autocorrelation function plots for different tuning of the acceptance rate. Left: SD is 0.02, autocorrelation is strong, decreases slowly with increasing lag and mixing is bad. Right: SD is 2, autocorrelation is strong, decreases slowly with increasing lag and mixing is bad. Middle: SD is 0.2, autocorrelation is weak, decreases rapidly with increasing lag and mixing is good.
</p>
</div>
<p>Autocorrelation is not necessarily a big issue. Strongly correlated observations just require large sample sizes and therefore longer simulations. But how many iterations exactly? The effective sample size (<code>n.eff</code>) measures chain length while taking into account chain autocorrelation. Obviously, <code>n.eff</code> is less than the number of MCMC iterations. You should check the <code>n.eff</code> of every parameter of interest, and of any interesting parameter combinations. In general, we need <span class="math inline">\(\text{n.eff} \geq 100\)</span> independent steps to get reasonable Monte Carlo estimates of model parameters. <strong>calculer neff sur exemple</strong></p>
</div>
<div id="what-if-you-have-issues-of-convergence" class="section level3" number="1.6.3">
<h3>
<span class="header-section-number">1.6.3</span> What if you have issues of convergence?<a class="anchor" aria-label="anchor" href="#what-if-you-have-issues-of-convergence"><i class="fas fa-link"></i></a>
</h3>
<p>When the effective sample size is too small, you just need to increase burn-in and/or sample more. Using more informative priors might also make the Markov chain converge faster by helping your MCMC algorithm navigating more efficiently the parameter space. In the same spirit, picking better initial values, or good guess, for starting the chain does not harm. A strategy consists in using estimates from simpler models.</p>
<p>If issues persist, then the problem is probably more profound, and you might ask whether there is something wrong with your model <strong>fork theorem of Gelman?</strong>. A bug in the code? A typo somewhere? A msitake in your maths? As often when coding is involved, the issue can be identified by removing complexities, and start with a simpler model until you find what the problem is.</p>
<p>A general advice is to always start by seeing your model as a data generating tool in the first place, simulate data from it using some realistic values for parameters, and try to recover these parameters by fitting the model to simulated data.</p>
<p>We will also see other strategies to improve convergence in chapter XX. Change your sampler. Reparameterize (standardize covariates, plus non-centering: <span class="math inline">\(\alpha \sim N(0,\sigma)\)</span> becomes <span class="math inline">\(\alpha = z \sigma\)</span> with <span class="math inline">\(z \sim N(0,1)\)</span>).</p>
<p><strong>lisser cette partie</strong></p>
</div>
</div>
<div id="summary" class="section level2" number="1.7">
<h2>
<span class="header-section-number">1.7</span> Summary<a class="anchor" aria-label="anchor" href="#summary"><i class="fas fa-link"></i></a>
</h2>
<ul>
<li><p>With the Bayes’ theorem, you may update your beliefs (<em>prior</em>) with new data (<em>likelihood</em>) to get posterior beliefs (<em>posterior</em>): <span class="math inline">\(\text{posterior} \propto \text{likelihood} \times \text{prior}\)</span></p></li>
<li><p>The idea of Markov chain Monte Carlo (MCMC) is to simulate values from a Markov chain which has a stationary distribution equal to the posterior distribution you’re after.</p></li>
<li><p>In practice, you run a Markov chain multiple times starting from over-dispersed initial values.</p></li>
<li><p>You discard iterations in an initial burn-in phase and achieve convergence when all replicates reach the same regime.</p></li>
<li><p>From there, you run the chain long enough and proceed with calculating Monte Carlo estimates (numerical summaries) for model parameters.</p></li>
</ul>
<p><strong>Takes some training; will make more sense in next chapter, and also put to use in case studies!</strong></p>
</div>
<div id="further-reading" class="section level2" number="1.8">
<h2>
<span class="header-section-number">1.8</span> Further reading<a class="anchor" aria-label="anchor" href="#further-reading"><i class="fas fa-link"></i></a>
</h2>
<p><strong>cette partie dans chaque chapitre c’est plutôt les trucs essentiels en lien avec le chapitre il me semble non?</strong></p>
<ul>
<li><p>Gelman, A. and Hill, J. (2006). <a href="https://www.cambridge.org/core/books/data-analysis-using-regression-and-multilevelhierarchical-models/32A29531C7FD730C3A68951A17C9D983">Data Analysis Using Regression and Multilevel/Hierarchical Models (Analytical Methods for Social Research)</a>. Cambridge: Cambridge University Press.</p></li>
<li><p>McCarthy, M. (2007). <a href="https://www.cambridge.org/core/books/bayesian-methods-for-ecology/9225F65B8A25D69B0B6C50B5A9A78201">Bayesian Methods for Ecology</a>. Cambridge: Cambridge University Press.</p></li>
<li><p>McElreath, R. (2020). <a href="https://xcelab.net/rm/statistical-rethinking/">Statistical Rethinking: A Bayesian Course with Examples in R and Stan (2nd ed.)</a>. CRC Press.</p></li>
</ul>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-king_bayesian_2009" class="csl-entry">
King, Ruth, B. J. T. Morgan, O. Gimenez, and S. P. Brooks. 2009. <em>Bayesian <span>Analysis</span> for <span>Population</span> <span>Ecology</span></em>. Chapman; Hall/CRC.
</div>
<div id="ref-mcelreathbook" class="csl-entry">
McElreath, Richard. 2016. <em>Statistical <span>Rethinking</span>: <span>A</span> <span>Bayesian</span> <span>Course</span> with <span>Examples</span> in <span>R</span> and <span>Stan</span></em>. 1st edition. Chapman; Hall/CRC.
</div>
<div id="ref-mcgrayne2011" class="csl-entry">
McGrayne, Sharon Bertsch. 2011. <em>The <span>Theory</span> <span>That</span> <span>Would</span> <span>Not</span> <span>Die</span>: <span>How</span> <span>Bayes</span>’ <span>Rule</span> <span>Cracked</span> the <span>Enigma</span> <span>Code</span>, <span>Hunted</span> <span>Down</span> <span>Russian</span> <span>Submarines</span>, and <span>Emerged</span> <span>Triumphant</span> from <span>Two</span> <span>Centuries</span> of <span>Controversy</span></em>. Yale University Press.
</div>
</div>
</div>
</div>














  <div class="chapter-nav">
<div class="prev"><a href="about-the-author.html">About the Author</a></div>
<div class="empty"></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#crashcourse"><span class="header-section-number">1</span> Bayesian statistics &amp; MCMC</a></li>
<li><a class="nav-link" href="#introduction"><span class="header-section-number">1.1</span> Introduction</a></li>
<li><a class="nav-link" href="#bayes-theorem"><span class="header-section-number">1.2</span> Bayes’ theorem</a></li>
<li><a class="nav-link" href="#what-is-the-bayesian-approach"><span class="header-section-number">1.3</span> What is the Bayesian approach?</a></li>
<li><a class="nav-link" href="#numerical-approx"><span class="header-section-number">1.4</span> Approximating posterior distributions via numerical integration</a></li>
<li><a class="nav-link" href="#bayesian-computation-with-markov-chain-monte-carlo-mcmc"><span class="header-section-number">1.5</span> Bayesian computation with Markov chain Monte Carlo (MCMC)</a></li>
<li>
<a class="nav-link" href="#assessing-convergence"><span class="header-section-number">1.6</span> Assessing convergence</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#burn-in"><span class="header-section-number">1.6.1</span> Burn-in</a></li>
<li><a class="nav-link" href="#chain-length"><span class="header-section-number">1.6.2</span> Chain length</a></li>
<li><a class="nav-link" href="#what-if-you-have-issues-of-convergence"><span class="header-section-number">1.6.3</span> What if you have issues of convergence?</a></li>
</ul>
</li>
<li><a class="nav-link" href="#summary"><span class="header-section-number">1.7</span> Summary</a></li>
<li><a class="nav-link" href="#further-reading"><span class="header-section-number">1.8</span> Further reading</a></li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
<li><a id="book-source" href="https://github.com/oliviergimenez/banana-book/blob/master/01-bayesMCMC.Rmd">View source <i class="fab fa-github"></i></a></li>
          <li><a id="book-edit" href="https://github.com/oliviergimenez/banana-book/edit/master/01-bayesMCMC.Rmd">Edit this page <i class="fab fa-github"></i></a></li>
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>Bayesian Analysis of Capture-Recapture Data with Hidden Markov Models – Theory and Case Studies in R</strong>" was written by Olivier Gimenez. It was last built on 2021-09-15.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script><script type="text/x-mathjax-config">const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
for (let popover of popovers) {
  const div = document.createElement('div');
  div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
  div.innerHTML = popover.getAttribute('data-content');

  var has_math = div.querySelector("span.math");
  if (has_math) {
    document.body.appendChild(div);
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
    MathJax.Hub.Queue(function() {
      popover.setAttribute('data-content', div.innerHTML);
      document.body.removeChild(div);
    })
  }
}
</script>
</body>
</html>
