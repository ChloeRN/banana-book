[{"path":"index.html","id":"welcome","chapter":"Welcome","heading":"Welcome","text":"Welcome online version book Bayesian Analysis Capture-Recapture Data Hidden Markov Models – Theory Case Studies R. HMM framework gained much attention ecological literature last decade, suggested general modelling framework demography plant animal populations. particular, HMMs increasingly used analyse capture-recapture data estimate key population parameters (e.g., survival, dispersal, recruitment abundance) applications fields ecology.parallel, Bayesian statistics well established fast growing ecology related disciplines, resonates scientific reasoning allows accommodating uncertainty smoothly. popularity Bayesian statistics also comes availability free pieces software (WinBUGS, OpenBUGS, JAGS, Stan, NIMBLE) allow practitioners code analyses.book offers Bayesian treatment HMMs applied capture-recapture data. learn use R package NIMBLE seen many future Bayesian statistical ecology deal complex models /big data. important part book consists case studies presented tutorial style abide “learning ” philosophy.’m currently writing book, welcome feedback. may raise issue , amend directly R Markdown file generated page ’re reading clicking ‘Edit page’ icon right panel, email . Many thanks!Olivier Gimenez, Montpellier, France\nLast updated: January 21, 2022","code":""},{"path":"index.html","id":"license","chapter":"Welcome","heading":"License","text":"online version book licensed Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License.code public domain, licensed Creative Commons CC0 1.0 Universal (CC0 1.0).","code":""},{"path":"preface.html","id":"preface","chapter":"Preface","heading":"Preface","text":"","code":""},{"path":"preface.html","id":"why-this-book","chapter":"Preface","heading":"Why this book?","text":"completed. capture-recapture data models, fields application.1 Brief history capture-recapture, switch state-space/hidden Markov model (HMM) formulation. Flexibility HMM decompose complex problems smaller pieces easier understand, model analyse. satellite guidance conservation endangered species. Bayes? Also three fav research topics – capture-recapture, HMM Bayes statistics – let’s enjoy great cocktail together.","code":""},{"path":"preface.html","id":"who-should-read-this-book","chapter":"Preface","heading":"Who should read this book?","text":"book aimed beginners ’re comfortable using R write basic code (including loops), well connoisseurs capture-recapture ’d like tap power Bayesian side statistics. audiences, thinking HMM framework help confidently building models make capture-recapture data.","code":""},{"path":"preface.html","id":"what-will-you-learn","chapter":"Preface","heading":"What will you learn?","text":"book divided five parts. first part aimed getting --speed Bayesian statistics, NIMBLE, hidden Markov models. second part teach capture-recapture models open populations, reproducible R code ease learning process. third part, focus issues inferring states (dealing uncertainty assignment, modelling waiting time distribution). fourth part provides real-world case studies scientific literature can reproduce using material covered previous chapters. problems can either ) used cement deepen understanding methods models, ii) adapted purpose, iii) serve teaching projects. fifth last chapter closes book take-home messages recommendations, list frequently asked questions references cited book. Likely amended feedbacks.","code":""},{"path":"preface.html","id":"what-wont-you-learn","chapter":"Preface","heading":"What won’t you learn?","text":"hardly maths book. equations use either simple enough understood without background maths, can skipped without prejudice. cover Bayesian statistics even hidden Markov models fully, provide just need work capture-recapture data. interested knowing topics, hopefully section Suggested reading end chapter put right direction. also number important topics specific capture-recapture cover, including closed-population capture-recapture models (Williams, Nichols, Conroy 2002), spatial capture-recapture models (Royle et al. 2013). models can treated HMMs, now usual formulation just fine. spatial considerations Covariates chapter w/ splines CAR. ’m sure yet SCR models (R. Glennie’s Biometrics paper HMMs open pop SCR easy Bayes transform implement NIMBLE).","code":""},{"path":"preface.html","id":"prerequisites","chapter":"Preface","heading":"Prerequisites","text":"book uses primarily R package NIMBLE, need install least R NIMBLE. bunch R packages used. can install running:","code":"\ninstall.packages(c(\n  \"magick\", \"MCMCvis\", \"nimble\", \"pdftools\", \n  \"tidyverse\", \"wesanderson\" \n))"},{"path":"preface.html","id":"acknowledgements","chapter":"Preface","heading":"Acknowledgements","text":"completed.","code":""},{"path":"preface.html","id":"how-this-book-was-written","chapter":"Preface","heading":"How this book was written","text":"writing book RStudio using bookdown. book website hosted GitHub Pages, automatically updated every push Github Actions. source available GitHub.version book ’re reading built R version 4.1.0 (2021-05-18) following packages:","code":""},{"path":"about-the-author.html","id":"about-the-author","chapter":"About the author","heading":"About the author","text":"name Olivier Gimenez (https://oliviergimenez.github.io/). senior (euphemism young anymore) scientist National Centre Scientific Research (CNRS) beautiful city Montpellier, France.struggled studying maths, obtained PhD applied statistics long time ago galaxy wine cheese. awarded habilitation (https://en.wikipedia.org/wiki/Habilitation) ecology evolution stop pretending understand colleagues talking . recently embarked sociology studies hey, .Lost somewhere interface animal ecology, statistical modeling social sciences, -called expertise lies population dynamics species distribution modeling address questions ecology conservation biology impact human activities management large carnivores. nothing without students colleagues kind enough bear .may find Twitter (https://twitter.com/oaggimenez), GitHub (https://github.com/oliviergimenez), get touch email.","code":""},{"path":"introduction.html","id":"introduction","chapter":"Introduction","heading":"Introduction","text":"","code":""},{"path":"crashcourse.html","id":"crashcourse","chapter":"1 Bayesian statistics & MCMC","heading":"1 Bayesian statistics & MCMC","text":"","code":""},{"path":"crashcourse.html","id":"introduction-1","chapter":"1 Bayesian statistics & MCMC","heading":"1.1 Introduction","text":"first chapter, learn Bayesian theory , may use simple example. also see implement simulation algorithms implement Bayesian method complex analyses. exhaustive treatment Bayesian statistics, get need navigate rest book.","code":""},{"path":"crashcourse.html","id":"bayes-theorem","chapter":"1 Bayesian statistics & MCMC","heading":"1.2 Bayes’ theorem","text":"Let’s wait longer jump . Bayesian statistics relies Bayes’ theorem (law, rule, whatever prefer) named Reverend Thomas Bayes (Figure 1.1). theorem published 1763 two years Bayes’ death thanks friend’s efforts Richard Price, independently discovered Pierre-Simon Laplace (McGrayne 2011).\nFigure 1.1: Cartoon Thomas Bayes Bayes’ theorem background. Source: James Kulich\nsee minute, Bayes’ theorem conditional probabilities, somehow tricky understand. Conditional probability outcome event given event B, denote \\(\\Pr(\\mid B)\\), probability occurs, revised considering additional information event B occurred.2 order B appear important, make sure confuse \\(\\Pr(\\mid B)\\) \\(\\Pr(B \\mid )\\).Bayes’ theorem (Figure 1.2) gives \\(\\Pr(\\mid B)\\) using marginal probabilities \\(\\Pr()\\) \\(\\Pr(B)\\) \\(\\Pr(B \\mid )\\):\n\\[\\Pr(\\mid B) = \\displaystyle{\\frac{ \\Pr(B \\mid ) \\; \\Pr()}{\\Pr(B)}}.\\]\nOriginally, Bayes’ theorem seen way infer unkown cause particular effect B, knowing probability effect B given cause . Think example situation medical diagnosis needed, unkown disease B symptoms, doctor knows P(symptoms|disease) wants derive P(disease|symptoms). way reversing \\(\\Pr(B \\mid )\\) \\(\\Pr(\\mid B)\\) explains Bayesian thinking used referred ‘inverse probability.’\nFigure 1.2: Bayes’ theorem spelt blue neon. Source: Wikipedia\ndon’t know , need think twice messing letters around. find easier remember Bayes’ theorem written like this3:great think , exactly scientific method ! ’d like know plausible hypothesis based data collected, possibly compare several hypotheses among . respect, Bayesian reasoning matches scientific reasoning, probably explains Bayesian framework natural understanding statistics.might ask , Bayesian statistics default statistics? Clearly, futile wars male statisticians (including Ronald Fisher, Jerzy Neyman Egon Sharpe Pearson among others), little progress made two centuries. Also, recently, practical problems implement Bayes’ theorem. Recent advances computational power coupled development new algorithms led great increase application Bayesian methods within last three decades.","code":""},{"path":"crashcourse.html","id":"what-is-the-bayesian-approach","chapter":"1 Bayesian statistics & MCMC","heading":"1.3 What is the Bayesian approach?","text":"Typical statistical problems involve estimating parameter (several parameters) \\(\\theta\\) available data. , might used frequentist rather Bayesian method. frequentist approach, particular maximum likelihood estimation (MLE), assumes parameters fixed, unknown values estimated. Therefore classical estimates generally point estimates parameters interest. contrast, Bayesian approach assumes parameters fixed, unknown distribution4.Bayesian approach based upon idea , experimenter, begin prior beliefs system. collect data update prior beliefs basis observations. observations might arise field work, lab work expertise esteemed colleagues. updating process based upon Bayes’ theorem. Loosely, let’s say \\(= \\theta\\) \\(B = \\text{data}\\), Bayes’ theorem gives way estimate parameter \\(\\theta\\) given data :\\[{\\color{red}{\\Pr(\\theta \\mid \\text{data})}} = \\frac{\\color{blue}{\\Pr(\\text{data} \\mid \\theta)} \\times \\color{green}{\\Pr(\\theta)}}{\\color{orange}{\\Pr(\\text{data})}}.\\]\nLet’s spend time going quantity formula.left-hand side \\(\\color{red}{\\text{posterior distribution}}\\). represents know seen data. basis inference clearly ’re , distribution, possibly multivariate one parameter.right-hand side, \\(\\color{blue}{\\text{likelihood}}\\). quantity MLE approach. Yes, Bayesian frequentist approaches likelihood core, mostly explains results often differ much. likelihood captures information data, given model parameterized \\(\\theta\\).\\(\\color{green}{\\text{prior distribution}}\\). quantity represents know seeing data. source much discussion Bayesian approach. may vague don’t know anything \\(\\theta\\). Usually however, never start scratch, ’d like prior reflect information have5.Last, \\(\\color{orange}{\\Pr(\\text{data})}\\) sometimes called average likelihood obtained integrating likelihood respect prior \\(\\color{orange}{\\Pr(\\text{data}) = \\int{L(\\text{data} \\mid \\theta)\\Pr(\\theta) d\\theta}}\\) posterior standardized, integrates one posterior distribution. average likelihood integral dimension number parameters \\(\\theta\\) need estimate. quantity difficult, impossible, calculate general. one reasons Bayesian method wasn’t used recently, need algorithms estimate posterior distributions illustrate next section.","code":""},{"path":"crashcourse.html","id":"numerical-approx","chapter":"1 Bayesian statistics & MCMC","heading":"1.4 Approximating posteriors via numerical integration","text":"Let’s take example illustrate Bayes’ theorem. Say capture, mark release \\(n = 57\\) animals beginning winter, recapture \\(y = 19\\) animals alive6. ’d like estimate winter survival \\(\\theta\\).build model first. Assuming animals independent survival probability, \\(y\\) number alive animals end winter binomial distribution7 \\(n\\) trials \\(\\theta\\) probability success:\\[\\begin{align*}\ny &\\sim \\text{Binomial}(n, \\theta) &\\text{[likelihood]}\n\\end{align*}\\]likelihood can visualised R:\nFigure 1.3: Binomial likelihood \\(n = 57\\) released animals \\(y = 19\\) survivors winter. value survival (x-axis) corresponds maximum likelihood function (y-axis) MLE, proportion success example, close 0.33.\nBesides likelihood, priors another component model Bayesian approach. parameter probability, one thing know prior continuous random variable lies 0 1. reflect , often go uniform distribution \\(U(0,1)\\) imply vague priors. vague means survival , see data, probability falling 0.1 0.2 0.8 0.9, example.\\[\\begin{align*}\n\\theta &\\sim \\text{Uniform}(0, 1) &\\text{[prior }\\theta \\text{]}\n\\end{align*}\\]Now apply Bayes’ theorem. write R function computes product likelihood times prior, numerator Bayes’ theorem: \\(\\Pr(\\text{data} \\mid \\theta) \\times \\Pr(\\theta)\\)write another function calculates denominator, average likelihood: \\(\\Pr(\\text{data}) = \\int{L(\\theta \\mid \\text{data}) \\Pr(\\theta) d\\theta}\\)use R function integrate calculate integral denominator, implements quadrature techniques divide little squares area underneath curve delimited function integrate (numerator), count .get numerical approximation posterior Figure 1.4 applying Bayes’ theorem.\nFigure 1.4: Winter survival posterior distribution obtained numerical integration.\ngood numerical approximation survival posterior distribution? Ideally, want compare approximation true posterior distribution. Although closed-form expression posterior distribution general intractable, combine binomial likelihood together beta distribution prior, posterior distribution also beta distribution, makes amenable sorts exact calculations8. beta distribution continuous 0 1, extends uniform distribution situations outcomes equally likely. two parameters \\(\\) \\(b\\) control shape (Figure 1.5).\nFigure 1.5: distribution beta(\\(\\),\\(b\\)) different values \\(\\) \\(b\\). Note \\(= b = 1\\), get uniform distribution 0 1 top left panel. \\(\\) \\(b\\) equal, distribution symmetric, bigger \\(\\) \\(b\\), peaked distribution smaller variance.\n\nFigure 1.6: Comparison exact (dashed line) vs. numerical approximation (continuous line) winter survival posterior distribution.\nexample, single parameter estimate, winter survival. means dealing one-dimensional integral denominator pretty easy quadrature techniques R function integrate(). Now multiple parameters? example, imagine ’d like fit capture-recapture model detection probability \\(p\\) regression parameters \\(\\alpha\\) \\(\\beta\\) intercept slope relationship survival probability covariate, Bayes’ theorem gives posterior distribution three parameters together:\\[ \\Pr(\\alpha, \\beta, p \\mid \\text{data}) = \\frac{ \\Pr(\\text{data} \\mid \\alpha, \\beta, p) \\times \\Pr(\\alpha, \\beta, p)}{\\iiint \\, \\Pr(\\text{data} \\mid \\alpha, \\beta, p) \\Pr(\\alpha, \\beta, p) d\\alpha d\\beta dp} \\]\ntwo computational challenges formula. First, really wish calculate three-dimensional integral? answer , one-dimensional two-dimensional integrals much can go standard methods. Second, ’re interested posterior distribution parameter separately joint posterior distribution. -called marginal distribution \\(p\\) example obtained integrating parameters – two-dimensional integral example. Now imagine tens hundreds parameters estimate, integrals become highly multi-dimensional simply intractable. next section, introduce powerful simulation methods circumvent issue.","code":"\ny <- 19 # nb of success\nn <- 57 # nb of attempts\ngrid <- seq(0, 1, 0.01) # grid of values for survival\nlikelihood <- dbinom(y, n, grid) # compute binomial likelihood\ndf <- data.frame(survival = grid, likelihood = likelihood) \ndf %>%\n  ggplot() + \n  aes(x = survival, y = likelihood) + \n  geom_line(size = 1.5)\nnumerator <- function(theta) dbinom(y, n, theta) * dunif(theta, 0, 1)\ndenominator <- integrate(numerator,0,1)$value\ngrid <- seq(0, 1, 0.01) # grid of values for theta\nnumerical_posterior <- data.frame(survival = grid, \n                                  posterior = numerator(grid)/denominator) # Bayes' theorem\nnumerical_posterior %>%\n  ggplot() +\n  aes(x = survival, y = posterior) + \n  geom_line(size = 1.5)"},{"path":"crashcourse.html","id":"markov-chain-monte-carlo-mcmc","chapter":"1 Bayesian statistics & MCMC","heading":"1.5 Markov chain Monte Carlo (MCMC)","text":"early 1990s, statisticians rediscovered work 1950’s physics. famous paper lay fundations modern Bayesian statistics (Figure 1.7), authors use simulations approximate posterior distributions precision drawing large samples. neat trick avoid explicit calculation multi-dimensional integrals struggle using Bayes’ theorem.\nFigure 1.7: MCMC article cover. Source: Journal Chemical Physics\nsimulation algorithms called Markov chain Monte Carlo (MCMC), definitely gave boost Bayesian statistics. two parts MCMC, Markov chain Monte Carlo, let’s try make sense terms.","code":""},{"path":"crashcourse.html","id":"monte-carlo-integration","chapter":"1 Bayesian statistics & MCMC","heading":"1.5.1 Monte Carlo integration","text":"Monte Carlo stand ? Monte Carlo integration simulation technique calculate integrals function \\(f\\) random variable \\(X\\) distribution \\(\\Pr(X)\\) say \\(\\int f(X) \\Pr(X)dX\\). draw values \\(X_1,\\ldots,X_k\\) \\(\\Pr(X)\\) distribution \\(X\\), apply function \\(f\\) values, calculate mean new values \\(\\displaystyle{\\frac{1}{k}}\\sum_{=1}^k{f(X_i)}\\) approximate integral. Monte Carlo integration used Bayesian context? posterior distribution contains information need parameter estimated. dealing many parameters however, may want summarise posterior results calculating numerical summaries. simplest numerical summary mean posterior distribution, \\(E(\\theta) = \\int \\theta \\Pr(\\theta|\\text{data})\\), \\(X\\) \\(\\theta\\) now \\(f\\) identity function. Posterior mean can calculated Monte Carlo integration:may check mean just calculated matches closely expectation beta distribution10:Another useful numerical summary credible interval within parameter falls probability, usually 0.95 hence 95\\(\\%\\) credible interval. Finding bounds credible interval requires calculating quantiles, turn involves integrals use Monte Carlo integration. 95\\(\\%\\) credible interval winter survival can obtained R :","code":"\nsample_from_posterior <- rbeta(1000, 20, 39) # draw 1000 values from posterior survival beta(20,39)\nmean(sample_from_posterior) # compute mean with Monte Carlo integration\n## [1] 0.3399\n20/(20+39) # expectation of beta(20,39)\n## [1] 0.339\nquantile(sample_from_posterior, probs = c(2.5/100, 97.5/100))\n##   2.5%  97.5% \n## 0.2314 0.4578"},{"path":"crashcourse.html","id":"markov-chains","chapter":"1 Bayesian statistics & MCMC","heading":"1.5.2 Markov chains","text":"Markov chain? Markov chain random sequence numbers, number depends previous number. example weather home town Southern France, Montpellier, sunny day likely followed another sunny day, say probability 0.8, rainy day rarely followed another rainy day, say probability 0.1. dynamic Markov chain captured transition matrix \\(\\mathbf{\\Gamma}\\):\n\\[\n\\begin{matrix}\n& \\\\\n\\mathbf{\\Gamma} = \n    \\left ( \\vphantom{ \\begin{matrix} 12 \\\\ 12 \\end{matrix} } \\right .\n\\end{matrix}\n\\hspace{-1.2em}\n\\begin{matrix}\n    \\text{sunny tomorrow} & \\text{rainy tomorrow} \\\\ \n0.8 & 0.2 \\\\ \n0.9 & 0.1 \\\\ \n\\end{matrix}\n\\hspace{-0.2em}\n\\begin{matrix}\n& \\\\\n\\left . \\vphantom{ \\begin{matrix} 12 \\\\ 12 \\\\ 12 \\end{matrix} } \\right )\n    \\begin{matrix}\n    \\text{sunny today} \\\\ \\text{rainy today}\n    \\end{matrix}\n\\end{matrix}\n\\]\nrows weather today, columns weather tomorrow. cells give probability sunny rainy day tomorrow, given day sunny rainy today. certain conditions11, Markov chain converge unique stationary distribution. weather example, let’s run Markov chain 20 steps:row transition matrix converges distribution \\((0.82, 0.18)\\) number steps increases. Convergence happens matter state start , always probability 0.82 day sunny 0.18 day rainy.Back MCMC, core idea can build Markov chain given stationary distribution set desired posterior distribution.","code":"\nweather <- matrix(c(0.8, 0.2, 0.9, 0.1), nrow = 2, byrow = T) # transition matrix\nsteps <- 20\nfor (i in 1:steps){\n  weather <- weather %*% weather # matrix multiplication\n}\nround(weather, 2) # matrix product after 20 steps\n##      [,1] [,2]\n## [1,] 0.82 0.18\n## [2,] 0.82 0.18"},{"path":"crashcourse.html","id":"metropolis-algorithm","chapter":"1 Bayesian statistics & MCMC","heading":"1.5.3 Metropolis algorithm","text":"several ways constructing Markov chains Bayesian inference12. illustrate Metropolis algorithm implement practice13.Let’s go back example animal survival estimation. illustrate sampling survival posterior distribution. write functions likelihood, prior posterior.Metropolis algorithm works follows:pick value parameter estimated. start Markov chain – starting value.pick value parameter estimated. start Markov chain – starting value.decide go next, propose move away current value parameter – candidate value. , add current value random value e.g. normal distribution variance – proposal distribution. Metropolis algorithm particular case Metropolis-Hastings algorithm symmetric proposals.decide go next, propose move away current value parameter – candidate value. , add current value random value e.g. normal distribution variance – proposal distribution. Metropolis algorithm particular case Metropolis-Hastings algorithm symmetric proposals.compute ratio probabilities candidate current locations \\(R=\\displaystyle{\\frac{{\\Pr(\\text{candidate}|\\text{data})}}{{\\Pr(\\text{current}|\\text{data})}}}\\). magic MCMC happens, \\(\\Pr(\\text{data})\\), denominator Bayes’ theorem, appears numerator denominator \\(R\\) therefore cancels need calculated.compute ratio probabilities candidate current locations \\(R=\\displaystyle{\\frac{{\\Pr(\\text{candidate}|\\text{data})}}{{\\Pr(\\text{current}|\\text{data})}}}\\). magic MCMC happens, \\(\\Pr(\\text{data})\\), denominator Bayes’ theorem, appears numerator denominator \\(R\\) therefore cancels need calculated.posterior candidate location \\(\\Pr(\\text{candidate}|\\text{data})\\) higher current location \\(\\Pr(\\text{current}|\\text{data})\\), words candidate value plausible current value, definitely accept candidate value. , accept candidate value probability \\(R\\) reject probability \\(1-R\\). example, candidate value ten times less plausible current value, accept probability 0.1 reject probability 0.9. work practice? use continuous spinner lands somewhere 0 1 – call random spin \\(X\\). \\(X\\) smaller \\(R\\), move candidate location, otherwise remain current location. want accept reject often. practice, Metropolis algorithm acceptance probability 0.2 0.4, can achieved tuning variance normal proposal distribution.posterior candidate location \\(\\Pr(\\text{candidate}|\\text{data})\\) higher current location \\(\\Pr(\\text{current}|\\text{data})\\), words candidate value plausible current value, definitely accept candidate value. , accept candidate value probability \\(R\\) reject probability \\(1-R\\). example, candidate value ten times less plausible current value, accept probability 0.1 reject probability 0.9. work practice? use continuous spinner lands somewhere 0 1 – call random spin \\(X\\). \\(X\\) smaller \\(R\\), move candidate location, otherwise remain current location. want accept reject often. practice, Metropolis algorithm acceptance probability 0.2 0.4, can achieved tuning variance normal proposal distribution.repeat 2-4 number times – steps.repeat 2-4 number times – steps.Enough theory, let’s implement Metropolis algorithm R. Let’s start setting scene.Now follow 5 steps ’ve just described. First, pick starting value, store (step 1)., need function propose candidate value. add value taken normal distribution mean zero standard deviation call away. work logit scale make sure candidate value survival lies 0 1.Now ’re ready steps 2, 3 4. write loop take care step 5. start initial value 0.5 run algorithm 100 steps iterations.get following values.\nFigure 1.8: Visualisation Markov chain starting value 0.5, steps iterations x-axis, samples y-axis. graphical representation called trace plot.\nacceptance probability average number times accepted candidated value, 0.44 almost satisfying.\nFigure 1.9: Trace plot survival two chains starting 0.2 (yellow) 0.5 (blue) run 100 steps.\n\nFigure 1.10: Trace plot survival chain starting 0.5 1000 steps.\n’re , trace plot looks like beautiful lawn, see Section 1.6. find informative look animated version Figure 1.10, helps understanding stochastic behavior algorithm, also realise chains converge stationary distribution, see Figure 1.11.\nFigure 1.11: Animated trace plot survival three chains starting 0.2, 0.5 0.7 run 1000 steps.\nstationary distribution reached, may regard realisations Markov chain sample posterior distribution, obtain numerical summaries. next section, consider several important implementation issues.","code":"\n# 19 animals recaptured alive out of 57 captured, marked and released\nsurvived <- 19\nreleased <- 57\n\n# binomial log-likelihood function\nloglikelihood <- function(x, p){\n  dbinom(x = x, size = released, prob = p, log = TRUE)\n}\n\n# uniform prior density\nlogprior <- function(p){\n  dunif(x = p, min = 0, max = 1, log = TRUE)\n}\n\n# posterior density function (log scale)\nposterior <- function(x, p){\n  loglikelihood(x, p) + logprior(p) # - log(Pr(data))\n}\nsteps <- 100 # number of steps\ntheta.post <- rep(NA, steps) # vector to store samples\naccept <- rep(NA, steps) # keep track of accept/reject\nset.seed(1234) # for reproducibility\ninits <- 0.5\ntheta.post[1] <- inits\naccept[1] <- 1\nmove <- function(x, away = 1){ # by default, standard deviation of the proposal distribution is 1\n  logitx <- log(x / (1 - x)) # apply logit transform (-infinity,+infinity)\n  logit_candidate <- logitx + rnorm(1, 0, away) # add a value taken from N(0,sd=away) to current value\n  candidate <- plogis(logit_candidate) # back-transform (0,1)\n  return(candidate)\n}\nfor (t in 2:steps){ # repeat steps 2-4 (step 5)\n  \n  # propose candidate value for survival (step 2)\n  theta_star <- move(theta.post[t-1])\n  \n  # calculate ratio R (step 3)\n  pstar <- posterior(survived, p = theta_star)  \n  pprev <- posterior(survived, p = theta.post[t-1])\n  logR <- pstar - pprev # likelihood and prior are on the log scale\n  R <- exp(logR)\n  \n  # accept candidate value or keep current value (step 4)\n  X <- runif(1, 0, 1) # spin continuous spinner\n  if (X < R){\n    theta.post[t] <- theta_star # accept candidate value\n    accept[t] <- 1 # accept\n  }\n  else{\n    theta.post[t] <- theta.post[t-1] # keep current value\n    accept[t] <- 0 # reject\n  }\n}\nhead(theta.post) # first values\n## [1] 0.5000 0.2302 0.2906 0.2906 0.2980 0.2980\ntail(theta.post) # last values\n## [1] 0.2622 0.2622 0.2622 0.3727 0.3232 0.3862"},{"path":"crashcourse.html","id":"convergence-diag","chapter":"1 Bayesian statistics & MCMC","heading":"1.6 Assessing convergence","text":"","code":""},{"path":"crashcourse.html","id":"burn-in","chapter":"1 Bayesian statistics & MCMC","heading":"1.6.1 Burn-in","text":"practice, discard observations start Markov chain just use observations chain converged. initial observations discard usually referred burn-.simplest method determine length burn-period look trace plots. Going back example, see trace plot Figure 1.12 need least 100 iterations achieve convergence toward average survival around 0.3. always better conservative specifying length burn-period, example, use 250 even 500 iterations burn-. length burn-period can determined performing preliminary MCMC short runs.\nFigure 1.12: Determining length burn-period. chain starts value 0.99 rapidly stabilises, values bouncing back forth around 0.3 100th iteration onwards. may choose shaded area burn-, discard corresponding values.\nInspecting trace plot single run Markov chain useful. However, usually run Markov chain several times, starting different -dispersed points, check runs achieve stationary distribution. approach formalised using Brooks-Gelman-Rubin (BGR) statistic \\(\\hat{R}\\) measures ratio total variability combining multiple chains (-chain plus within-chain) within-chain variability. BGR statistic asks whether chain effect, much alike \\(F\\) test analysis variance. Values 1.1 indicate likely convergence.Back example, run two Markov chains starting values 0.2 0.8 using 100 5000 iterations, calculate BGR statistic using half number iterations length burn-. Figure 1.13, get value BGR statistic near 1 2000 iterations, suggests 2000 iterations burn-, evidence lack convergence.\nFigure 1.13: Brooks-Gelman-Rubin statistic function number iterations.\nimportant bear mind value near 1 BGR statistic necessary sufficient condition convergence. words, diagnostic tell sure Markov chain achieved convergence, .14","code":""},{"path":"crashcourse.html","id":"chain-length","chapter":"1 Bayesian statistics & MCMC","heading":"1.6.2 Chain length","text":"long chain needed produce reliable parameter estimates? answer question, need keep mind successive steps Markov chain independent – usually referred autocorrelation. Ideally, like keep autocorrelation low possible. , trace plots useful diagnose issues autocorrelation. Let’s get back survival example. Figure 1.14 shows trace plots different values standard deviation (parameter away) (normal) proposal distribution use propose candidate value (Section 1.5.3). Small big moves provide high correlations successive observations Markov chain, whereas standard deviation 1 allows efficient exploration parameter space. movement around parameter space referred mixing. Mixing bad chain makes small big moves, good otherwise.\nFigure 1.14: Trace plots different values standard deviation (SD) proposal distribution. Left: chain exhibits small moves mixing bad. Right: chain exhibits big moves mixing bad. Middle: chain exhibits adequate moves mixing good. thousand last iterations shown.\naddition trace plots, autocorrelation function (ACF) plots convenient way displaying strength autocorrelation given sample values. ACF plots provide autocorrelation successively sampled values separated increasing number iterations, lag (Figure 1.15).\nFigure 1.15: Autocorrelation function plots different values standard deviation (SD) proposal distribution. Left right: Autocorrelation strong, decreases slowly increasing lag mixing bad. Middle: Autocorrelation weak, decreases rapidly increasing lag mixing good.\nAutocorrelation necessarily big issue. Strongly correlated observations just require large sample sizes therefore longer simulations. many iterations exactly? effective sample size (n.eff) measures chain length taking account chain autocorrelation. check n.eff every parameter interest, interesting parameter combinations. general, need \\(\\text{n.eff} \\geq 1000\\) independent steps get reasonable Monte Carlo estimates model parameters. animal survival example, n.eff can calculated R coda::effectiveSize() function.expected, n.eff less number MCMC iterations autocorrelation. standard deviation proposal distribution 1 mixing good (Figures 1.14 1.15) get satisfying effective sample size.","code":""},{"path":"crashcourse.html","id":"what-if-you-have-issues-of-convergence","chapter":"1 Bayesian statistics & MCMC","heading":"1.6.3 What if you have issues of convergence?","text":"diagnosing MCMC convergence, () often run troubles. section find helpful tips hope.mixing bad effective sample size small, may just need increase burn-/sample . Using informative priors might also make Markov chains converge faster helping MCMC sampler (e.g. Metropolis algorithm) navigating efficiently parameter space. spirit, picking better initial values starting chain harm. , strategy consists using estimates simpler model MCMC chains converge.convergence issues persist, often problem model15. bug code? typo somewhere? mistake maths? often coding involved, issue can identified removing complexities, start simpler model find problem .general advice see model data generating tool first place, simulate data using realistic values parameters, try recover parameter values fitting model simulated data. Simulating model help understanding works, , data need get reasonable parameter estimates.see strategies improve convergence next chapters.16","code":""},{"path":"crashcourse.html","id":"summary","chapter":"1 Bayesian statistics & MCMC","heading":"1.7 Summary","text":"Bayes’ theorem, update beliefs (prior) new data (likelihood) get posterior beliefs (posterior): posterior \\(\\propto\\) likelihood \\(\\times\\) prior.Bayes’ theorem, update beliefs (prior) new data (likelihood) get posterior beliefs (posterior): posterior \\(\\propto\\) likelihood \\(\\times\\) prior.idea Markov chain Monte Carlo (MCMC) simulate values Markov chain stationary distribution equal posterior distribution ’re .idea Markov chain Monte Carlo (MCMC) simulate values Markov chain stationary distribution equal posterior distribution ’re .practice, run Markov chain multiple times starting -dispersed initial values.practice, run Markov chain multiple times starting -dispersed initial values.discard iterations initial burn-phase achieve convergence chains reach regime.discard iterations initial burn-phase achieve convergence chains reach regime., run chains long enough proceed calculating Monte Carlo estimates numerical summaries (e.g. posterior means credible intervals) parameters., run chains long enough proceed calculating Monte Carlo estimates numerical summaries (e.g. posterior means credible intervals) parameters.","code":""},{"path":"crashcourse.html","id":"suggested-reading","chapter":"1 Bayesian statistics & MCMC","heading":"1.8 Suggested reading","text":"Gelman, . Hill, J. (2006). Data Analysis Using Regression Multilevel/Hierarchical Models (Analytical Methods Social Research). Cambridge: Cambridge University Press.Gelman, . Hill, J. (2006). Data Analysis Using Regression Multilevel/Hierarchical Models (Analytical Methods Social Research). Cambridge: Cambridge University Press.Gelman, . colleagues (2020). Bayesian workflow. arXiv preprint.Gelman, . colleagues (2020). Bayesian workflow. arXiv preprint.McCarthy, M. (2007). Bayesian Methods Ecology. Cambridge: Cambridge University Press.McCarthy, M. (2007). Bayesian Methods Ecology. Cambridge: Cambridge University Press.McElreath, R. (2020). Statistical Rethinking: Bayesian Course Examples R Stan (2nd ed.). CRC Press.McElreath, R. (2020). Statistical Rethinking: Bayesian Course Examples R Stan (2nd ed.). CRC Press.","code":""},{"path":"intronimble.html","id":"intronimble","chapter":"2 NIMBLE tutorial","heading":"2 NIMBLE tutorial","text":"","code":""},{"path":"intronimble.html","id":"introduction-2","chapter":"2 NIMBLE tutorial","heading":"2.1 Introduction","text":"second chapter, get familiar NIMBLE, R package implements --date MCMC algorithms fitting complex models. NIMBLE spares coding MCMC algorithms hand, requires specification likelihood priors model parameters. go simple example illustrate NIMBLE main features, ideas hold problems.","code":""},{"path":"intronimble.html","id":"what-is-nimble","chapter":"2 NIMBLE tutorial","heading":"2.2 What is NIMBLE?","text":"NIMBLE stands Numerical Inference statistical Models using Bayesian Likelihood Estimation. Briefly speaking, NIMBLE R package implements MCMC algorithms generate samples posterior distribution model parameters. Freed burden coding MCMC algorithms, specify likelihood priors apply Bayes theorem. , NIMBLE uses syntax similar R syntax, make life easier. -called BUGS language also used programs like WinBUGS, OpenBUGS, JAGS.use NIMBLE may ask? short answer NIMBLE capable much ! First, work within R, background NIMBLE translate code C++ (general) faster computation. Second, NIMBLE extends BUGS language writing new functions statistical distributions , borrow written others. Third, NIMBLE gives full control MCMC samplers, may pick algorithms defaults. Fourth, NIMBLE comes library numerical methods MCMC algorithms, including sequential Monte Carlo (particle filtering) Monte Carlo Expectation Maximization (maximum likelihood). Last least, development team friendly helpful, based users’ feedbacks, NIMBLE folks work constantly improving package capabilities.","code":""},{"path":"intronimble.html","id":"nimble-workflow","chapter":"2 NIMBLE tutorial","heading":"2.3 NIMBLE workflow","text":"run NIMBLE, need specify three things: (1) model (likelihood priors), (2) data, (3) parameters want make inference , (4) initial values (5) MCMC details (number chains, length burn-period number iterations following burn-).First things first, let’s forget load nimble package.Now let’s go back example animal survival previous chapter. First step build model specifying binomial likelihood uniform prior survival probability. use nimbleCode() function.code , ~ means distributed .BUGS declarative language graphical (hierarchical) models. programming languages imperative, means series commands executed order written. declarative language like BUGS like building machine using . line declares component plugged machine, doesn’t matter order declared long right components plugged end code.machine case graphical model12. node (sometimes called vertex) holds one value, may scalar vector. Edges define relationships nodes. huge variety statistical models can thought graphs.code define create simple linear regression model four observations’ deterministic relationships declared ‘<-.’ example, y[] follows normal distribution mean predicted.y[] standard deviation sigma. predicted.y[] result intercept + slope * x[]. -loop yields equivalent writing four lines code, different value . matter order nodes declared. Imagine line code draws part Figure 5.1, matters everything gets drawn end. Available distributions, default alternative parameterizations, functions listed Section 5.2.4.NIMBLE calls non-stochastic nodes ‘deterministic,’ whereas BUGS caThe model definition consists series relations inside block delimited curly brackets { } preceded keyword model. simple linear regression example:relation defines node model. node left relation defined \nterms nodes – referred parent nodes – appear right hand side.\nTaken together, nodes model form directed acyclic graph (parent/child\nrelationships represented directed edges). top-level nodes graph, \nparents, constant nodes, defined either model definition (e.g. 1.0E-3),\ndata model compiled (e.g. x[1]).\nRelations can two types. stochastic relation (~) defines stochastic node, repre-\nsenting random variable model. deterministic relation (<-) defines deterministic\nnode, value determined exactly values parents. equals sign\n(=) can used deterministic relation place left arrow (<-).R2jags can specify model creating special kind function.6 avoids need create temporary files (rjags requires) keeps things tidier R markdown documents.Describe distributions. also nodes. Stochastic. Deterministic. distributed . Provide list built-distributions? can provide , see e.g. Read data.Distinguish constants data. Nimble, “data” data…Constants:\n+ Can never changed\n+ Must provided model defined (part model structure)\n+ E.g. vector known index values, variables used define -loops, etc.defining model code, define constants, initial values data list. Compared WinBUGS JAGS, data initial values can defined way, ‘constants’ new list contains values change, including variables define -loop indices. settings, lists data, constants initial values given follows:Data:\n+ Can changed without re-building model\n+ Can (re-)simulated within model\n+ E.g. stuff appears left “~”computational efficiency, better specify much possible constants. NIMBLE help !can also control starting point chains. Starting different chains quite different parameter values can helpverify MCMC algorithm overly sensitive starting , \nensure MCMC algorithm explored posterior distribution sufficiently.hand, start chain far peak posterior distribution, chain may trouble converging.can provide either specific starting points chain function generates random starting points.Specify initial values.parameters save? Define parameters keep track (.e., parameters interest).MCMC detailsNumber posterior samples per chain:\n\\[n.posterior = \\frac{n.iter - n.burnin}{n.thin}\\]Run model, tadaa!Details messages received.Say thin. ok ’d like, just think \\(n.posterior = \\frac{n.iter - n.burnin}{n.thin}\\).Proposer le même modèle avec la bernoulli pour montrer une boucle. binomial just sum Bernoulli outcomes. Like flipping coin individual get survivor prob phi. Comme dans annexe Hobbs. Vectorize also.","code":"\nlibrary(nimble)\nmodel <- nimbleCode({\n  # likelihood\n  survived ~ dbinom(theta, released)\n  # prior\n  theta ~ dunif(0, 1)\n})\nmy.data <- list(released = 57, survived = 19)\nmy.constants <- list(released = 57)\nmy.data <- list(survived = 19)\ninitial.values <- function() list(theta = runif(1,0,1))\ninitial.values()\n## $theta\n## [1] 0.2046\nparameters.to.save <- c(\"theta\")\nn.iter <- 5000\nn.burnin <- 1000\nn.chains <- 2\nmcmc.output <- nimbleMCMC(code = model,\n                          data = my.data,\n                          constants = my.constants,\n                          inits = initial.values,\n                          monitors = parameters.to.save,\n                          niter = n.iter,\n                          nburnin = n.burnin,\n                          nchains = n.chains)\n## |-------------|-------------|-------------|-------------|\n## |-------------------------------------------------------|\n## |-------------|-------------|-------------|-------------|\n## |-------------------------------------------------------|"},{"path":"intronimble.html","id":"post-process-mcmc-outputs-by-hand","chapter":"2 NIMBLE tutorial","heading":"2.3.1 Post-process MCMC outputs by hand","text":"","code":"\nstr(mcmc.output)\n## List of 2\n##  $ chain1: num [1:4000, 1] 0.36 0.432 0.374 0.374 0.412 ...\n##   ..- attr(*, \"dimnames\")=List of 2\n##   .. ..$ : NULL\n##   .. ..$ : chr \"theta\"\n##  $ chain2: num [1:4000, 1] 0.325 0.325 0.384 0.384 0.475 ...\n##   ..- attr(*, \"dimnames\")=List of 2\n##   .. ..$ : NULL\n##   .. ..$ : chr \"theta\"\nhead(mcmc.output$chain1)\n##       theta\n## [1,] 0.3604\n## [2,] 0.4323\n## [3,] 0.3739\n## [4,] 0.3739\n## [5,] 0.4125\n## [6,] 0.4125"},{"path":"intronimble.html","id":"post-process-mcmc-outputs-without-pain","chapter":"2 NIMBLE tutorial","heading":"2.3.2 Post-process MCMC outputs without pain","text":"use MCMCvis, perfectly valid options like ggmcmc basicMCMCplots.Numerical summaries.Trace posterior density","code":"\nlibrary(MCMCvis)\nMCMCsummary(mcmc.output, round = 2)\n##       mean   sd 2.5%  50% 97.5% Rhat n.eff\n## theta 0.34 0.06 0.23 0.34  0.46    1  1926\nMCMCtrace(mcmc.output,\n          pdf = FALSE)\nMCMCtrace(mcmc.output,\n          pdf = FALSE,\n          ind = TRUE,\n          Rhat = TRUE,\n          n.eff = TRUE)"},{"path":"intronimble.html","id":"syntax-whats-newbetterdifferent","chapter":"2 NIMBLE tutorial","heading":"2.4 Syntax: what’s new/better/different?","text":"basculer des trucs de speed iciVectorizationMore flexible specification distributionsYour functions distributionsThe end empty indices& …","code":"\n# JAGS (& Nimble)\nfor(t in 1:Tmax){\n  x[t] <- Mu.x + epsilon[t]\n}\n\n# Nimble\nx[1:Tmax] <- Mu.x + epsilon[1:Tmax]\n# JAGS (& Nimble)\nfor(t in 1:Tmax){\n  epsilon[t] ~ dnorm(0, tau)\n}\ntau <- pow(sigma, -2)\nsigma ~ dunif(0, 5)\n\n# Nimble\nfor(t in 1:Tmax){\n  epsilon[t] ~ dnorm(0, sd = sigma)\n}\nsigma ~ dunif(0, 5)\nx[1:Tmax] <- myNimbleFunction(a = Mu.x, b = epsilon[1:Tmax])\nsigma ~ dCustomDistr(c = 0.5, z = 10)\n# JAGS\nsum.x <- sum(x[])\n\n# Nimble\nsum.x <- sum(x[1:Tmax])"},{"path":"intronimble.html","id":"our-nimble-workflow-so-far","chapter":"2 NIMBLE tutorial","heading":"2.5 Our nimble workflow so far","text":"nimble gives full access MCMC engine","code":"\nknitr::include_graphics(\"images/nimble_workflow_sofar.png\")\nknitr::include_graphics(\"images/nimble_workflow.png\")\nknitr::include_graphics(\"images/I1bIY06.gif\")"},{"path":"intronimble.html","id":"functions","chapter":"2 NIMBLE tutorial","heading":"2.6 Functions","text":"Say want R function adds 2 every value vector.Change format vectorise.Now paramater estimate parameter R function.general need nimbleRcall like , couple considerations. common need write wrapper function, .e. function access via nimbleRcall calls actual function interest arguments return value rearranged needed. example, just need eigenvectors, wrapper function pick return . bigger issue returnType declaration: nimble type declarations include R list type declarations nimble type. think use nimbleList data structure purpose. create nimbleList type use declared returnType. still need write wrapper, convert list returned base::eigen nimbleList object return wrapper. hope makes sense.https://kenkellner.com/blog/models--integrals.htmlSame thing w/ global environment.","code":"\nadd2 <- function(x) {\n   x + 2 \n}\nRadd2 <- nimbleRcall(function(x = double(0)){}, \n                     Rfun = 'add2',\n                     returnType = double(0))\ndemoCode <- nimbleCode({\n  mu ~ dnorm(0,1)\n  for(i in 1:n) {\n    x[i] ~ dnorm(mu, sd = 1)\n    z[i] <- Radd2(x[i])\n    } \n})\n\nparam_names <- c(\"mu\", \"z\")\nmcmc.out <- nimbleMCMC(code = demoCode, \n                      constants = list(n = 4),\n                      data = list(x = c(-1, -2, 1, 2)), \n                      inits = list(mu = rnorm(1)),\n                      monitors = param_names,\n                      nchains = 2, \n                      niter = 1000,\n                      nburnin = 500)\n## |-------------|-------------|-------------|-------------|\n## |-------------------------------------------------------|\n## |-------------|-------------|-------------|-------------|\n## |-------------------------------------------------------|\nlibrary(MCMCvis)\nMCMCsummary(object = mcmc.out, round = 2)\n##      mean   sd 2.5%  50% 97.5% Rhat n.eff\n## mu   0.01 0.45 -0.9 0.03  0.91 1.02   893\n## z[1] 1.00 0.00  1.0 1.00  1.00  NaN     0\n## z[2] 0.00 0.00  0.0 0.00  0.00  NaN     0\n## z[3] 3.00 0.00  3.0 3.00  3.00  NaN     0\n## z[4] 4.00 0.00  4.0 4.00  4.00  NaN     0\nadd2 <- function(x) {\n   x + 2 \n}\nRadd2 <- nimbleRcall(function(x = double(1)){}, \n                     Rfun = 'add2',\n                     returnType = double(1))\ndemoCode <- nimbleCode({\n  mu ~ dnorm(0,1)\n  for(i in 1:n) {\n    x[i] ~ dnorm(mu, sd = 1)\n    }\n    z[1:4] <- Radd2(x[1:4])\n})\n\nparam_names <- c(\"mu\", \"z\")\nmcmc.out <- nimbleMCMC(code = demoCode, \n                      constants = list(n = 4),\n                      data = list(x = c(-1, -2, 1, 2)), \n                      inits = list(mu = rnorm(1)),\n                      monitors = param_names,\n                      nchains = 2, \n                      niter = 1000,\n                      nburnin = 500)\n## |-------------|-------------|-------------|-------------|\n## |-------------------------------------------------------|\n## |-------------|-------------|-------------|-------------|\n## |-------------------------------------------------------|\nlibrary(MCMCvis)\nMCMCsummary(object = mcmc.out, round = 2)\n##      mean   sd  2.5%  50% 97.5% Rhat n.eff\n## mu   0.03 0.44 -0.86 0.05  0.93    1  1000\n## z[1] 1.00 0.00  1.00 1.00  1.00  NaN     0\n## z[2] 0.00 0.00  0.00 0.00  0.00  NaN     0\n## z[3] 3.00 0.00  3.00 3.00  3.00  NaN     0\n## z[4] 4.00 0.00  4.00 4.00  4.00  NaN     0\nadd2 <- function(x) {\n   x + 2 \n}\nRadd2 <- nimbleRcall(function(x = double(0)){}, \n                     Rfun = 'add2',\n                     returnType = double(0))\ndemoCode <- nimbleCode({\n  mu ~ dnorm(0,1)\n  for(i in 1:n) {x[i] ~ dnorm(mu, sd = 1)} \n  z <- Radd2(mu)\n})\n\nparam_names <- c(\"mu\", \"z\")\nmcmc.out <- nimbleMCMC(code = demoCode, \n                      constants = list(n = 4),\n                      data = list(x = c(-1, -2, 1, 2)), \n                      inits = list(mu = rnorm(1)),\n                      monitors = param_names,\n                      nchains = 2, \n                      niter = 1000,\n                      nburnin = 500)\n## |-------------|-------------|-------------|-------------|\n## |-------------------------------------------------------|\n## |-------------|-------------|-------------|-------------|\n## |-------------------------------------------------------|\nlibrary(MCMCvis)\nMCMCsummary(object = mcmc.out, round = 2)\n##    mean   sd  2.5%  50% 97.5% Rhat n.eff\n## mu 0.02 0.44 -0.87 0.01   0.9    1  1158\n## z  2.02 0.44  1.13 2.01   2.9    1  1158\nlibrary(nimble)\nadd2 <- function(x) {\n   x + 2 + globvar\n}\nadd2(2)\nglobvar <- 2020\nadd2(2)\nRadd2 <- nimbleRcall(function(x = double(0)){}, \n                     Rfun = 'add2',\n                     returnType = double(0))\ndemoCode <- nimbleCode({\n  mu ~ dnorm(0,1)\n  for(i in 1:n) {x[i] ~ dnorm(mu, sd = 1)} \n  z <- Radd2(mu)\n})\n\nparam_names <- c(\"mu\", \"z\")\nmcmc.out <- nimbleMCMC(code = demoCode, \n                       constants = list(n = 4),\n                       data = list(x = c(-1, -2, 1, 2)), \n                       inits = list(mu = rnorm(1)),\n                       monitors = param_names,\n                       nchains = 2, \n                       niter = 1000,\n                       nburnin = 500)\n#printErrors()\n# pb is y is not recognized\n#ls()\n# assign y to global env\n# https://stackoverflow.com/questions/9726705/assign-multiple-objects-to-globalenv-from-within-a-function\n#assign(\"globvar\", 20, envir = .GlobalEnv)\n\nlibrary(MCMCvis)\nMCMCsummary(object = mcmc.out, round = 2)"},{"path":"intronimble.html","id":"code-your-own-sampler","chapter":"2 NIMBLE tutorial","heading":"2.7 Code your own sampler","text":"","code":"\nlibrary(nimble)\nload('matos/ressources-chapters/nimble/dipper_data.Rdata')\n\ndipperCode <- nimbleCode({\n    logit.p ~ dnorm(0, 0.001)\n    logit.phi ~ dnorm(0, 0.001)\n    p <- expit(logit.p)\n    phi <- expit(logit.phi)\n    ##phi ~ dunif(0, 1)\n    ##p ~ dunif(0, 1)\n    for(i in 1:N) {\n        x[i, first[i]] <- 1\n        y[i, first[i]] <- 1\n        for(t in (first[i]+1):T) {\n            x[i, t] ~ dbern(phi * x[i, t-1])\n            y[i, t] ~ dbern(p * x[i, t])\n        }\n    }\n})\n\nN <- dim(sightings)[1]\nT <- dim(sightings)[2]\ndipperConsts <- list(N = N, T = T, first = first)\ndipperData <- list(y = sightings)\nxInit <- ifelse(!is.na(sightings), 1, 0)\ndipperInits <- list(logit.phi = 0, logit.p = 0, x = xInit)\n\nsamples <- nimbleMCMC(dipperCode, dipperConsts, dipperData, dipperInits,\n                      niter = 10000, nburnin = 5000,\n                      monitors = c('p', 'phi'))\n\nmy_MH <- nimbleFunction(\n    name = 'my_MH',\n    contains = sampler_BASE,\n    setup = function(model, mvSaved, target, control) {\n        calcNodes <- model$getDependencies(target)\n        scale <- control$scale\n    },\n    run = function() {\n        initialLP <- model$getLogProb(calcNodes)\n        current <- model[[target]]\n        proposal <- rnorm(1, current, scale)\n        model[[target]] <<- proposal\n        proposalLP <- model$calculate(calcNodes)\n        lMHR <- proposalLP - initialLP\n        if(runif(1,0,1) < exp(lMHR)) {\n            ## accept\n            copy(from = model, to = mvSaved, nodes = calcNodes, logProb = TRUE, row = 1)\n        } else {\n            ## reject\n            copy(from = mvSaved, to = model, nodes = calcNodes, logProb = TRUE, row = 1)\n        }\n    },\n    methods = list(\n        reset = function() {}\n    )\n)\n\nscale <- 0.05\n\nRmodel <- nimbleModel(dipperCode, dipperConsts, dipperData, dipperInits)\nconf <- configureMCMC(Rmodel, monitors = c('p', 'phi'))\nconf$printSamplers()\nconf$printSamplers(byType = TRUE)\nconf$removeSamplers(c('logit.p', 'logit.phi'))\nconf$addSampler(target = 'logit.p', type = 'my_MH', control = list(scale = scale))\nconf$addSampler(target = 'logit.phi', type = 'my_MH', control = list(scale = scale))\nconf$printSamplers()\nconf$printMonitors()\nRmcmc <- buildMCMC(conf)\n\nout <- compileNimble(list(model=Rmodel, mcmc=Rmcmc))\nCmcmc <- out$mcmc\n\nsamples2 <- runMCMC(Cmcmc, niter = 10000, nburnin = 5000)\n\nsamplesSummary(samples2)\n\nbasicMCMCplots::chainsPlot(samples2)"},{"path":"intronimble.html","id":"a-dire-quelque-part","chapter":"2 NIMBLE tutorial","heading":"2.8 A dire quelque part?","text":"Pourquoi Nimble plutôt que Stan? Syntaxe BUGS, also discrete latent states easier deal , need marginalise. Stan marginalise (ref forward relevant section book), difficult endeavour, need NIMBLE, algorithms work fine discrete latent states.","code":""},{"path":"intronimble.html","id":"when-things-go-wrong-tip-and-tricks","chapter":"2 NIMBLE tutorial","heading":"2.9 When things go wrong: Tip and tricks","text":"","code":""},{"path":"intronimble.html","id":"full-potential","chapter":"2 NIMBLE tutorial","heading":"2.10 Full potential","text":"’re exactly right, Keith - options available using nimbleMCMC. nimbleMCMC interface designed running “default” MCMC, involves customization MCMC sampling algorithms . want delve deeper customizing MCMC sampling strategies, ’re right, workflow use :nimbleModel\nconfigureMCMC\nbuildMCMC\ncompileNimble\nrunMCMCI know steps single call nimbleMCMC, changes using nimbleMCMC functional workflow quite mechanical. direction /various arguments functions organized, just let know - problem .","code":""},{"path":"intronimble.html","id":"summary-1","chapter":"2 NIMBLE tutorial","heading":"2.11 Summary","text":"Blabla.Blabla.Reblabla.Reblabla.","code":""},{"path":"intronimble.html","id":"suggested-reading-1","chapter":"2 NIMBLE tutorial","heading":"2.12 Suggested reading","text":"Official website https://r-nimble.orgOfficial website https://r-nimble.orgUser Manual https://r-nimble.org/html_manual/cha-welcome-nimble.html cheatsheet.User Manual https://r-nimble.org/html_manual/cha-welcome-nimble.html cheatsheet.Users mailing list https://groups.google.com/forum/#!forum/nimble-usersUsers mailing list https://groups.google.com/forum/#!forum/nimble-usersTraining material https://github.com/nimble-trainingTraining material https://github.com/nimble-trainingReference cite using nimble publication:Reference cite using nimble publication:de Valpine, P., D. Turek, C. J. Paciorek, C. Anderson-Bergman, D. Temple Lang, R. Bodik (2017). Programming Models: Writing Statistical Algorithms General Model Structures NIMBLE. Journal Computational Graphical Statistics 26 (2): 403–13.","code":""},{"path":"hmmcapturerecapture.html","id":"hmmcapturerecapture","chapter":"3 Hidden Markov models","heading":"3 Hidden Markov models","text":"–>\n –>–>\n –>\n –>\n –>\n –>","code":""},{"path":"introduction-3.html","id":"introduction-3","chapter":"Introduction","heading":"Introduction","text":"","code":""},{"path":"survival.html","id":"survival","chapter":"4 Survival","heading":"4 Survival","text":"–>\n –>–>–>–>–>\n –>\n –>–>–>\n –>\n –>\n –>","code":""},{"path":"covariates.html","id":"covariates","chapter":"5 Covariates","heading":"5 Covariates","text":"","code":""},{"path":"dispersal.html","id":"dispersal","chapter":"6 Dispersal","heading":"6 Dispersal","text":"","code":""},{"path":"model-selection.html","id":"model-selection","chapter":"7 Model selection and validation","heading":"7 Model selection and validation","text":"","code":""},{"path":"introduction-4.html","id":"introduction-4","chapter":"Introduction","heading":"Introduction","text":"","code":""},{"path":"uncertainty.html","id":"uncertainty","chapter":"8 State uncertainty","heading":"8 State uncertainty","text":"","code":""},{"path":"hsmm.html","id":"hsmm","chapter":"9 Hidden semi-Markov models","heading":"9 Hidden semi-Markov models","text":"","code":""},{"path":"introduction-5.html","id":"introduction-5","chapter":"Introduction","heading":"Introduction","text":"","code":""},{"path":"tradeoffs.html","id":"tradeoffs","chapter":"10 Life history theory","heading":"10 Life history theory","text":"","code":""},{"path":"tradeoffs.html","id":"tradeoffs-1","chapter":"10 Life history theory","heading":"10.1 Tradeoffs","text":"Morano et al. (2013), Shefferson et al. (2003), Cruz-Flores et al. (n.d.)","code":""},{"path":"tradeoffs.html","id":"breeding-dynamics","chapter":"10 Life history theory","heading":"10.2 Breeding dynamics","text":"Pradel, Choquet, Béchet (2012), Desprez et al. (2011), Desprez et al. (2013), Pacoureau et al. (2019)","code":""},{"path":"tradeoffs.html","id":"actuarial-senescence","chapter":"10 Life history theory","heading":"10.3 Actuarial senescence","text":"Choquet et al. (2011), Péron et al. (2016)","code":""},{"path":"tradeoffs.html","id":"cause-specific-mortalities","chapter":"10 Life history theory","heading":"10.4 Cause-specific mortalities","text":"Fernández-Chacón et al. (2016) Ruette et al. (2015)","code":""},{"path":"tradeoffs.html","id":"disease-dynamics","chapter":"10 Life history theory","heading":"10.5 Disease dynamics","text":"Marescot et al. (2018) Santoro et al. (2014)","code":""},{"path":"tradeoffs.html","id":"sex-uncertainty","chapter":"10 Life history theory","heading":"10.6 Sex uncertainty","text":"Pradel et al. (2008) Genovart, Pradel, Oro (2012)","code":""},{"path":"abundance.html","id":"abundance","chapter":"11 Abundance","heading":"11 Abundance","text":"","code":""},{"path":"abundance.html","id":"horvitz-thompson","chapter":"11 Abundance","heading":"11.1 Horvitz-Thompson","text":"Santostasi et al. (2019)","code":""},{"path":"abundance.html","id":"jolly-seber","chapter":"11 Abundance","heading":"11.2 Jolly-Seber","text":"","code":""},{"path":"abundance.html","id":"robust-design","chapter":"11 Abundance","heading":"11.3 Robust design","text":"Karamanlidis et al. (2015), Santostasi et al. (2016), Gibson et al. (2018), Rankin et al. (2016)","code":""},{"path":"stopover.html","id":"stopover","chapter":"12 Stopover duration","heading":"12 Stopover duration","text":"Guérin et al. (2017)","code":""},{"path":"individual-dependence.html","id":"individual-dependence","chapter":"13 Individual dependence","heading":"13 Individual dependence","text":"","code":""},{"path":"individual-dependence.html","id":"dependence-among-individuals","chapter":"13 Individual dependence","heading":"13.1 Dependence among individuals","text":"Culina et al. (2013) Cubaynes et al. (2021)","code":""},{"path":"individual-dependence.html","id":"individual-heterogeneity","chapter":"13 Individual dependence","heading":"13.2 Individual heterogeneity","text":"Cubaynes et al. (2010), Gimenez Choquet (2010), Turek, Wehrhahn, Gimenez (2021)","code":""},{"path":"take-home-messages.html","id":"take-home-messages","chapter":"Take-home messages","heading":"Take-home messages","text":"–>\n –>–>–>–>","code":""},{"path":"faq.html","id":"faq","chapter":"FAQ","heading":"FAQ","text":"","code":""},{"path":"references.html","id":"references","chapter":"References","heading":"References","text":"","code":""}]
