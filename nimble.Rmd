# NIMBLE tutorial {#intronimble}

## Introduction

In this second chapter, you will get familiar with NIMBLE, an R package that implements up-to-date MCMC algorithms for fitting complex models. NIMBLE spares you from coding the MCMC algorithms by hand, and requires only the specification of a likelihood and priors for model parameters. We will illustrate NIMBLE main features with a simple example, but the ideas hold for other problems.

**go through https://r-nimble.org/documentation-2 and https://r-nimble.org/examples**

## What is NIMBLE?

NIMBLE stands for **N**umerical **I**nference for statistical **M**odels using **B**ayesian and **L**ikelihood **E**stimation. Briefly speaking, NIMBLE is an R package that implements for you MCMC algorithms to generate samples from the posterior distribution of model parameters. Freed from the burden of coding your own MCMC algorithms, you only have to specify a likelihood and priors to apply the Bayes theorem. To do so, NIMBLE uses a syntax very similar to the R syntax, which should make your life easier. This so-called BUGS language is also used by other programs like WinBUGS, OpenBUGS, and JAGS. 

So why use NIMBLE you may ask? The short answer is that NIMBLE is capable of so much more! First, you will work from within R, but in the background NIMBLE will translate your code in C++ for (in general) faster computation. Second, NIMBLE extends the BUGS language for writing new functions and statistical distributions of your own, or borrow those written by others. Third, NIMBLE gives you full control of the MCMC samplers, and you may pick other algorithms than the defaults. Fourth, NIMBLE comes with a library of numerical methods other than MCMC algorithms, including sequential Monte Carlo (for particle filtering) and Monte Carlo Expectation Maximization (for maximum likelihood). Last but not least, the development team is friendly and helpful, and based on users' feedbacks, NIMBLE folks work constantly at improving the package capabilities. 

## Getting started

```{block2 nimble_workflow, type='rmdnote', eval = FALSE}
To run NIMBLE, you will need to:  
1. Build a model consisting of a likelihood and priors.   
2. Read in some data.   
3. Specify parameters you want to make inference about.   
4. Pick initial values for parameters to be estimated (for each chain).   
5. Provide MCMC details namely the number of chains, the length of the burn-in period and the number of iterations following burn-in.
```

First things first, let's not forget to load the `nimble` package:
```{r}
library(nimble)
```

Now let's go back to our example on animal survival from the previous chapter. First step is to build our model by specifying the binomial likelihood and a uniform prior on survival probability `theta`. We use the `nimbleCode()` function and wrap code within curly brackets:
```{r}
model <- nimbleCode({
  # likelihood
  survived ~ dbinom(theta, released)
  # prior
  theta ~ dunif(0, 1)
  # derived quantity
  lifespan <- -1/log(theta)
})
```

You can check that the `model` R object contains your code:
```{r}
model
```

In the code above, `survived` and `released` are known, only `theta` needs to be estimated. The line `survived ~ dbinom(theta, released)` states that the number of successes or animals that have survived over winter  `survived` is distributed as (that's the `~`) as a binomial with `released` trials and probability of success or survival `theta`. Then the line `theta ~ dunif(0, 1)` assigns a uniform between 0 and 1 as a prior distribution to the survival probability. This is all you need, a likelihood and priors for model parameters, NIMBLE knows the Bayes theorem. The last line `lifespan <- - 1/log(theta)` calculates a quantity derived from `theta`, which is the expected lifespan assuming constant survival^[Cook LM, Brower LP, Croze HJ (1967) The accuracy of a population estimation from multiple recapture data. J Anim Ecol 36:57–60].

A few comments:    

+ The most common distributions are available in NIMBLE. Among others, we will use later in the book `dbeta`, `dmultinom` and `dnorm`. If you cannot find what you need in NIMBLE, you can write your own distribution as illustrated in Section \@ref(write-your-own-distribution).

+ It does not matter in what order you write each line of code, NIMBLE uses what is called a declarative language for building models. In brief, you write code that tells NIMBLE what you want to achieve, and not how to get there. In contrast, an imperative language requires that you write what you want your program to do step by step.

+ You can think of models in NIMBLE as graphs as in Figure \@ref(fig:dag-survival). A graph is made of relations (or edges) that can be of two types. A stochastic relation is signaled by a `~` sign and defines a random variable in the model, such as `survived` or `theta`. A deterministic relation is signaled by a `<-` sign, like `lifespan`. Relations define nodes on the left - the children - in terms of other nodes on the right - the parents, and relations are directed edges from parents to children. Such graphs are called directed acyclic graph or DAG.
```{r dag-survival, echo = FALSE, fig.cap = "Graph of the animal survival model. Survived is a stochastic node defined by its parents `released` and `theta`, while `lifespan` is a deterministic node the value of which is defined exactly by the value of its parent `theta`."}
mc <- nimbleModel(model, data = list(released = 57, survived = 19))
#mc$getVarNames()
#mc$getNodeNames()
#mc$getNodeNames(determOnly = TRUE)
#mc$getNodeNames(stochOnly = TRUE)
#mc$getNodeNames(dataOnly = TRUE)
#mc$getDependencies("theta")
mc$plotGraph()
```

Second step in our workflow is to read in some data. We use a list in which each component corresponds to a known quantity in the model:
```{r}
my.data <- list(released = 57, survived = 19)
```

You can proceed with data passed this way, but you should know a little more about how NIMBLE sees data. NIMBLE distinguishes data and constants. Constants are values that do not change, e.g. vectors of known index values or the indices used to define for-loops. Data are values that you might want to change, basically anything that only appears on the left of a `~`. Declaring relevant values as constants is better for computational efficiency, but it is easy to forget, and fortunately NIMBLE will by itself distinguish data and constants. **More details? Forward reference to an example with constants?**

<!-- In passing say that full indexing is needed, you cannot let NIMBLE guess dimensions. -->
<!-- Ici on reprend le modèle simple du dessus, et on l'exprime un peu différemment pour illustrer qqs autres features de NIMBLE: i) loops, ii) distinction between constants and data. The binomial is a sum of independent Bernoulli outcomes with same probability. the Like flipping a coin for each individual and get a survivor with prob theta. Here survived is. Going back to our animal survival example, it means that the likelihood can be written as a Bernoulli random variable taking value 1 if animal survived, and 0 otherwise. **Voir dans annexe Hobbs**. E.g. `survived[1] ~ dbern(theta)` up to `survived[59] ~ dbern(theta)`. Likelihood contribution of individuals. Loops is the product. Iid. Instead of duplicating the same line of code `survived[i] ~ dbern(theta)` we use a loop. -->
<!-- ```{r} -->
<!-- model <- nimbleCode({ -->
<!--   # likelihood -->
<!--   for (i in 1:released){ -->
<!--     survived[i] ~ dbern(theta) -->
<!--   } -->
<!--   # prior -->
<!--   theta ~ dunif(0, 1) -->
<!-- }) -->
<!-- ``` -->

<!-- **If you try nimbleMCMC it won't work. This is because we ned to distinguish data from constants. Uncomment code.** -->

<!-- Distinguih constants and data. To Nimble, not all "data" is data... -->
<!-- ```{r} -->
<!-- my.constants <- list(released = 57) -->
<!-- my.data <- list(survived = 19) -->
<!-- ``` -->
<!-- ```{r, eval = FALSE} -->
<!-- mcmc.output <- nimbleMCMC(code = model, -->
<!--                           data = my.data, -->
<!--                           constants = my.constants, -->
<!--                           inits = initial.values, -->
<!--                           monitors = parameters.to.save, -->
<!--                           niter = n.iter, -->
<!--                           nburnin = n.burnin, -->
<!--                           nchains = n.chains) -->
<!-- ``` -->


<!-- ```{r} -->
<!-- my.data <- list(survived = c(rep(1,19), rep(0,57-19))) -->
<!-- my.constants <- list(released = 57) -->
<!-- ``` -->

<!-- The rest is the same. Steps 3, 4 and 5. -->
<!-- ```{r} -->
<!-- parameters.to.save <- c("theta", "life_expectancy") -->
<!-- initial.values <- function() list(theta = runif(1,0,1)) -->
<!-- n.iter <- 5000 -->
<!-- n.burnin <- 1000 -->
<!-- n.chains <- 3 -->
<!-- ``` -->

<!-- Run model, add argument `constants = my.constants`. -->
<!-- ```{r} -->
<!-- mcmc.output <- nimbleMCMC(code = model, -->
<!--                           data = my.data, -->
<!--                           constants = my.constants, -->
<!--                           inits = initial.values, -->
<!--                           monitors = parameters.to.save, -->
<!--                           niter = n.iter, -->
<!--                           nburnin = n.burnin, -->
<!--                           nchains = n.chains) -->
<!-- ``` -->

Third step is to tell NIMBLE which nodes in your model you would like to keep track of, in other words the quantities you'd like to do inference about. In our model we want survival `theta` and `lifespan`:
```{r}
parameters.to.save <- c("theta", "lifespan")
```

In general you have many quantities in your model, including some of little interest that are not worth displaying, and having full control on verbosity will prove handy.

Fourth step is to specify initial values for all model parameters. To make sure that the MCMC algorithm explores the posterior distribution, we start different chains with different parameter values. You can specify initial values for each chain in a list and put them in yet another list:
```{r}
init1 <- list(theta = 0.1)
init2 <- list(theta = 0.5)
init3 <- list(theta = 0.9)
initial.values <- list(init1, init2, init3)
initial.values
```

Alternatively, you can write a simple R function that generates random initial values:
```{r}
initial.values <- function() list(theta = runif(1,0,1))
initial.values()
```

Firth and last step, you need to tell NIMBLE the number of chains to run, say `n.chain`, how long the burn-in period should be, say `n.burnin`, and the number of iterations following the burn-in period to be used for posterior inference. In NIMBLE, you specify the total number of iterations, say `n.iter`, so that the number of posterior samples per chain is `n.iter - n.burnin`. NIMBLE also allows discarding samples after burn-in, a procedure known as thinning, which I will not use in this book^[Link, W.A. and Eaton, M.J. (2012), On thinning of chains in MCMC. Methods in Ecology and Evolution, 3: 112-115.]
```{r}
n.iter <- 5000
n.burnin <- 1000
n.chains <- 3
```

We now have all the ingredients to run model, that is to sample in the posterior distribution of model parameters using MCMC simulations. This is accomplished using function `nimbleMCMC()`: 
```{r, cache = TRUE}
mcmc.output <- nimbleMCMC(code = model,
                          data = my.data,
                          inits = initial.values,
                          monitors = parameters.to.save,
                          niter = n.iter,
                          nburnin = n.burnin,
                          nchains = n.chains)
```

NIMBLE goes through several steps that we will explain in Section \@ref(advanced-stuff). Function `nimbleMCMC()` takes other arguments that you might find useful. For example, you can suppress the progress bar if you find it to depressing when running long simulations with `progressBar = FALSE`. You can also get a summary of the outputs by specifying `summary = TRUE`. Check `?nimbleMCMC` for more details. 

Now let's inspect what we have in `mcmc.output`. 
```{r}
str(mcmc.output)
```

The R object `mcmc.output` is a list with three components, one for each MCMC chain. Let's have a look to `chain1` for example.
```{r}
dim(mcmc.output$chain1)
head(mcmc.output$chain1)
```

Each component of the list is a matrix. In rows, you have `r dim(mcmc.output$chain1)[1]` samples from the posterior distribution of `theta`, which corresponds to `n.iter - n.burnin`. In columns, you have the quantities we monitor, `theta` and `lifespan`. From there, you can compute the posterior mean of `theta`:
```{r}
mean(mcmc.output$chain1[,'theta'])
```

You can also obtain the 95% credible interval for `theta`:
```{r}
quantile(mcmc.output$chain1[,'theta'], probs = c(2.5, 97.5)/100)
```

Let's visualise the posterior distribution of `theta` with a histogram: 
```{r}
mcmc.output %>%
  as_tibble() %>%
  ggplot() +
  geom_histogram(aes(x = chain1[,"theta"]), color = "white") +
  labs(x = "survival probability")
```

There are less painful ways of doing posterior inference. In this book, I will use the R package `MCMCvis`^[https://github.com/caseyyoungflesh/MCMCvis] to summarise and visualize MCMC outputs, but there are other perfectly valid options out there like `ggmcmc`^[Fernández-i-Marín, X. (2016). ggmcmc: Analysis of MCMC Samples and Bayesian Inference. Journal of Statistical Software, 70(9), 1–20] and `basicMCMCplots`^[https://cran.r-project.org/web/packages/basicMCMCplots/index.html]. **Shall I demonstrate these other options?.**

<!-- Finally we want to look at our samples. NIMBLE returns samples as a simple matrix with named columns. There are numerous packages for processing MCMC output. If you want to use the coda package, you can convert a matrix to a coda mcmc object like this: -->
<!-- library(coda) -->
<!-- coda.samples <- as.mcmc(samples) -->
<!-- Alternatively, if you call nimbleMCMC with the argument samplesAsCodaMCMC = TRUE, the samples will be returned as a coda object. -->

Let's load the package `MCMCvis`:
```{r}
library(MCMCvis)
```

To get the most common numerical summaries, the function `MCMCsummary()` does the job:
```{r}
MCMCsummary(object = mcmc.output, round = 2)
```

You can use a caterpillar plot to visualise the posterior distributions of `theta` with `MCMCplot()`:
```{r}
MCMCplot(object = mcmc.output, 
         params = 'theta')
```

The point represents the posterior median, the thick line is the 50% credible interval and the thin line the 95% credible interval. 

The trace and posterior density of theta can be obtained with `MCMCtrace()`:
```{r}
MCMCtrace(object = mcmc.output,
          pdf = FALSE, # no export to PDF
          ind = TRUE, # separate density lines per chain
          params = "theta")
```

You can also add the diagnostics of convergence we discussed in the previous chapter:
```{r}
MCMCtrace(object = mcmc.output,
          pdf = FALSE,
          ind = TRUE,
          Rhat = TRUE, # add Rhat
          n.eff = TRUE, # add eff sample size
          params = "theta")
```

We calculated lifespan directly in our model with `lifespan <- -1/log(theta)`. But you can also calculate this quantity from outside NIMBLE. This is a nice by-product of using MCMC simulations: you can obtain the posterior distribution of any quantity that is function of your model parameters by applying this function to samples from the posterior distribution of these parameters. In our example, all you need is samples from the posterior distribution of `theta`, which we pool between the three chains with:
```{r}
theta_samples <- c(mcmc.output$chain1[,'theta'], 
                   mcmc.output$chain2[,'theta'],
                   mcmc.output$chain3[,'theta'])
```

To get samples from the posterior distribution of lifespan, we apply the function to calculate lifespan to the samples from the posterior distribution of survival:
```{r}
lifespan <- -1/log(theta_samples)
```

As usual then, you can calculate the posterior mean and 95% credible interval:
```{r}
mean(lifespan)
quantile(lifespan, probs = c(2.5, 97.5)/100)
```

You can also visualise the posterior distribution of lifespan:
```{r}
lifespan %>%
  as_tibble() %>%
  ggplot() +
  geom_histogram(aes(x = value), color = "white") +
  labs(x = "lifespan")
```

## Advanced stuff {#advanced-stuff}

**Move material from Speed slides here. Say that this section can be skipped.**

### Vectorization
```{r, eval = FALSE}
# JAGS (& Nimble)
for(t in 1:Tmax){
  x[t] <- Mu.x + epsilon[t]
}

# Nimble
x[1:Tmax] <- Mu.x + epsilon[1:Tmax]
```

```{r eval = FALSE}
model <- nimbleCode({
  # likelihood
  survived[1:released] ~ dbern(theta)
  # prior
  theta ~ dunif(0, 1)
  life_expectancy <- -1/log(theta)
})
```


### Flexible specification of distributions
```{r, eval = FALSE}
# JAGS (& Nimble)
for(t in 1:Tmax){
  epsilon[t] ~ dnorm(0, tau)
}
tau <- pow(sigma, -2)
sigma ~ dunif(0, 5)

# Nimble
for(t in 1:Tmax){
  epsilon[t] ~ dnorm(0, sd = sigma)
}
sigma ~ dunif(0, 5)
```

+ Your own functions and distributions
```{r, eval = FALSE}
x[1:Tmax] <- myNimbleFunction(a = Mu.x, b = epsilon[1:Tmax])
```

```{r, eval = FALSE}
sigma ~ dCustomDistr(c = 0.5, z = 10)
```

+ The end of empty indices
```{r, eval = FALSE}
# JAGS
sum.x <- sum(x[])

# Nimble
sum.x <- sum(x[1:Tmax])
```

Your own distribution is outside the scope of this book, reference NIMBLE manual.

### Behind the hood

Our workflow so far.

```{r}
knitr::include_graphics("images/nimble_workflow_sofar.png")
```

But `nimble` gives full access to the MCMC engine

```{r}
knitr::include_graphics("images/nimble_workflow.png")
```

The `nimbleMCMC()` interface runs the "default" MCMC, which precludes any customization of the MCMC sampling algorithms. This is fine for most use, but in some situations useful to know what's behind the hood. The workflow to use is: `nimbleModel()`, `configureMCMC()`, `buildMCMC()`, `compileNimble()` and `runMCMC()`. Why is it useful? **Go through example again. Show how to debug. Move stuff from Speed here.**

### Call R functions within NIMBLE

Say we want an R function that adds 2 to every value in a vector.
```{r}
calculate_life_expectancy <- function(x) {
   -1/log(x)
}
```

This function can be any function from R packages, easier to call with `name_package::my_function()`.

Now a wrapper. 
```{r}
Rcalculate_life_expectancy <- nimbleRcall(function(x = double(0)){}, 
                     Rfun = 'calculate_life_expectancy',
                     returnType = double(0))
```

**Explain format. double(0), double(1), double(2). Best advice is to have a look to other functions.**

```{r}
model <- nimbleCode({
  # likelihood
  for (i in 1:released){
    survived[i] ~ dbern(theta)
  }
  # prior
  theta ~ dunif(0, 1)
  life_expectancy <- Rcalculate_life_expectancy(theta)
})
```
Comment on the call to function `Rcalculate_life_expectancy()`.

Rest is unchanged.
```{r}
my.data <- list(survived = c(rep(1,19), rep(0,57-19)))
my.constants <- list(released = 57)
parameters.to.save <- c("theta", "life_expectancy")
initial.values <- function() list(theta = runif(1,0,1))
n.iter <- 5000
n.burnin <- 1000
n.chains <- 3
mcmc.output <- nimbleMCMC(code = model,
                          data = my.data,
                          constants = my.constants,
                          inits = initial.values,
                          monitors = parameters.to.save,
                          niter = n.iter,
                          nburnin = n.burnin,
                          nchains = n.chains)
```

Display outputs. 
```{r}
MCMCsummary(object = mcmc.output, round = 2)
```

Helpful when object format unusual for NIMBLE, like spatial object. Global environment. **Uncomment for more details**
<!-- "In general if you do need a nimbleRcall like this, there are a couple of considerations. It is common to need to write a wrapper function, i.e. a function you access via nimbleRcall that calls your actual function of interest with arguments and then return value rearranged as needed. For example, if you just need the eigenvectors, a wrapper function could pick those out and return them.  The bigger issue is the returnType declaration: nimble type declarations do not include an R list of type declarations as a nimble type.  I think you could use a nimbleList data structure for this purpose.  You would have to create a nimbleList type and then use that as the declared returnType.  But you would still need to write a wrapper, so that you could convert the list returned from base::eigen into a nimbleList object to return from your wrapper.". See also https://kenkellner.com/blog/models-with-integrals.html. -->

### Code your own sampler

```{r}
model <- nimbleCode({
  # likelihood
  for (i in 1:released){
    survived[i] ~ dbern(theta)
  }
  # prior
  theta ~ dunif(0, 1)
  life_expectancy <- Rcalculate_life_expectancy(theta)
})
my.data <- list(survived = c(rep(1,19), rep(0,57-19)))
my.constants <- list(released = 57)
parameters.to.save <- c("theta", "life_expectancy")
initial.values <- function() list(theta = runif(1,0,1))
n.iter <- 5000
n.burnin <- 1000
n.chains <- 3
mcmc.output <- nimbleMCMC(code = model,
                          data = my.data,
                          constants = my.constants,
                          inits = initial.values,
                          monitors = parameters.to.save,
                          niter = n.iter,
                          nburnin = n.burnin,
                          nchains = n.chains)
MCMCsummary(object = mcmc.output, round = 2)
```

Your own Metropolis. **Introduce nimbleFunction earlier https://r-nimble.org/html_manual/cha-user-defined.html#sec:user-functions**
```{r}
my_metropolis <- nimbleFunction(
  name = 'my_metropolis',
  contains = sampler_BASE,
  setup = function(model, mvSaved, target, control) {
    calcNodes <- model$getDependencies(target)
    scale <- control$scale
  },
  run = function() {
    initialLP <- model$getLogProb(calcNodes)
    current <- model[[target]]
    lcurrent <- log(current / (1 - current))
    lproposal <- lcurrent  + rnorm(1, 0, scale)
    proposal <- plogis(lproposal)
    model[[target]] <<- proposal
    proposalLP <- model$calculate(calcNodes)
    lMHR <- proposalLP - initialLP
    if(runif(1,0,1) < exp(lMHR)) {
      ## accept
      copy(from = model, to = mvSaved, nodes = calcNodes, logProb = TRUE, row = 1)
    } else {
      ## reject
      copy(from = mvSaved, to = model, nodes = calcNodes, logProb = TRUE, row = 1)
    }
  },
  methods = list(
    reset = function() {}
  )
)
```

Parameter to tune. Seek equivalent in previous chapter.
```{r}
scale <- 0.05
```

Detailed workflow.
```{r}
Rmodel <- nimbleModel(model, my.constants, my.data, initial.values())
conf <- configureMCMC(Rmodel, monitors = c('theta'))
conf$printSamplers()
conf$printSamplers(byType = TRUE)
conf$removeSamplers(c('theta'))
conf$addSampler(target = 'theta', type = 'my_metropolis', control = list(scale = scale))
conf$printSamplers()
conf$printMonitors()
Rmcmc <- buildMCMC(conf)
out <- compileNimble(list(model = Rmodel, mcmc = Rmcmc))
Cmcmc <- out$mcmc
samples <- runMCMC(Cmcmc, niter = 5000, nburnin = 1000)
samplesSummary(samples)
#chainsPlot(samples)
```

Change parameter to tune.
```{r}
scale <- 2
Rmodel <- nimbleModel(model, my.constants, my.data, initial.values())
conf <- configureMCMC(Rmodel, monitors = c('theta'))
conf$printSamplers()
conf$printSamplers(byType = TRUE)
conf$removeSamplers(c('theta'))
conf$addSampler(target = 'theta', type = 'my_metropolis', control = list(scale = scale))
conf$printSamplers()
conf$printMonitors()
Rmcmc <- buildMCMC(conf)
out <- compileNimble(list(model = Rmodel, mcmc = Rmcmc))
Cmcmc <- out$mcmc
samples <- runMCMC(Cmcmc, niter = 5000, nburnin = 1000)
samplesSummary(samples)
#chainsPlot(samples)
```

Draw parallel with previous chapter. 

### Write your own distribution {#write-your-own-distribution}

Example from https://r-nimble.org/html_manual/cha-user-defined.html#sec:user-distributions. Recode the beta distribution? Have a look to the raw NIMBLE code.

## When things go wrong: Tip and tricks

Main problems. Call to community.

## Summary

+ Blabla.

+ Reblabla. 

## Suggested reading

+ Official website [https://r-nimble.org](https://r-nimble.org)

+ User Manual [https://r-nimble.org/html_manual/cha-welcome-nimble.html](https://r-nimble.org/html_manual/cha-welcome-nimble.html) and [cheatsheet](https://r-nimble.org/cheatsheets/NimbleCheatSheet.pdf).

+ Users mailing list [https://groups.google.com/forum/#!forum/nimble-users](https://groups.google.com/forum/#!forum/nimble-users)

+ Training material [https://github.com/nimble-training](https://github.com/nimble-training)

+ Reference to cite when using nimble in a publication:

> de Valpine, P., D. Turek, C. J. Paciorek, C. Anderson-Bergman, D. Temple Lang, and R. Bodik (2017). [Programming With Models: Writing Statistical Algorithms for General Model Structures With NIMBLE](https://arxiv.org/pdf/1505.05093.pdf). *Journal of Computational and Graphical Statistics* **26** (2): 403–13.


<!-- ## A dire quelque part? -->

<!-- Pourquoi Nimble plutôt que Stan? Syntaxe BUGS, also discrete latent states easier to deal with, no need to marginalise. In Stan you marginalise (ref forward to relevant section of the book), but difficult endeavour, and you do not need to do that with NIMBLE, it has algorithms that work fine with discrete latent states.  -->


