# NIMBLE tutorial {#intronimble}

## Introduction

In this second chapter, you will get familiar with NIMBLE, an R package that implements up-to-date MCMC algorithms for fitting complex models. NIMBLE spares you from coding the MCMC algorithms by hand, and requires only the specification of a likelihood and priors for model parameters. We will illustrate NIMBLE main features with a simple example, but the ideas hold for other problems.

## What is NIMBLE?

NIMBLE stands for **N**umerical **I**nference for statistical **M**odels using **B**ayesian and **L**ikelihood **E**stimation. Briefly speaking, NIMBLE is an R package that implements for you MCMC algorithms to generate samples from the posterior distribution of model parameters. Freed from the burden of coding your own MCMC algorithms, you only have to specify a likelihood and priors to apply the Bayes theorem. To do so, NIMBLE uses a syntax very similar to the R syntax, which should make your life easier. This so-called BUGS language is also used by other programs like WinBUGS, OpenBUGS, and JAGS. 

So why use NIMBLE you may ask? The short answer is that NIMBLE is capable of so much more than just running MCMC algorithms! First, you will work from within R, but in the background NIMBLE will translate your code in C++ for (in general) faster computation. Second, NIMBLE extends the BUGS language for writing new functions and distributions of your own, or borrow those written by others. Third, NIMBLE gives you full control of the MCMC samplers, and you may pick other algorithms than the defaults. Fourth, NIMBLE comes with a library of numerical methods other than MCMC algorithms, including sequential Monte Carlo (for particle filtering) and Monte Carlo Expectation Maximization (for maximum likelihood). Last but not least, the development team is friendly and helpful, and based on users' feedbacks, NIMBLE folks work constantly at improving the package capabilities. 

```{r nimblelogo, echo = FALSE, fig.align="center", out.width="100%", fig.cap = "Logo of the NIMBLE R package designed by Luke Larson. **Ask Perry for context and meaning.**"}
knitr::include_graphics("images/nimble-icon.png")
```

## Getting started

```{block2 nimble_workflow, type='rmdnote', eval = FALSE}
To run NIMBLE, you will need to:  
1. Build a model consisting of a likelihood and priors.   
2. Read in some data.   
3. Specify parameters you want to make inference about.   
4. Pick initial values for parameters to be estimated (for each chain).   
5. Provide MCMC details namely the number of chains, the length of the burn-in period and the number of iterations following burn-in.
```

First things first, let's not forget to load the `nimble` package:
```{r}
library(nimble)
```

Now let's go back to our example on animal survival from the previous chapter. First step is to build our model by specifying the binomial likelihood and a uniform prior on survival probability `theta`. We use the `nimbleCode()` function and wrap code within curly brackets:
```{r}
model <- nimbleCode({
  # likelihood
  survived ~ dbinom(theta, released)
  # prior
  theta ~ dunif(0, 1)
  # derived quantity
  lifespan <- -1/log(theta)
})
```

You can check that the `model` R object contains your code:
```{r}
model
```

In the code above, `survived` and `released` are known, only `theta` needs to be estimated. The line `survived ~ dbinom(theta, released)` states that the number of successes or animals that have survived over winter  `survived` is distributed as (that's the `~`) as a binomial with `released` trials and probability of success or survival `theta`. Then the line `theta ~ dunif(0, 1)` assigns a uniform between 0 and 1 as a prior distribution to the survival probability. This is all you need, a likelihood and priors for model parameters, NIMBLE knows the Bayes theorem. The last line `lifespan <- - 1/log(theta)` calculates a quantity derived from `theta`, which is the expected lifespan assuming constant survival^[Cook LM, Brower LP, Croze HJ (1967) The accuracy of a population estimation from multiple recapture data. J Anim Ecol 36:57–60].

A few comments:    

+ The most common distributions are available in NIMBLE. Among others, we will use later in the book `dbeta`, `dmultinom` and `dnorm`. If you cannot find what you need in NIMBLE, you can write your own distribution as illustrated in Section \@ref(functions-in-nimble).

+ It does not matter in what order you write each line of code, NIMBLE uses what is called a declarative language for building models. In brief, you write code that tells NIMBLE what you want to achieve, and not how to get there. In contrast, an imperative language requires that you write what you want your program to do step by step.

+ You can think of models in NIMBLE as graphs as in Figure \@ref(fig:dag-survival). A graph is made of relations (or edges) that can be of two types. A stochastic relation is signaled by a `~` sign and defines a random variable in the model, such as `survived` or `theta`. A deterministic relation is signaled by a `<-` sign, like `lifespan`. Relations define nodes on the left - the children - in terms of other nodes on the right - the parents, and relations are directed edges from parents to children. Such graphs are called directed acyclic graph or DAG.
```{r dag-survival, echo = FALSE, fig.cap = "Graph of the animal survival model. Survived is a stochastic node defined by its parents `released` and `theta`, while `lifespan` is a deterministic node the value of which is defined exactly by the value of its parent `theta`."}
mc <- nimbleModel(model, data = list(released = 57, survived = 19))
#mc$getVarNames()
#mc$getNodeNames()
#mc$getNodeNames(determOnly = TRUE)
#mc$getNodeNames(stochOnly = TRUE)
#mc$getNodeNames(dataOnly = TRUE)
#mc$getDependencies("theta")
mc$plotGraph()
```

Second step in our workflow is to read in some data. We use a list in which each component corresponds to a known quantity in the model:
```{r}
my.data <- list(released = 57, survived = 19)
```

You can proceed with data passed this way, but you should know a little more about how NIMBLE sees data. NIMBLE distinguishes data and constants. Constants are values that do not change, e.g. vectors of known index values or the indices used to define for-loops. Data are values that you might want to change, basically anything that only appears on the left of a `~`. Declaring relevant values as constants is better for computational efficiency, but it is easy to forget, and fortunately NIMBLE will by itself distinguish data and constants. I will not use the distinction between data and constants in this chapter, but in the next chapters it will become important. 

<!-- In passing say that full indexing is needed, you cannot let NIMBLE guess dimensions. -->
<!-- Ici on reprend le modèle simple du dessus, et on l'exprime un peu différemment pour illustrer qqs autres features de NIMBLE: i) loops, ii) distinction between constants and data. The binomial is a sum of independent Bernoulli outcomes with same probability. the Like flipping a coin for each individual and get a survivor with prob theta. Here survived is. Going back to our animal survival example, it means that the likelihood can be written as a Bernoulli random variable taking value 1 if animal survived, and 0 otherwise. **Voir dans annexe Hobbs**. E.g. `survived[1] ~ dbern(theta)` up to `survived[59] ~ dbern(theta)`. Likelihood contribution of individuals. Loops is the product. Iid. Instead of duplicating the same line of code `survived[i] ~ dbern(theta)` we use a loop. -->
<!-- ```{r} -->
<!-- model <- nimbleCode({ -->
<!--   # likelihood -->
<!--   for (i in 1:released){ -->
<!--     survived[i] ~ dbern(theta) -->
<!--   } -->
<!--   # prior -->
<!--   theta ~ dunif(0, 1) -->
<!-- }) -->
<!-- ``` -->

<!-- **If you try nimbleMCMC it won't work. This is because we ned to distinguish data from constants. Uncomment code.** -->

<!-- Distinguih constants and data. To Nimble, not all "data" is data... -->
<!-- ```{r} -->
<!-- my.constants <- list(released = 57) -->
<!-- my.data <- list(survived = 19) -->
<!-- ``` -->
<!-- ```{r, eval = FALSE} -->
<!-- mcmc.output <- nimbleMCMC(code = model, -->
<!--                           data = my.data, -->
<!--                           constants = my.constants, -->
<!--                           inits = initial.values, -->
<!--                           monitors = parameters.to.save, -->
<!--                           niter = n.iter, -->
<!--                           nburnin = n.burnin, -->
<!--                           nchains = n.chains) -->
<!-- ``` -->


<!-- ```{r} -->
<!-- my.data <- list(survived = c(rep(1,19), rep(0,57-19))) -->
<!-- my.constants <- list(released = 57) -->
<!-- ``` -->

<!-- The rest is the same. Steps 3, 4 and 5. -->
<!-- ```{r} -->
<!-- parameters.to.save <- c("theta", "life_expectancy") -->
<!-- initial.values <- function() list(theta = runif(1,0,1)) -->
<!-- n.iter <- 5000 -->
<!-- n.burnin <- 1000 -->
<!-- n.chains <- 3 -->
<!-- ``` -->

<!-- Run model, add argument `constants = my.constants`. -->
<!-- ```{r} -->
<!-- mcmc.output <- nimbleMCMC(code = model, -->
<!--                           data = my.data, -->
<!--                           constants = my.constants, -->
<!--                           inits = initial.values, -->
<!--                           monitors = parameters.to.save, -->
<!--                           niter = n.iter, -->
<!--                           nburnin = n.burnin, -->
<!--                           nchains = n.chains) -->
<!-- ``` -->

Third step is to tell NIMBLE which nodes in your model you would like to keep track of, in other words the quantities you'd like to do inference about. In our model we want survival `theta` and `lifespan`:
```{r}
parameters.to.save <- c("theta", "lifespan")
```

In general you have many quantities in your model, including some of little interest that are not worth monitoring, and having full control on verbosity will prove handy.

Fourth step is to specify initial values for all model parameters. To make sure that the MCMC algorithm explores the posterior distribution, we start different chains with different parameter values. You can specify initial values for each chain in a list and put them in yet another list:
```{r}
init1 <- list(theta = 0.1)
init2 <- list(theta = 0.5)
init3 <- list(theta = 0.9)
initial.values <- list(init1, init2, init3)
initial.values
```

Alternatively, you can write a simple R function that generates random initial values:
```{r}
initial.values <- function() list(theta = runif(1,0,1))
initial.values()
```

Firth and last step, you need to tell NIMBLE the number of chains to run, say `n.chain`, how long the burn-in period should be, say `n.burnin`, and the number of iterations following the burn-in period to be used for posterior inference. In NIMBLE, you specify the total number of iterations, say `n.iter`, so that the number of posterior samples per chain is `n.iter - n.burnin`. NIMBLE also allows discarding samples after burn-in, a procedure known as thinning, which I will not use in this book^[Link, W.A. and Eaton, M.J. (2012), On thinning of chains in MCMC. Methods in Ecology and Evolution, 3: 112-115.].
```{r}
n.iter <- 5000
n.burnin <- 1000
n.chains <- 3
```

We now have all the ingredients to run model, that is to sample in the posterior distribution of model parameters using MCMC simulations. This is accomplished using function `nimbleMCMC()`: 
```{r, cache = TRUE}
mcmc.output <- nimbleMCMC(code = model,
                          data = my.data,
                          inits = initial.values,
                          monitors = parameters.to.save,
                          niter = n.iter,
                          nburnin = n.burnin,
                          nchains = n.chains)
```

NIMBLE goes through several steps that we will explain in Section \@ref(under-the-hood). Function `nimbleMCMC()` takes other arguments that you might find useful. For example, you can suppress the progress bar if you find it to depressing when running long simulations with `progressBar = FALSE`. You can also get a summary of the outputs by specifying `summary = TRUE`. Check `?nimbleMCMC` for more details. 

Now let's inspect what we have in `mcmc.output`. 
```{r}
str(mcmc.output)
```

The R object `mcmc.output` is a list with three components, one for each MCMC chain. Let's have a look to `chain1` for example.
```{r}
dim(mcmc.output$chain1)
head(mcmc.output$chain1)
```

Each component of the list is a matrix. In rows, you have `r dim(mcmc.output$chain1)[1]` samples from the posterior distribution of `theta`, which corresponds to `n.iter - n.burnin` iterations. In columns, you have the quantities we monitor, `theta` and `lifespan`. From there, you can compute the posterior mean of `theta`:
```{r}
mean(mcmc.output$chain1[,'theta'])
```

You can also obtain the 95% credible interval for `theta`:
```{r}
quantile(mcmc.output$chain1[,'theta'], probs = c(2.5, 97.5)/100)
```

Let's visualise the posterior distribution of `theta` with a histogram: 
```{r}
mcmc.output %>%
  as_tibble() %>%
  ggplot() +
  geom_histogram(aes(x = chain1[,"theta"]), color = "white") +
  labs(x = "survival probability")
```

There are less painful ways of doing posterior inference. In this book, I will use the R package `MCMCvis`^[https://github.com/caseyyoungflesh/MCMCvis] to summarise and visualize MCMC outputs, but there are other perfectly valid options out there like `ggmcmc`^[Fernández-i-Marín, X. (2016). ggmcmc: Analysis of MCMC Samples and Bayesian Inference. Journal of Statistical Software, 70(9), 1–20] and `basicMCMCplots`^[https://cran.r-project.org/web/packages/basicMCMCplots/index.html]. **Shall I demonstrate these other options?**

<!-- Finally we want to look at our samples. NIMBLE returns samples as a simple matrix with named columns. There are numerous packages for processing MCMC output. If you want to use the coda package, you can convert a matrix to a coda mcmc object like this: -->
<!-- library(coda) -->
<!-- coda.samples <- as.mcmc(samples) -->
<!-- Alternatively, if you call nimbleMCMC with the argument samplesAsCodaMCMC = TRUE, the samples will be returned as a coda object. -->

Let's load the package `MCMCvis`:
```{r}
library(MCMCvis)
```

To get the most common numerical summaries, the function `MCMCsummary()` does the job:
```{r}
MCMCsummary(object = mcmc.output, round = 2)
```

You can use a caterpillar plot to visualise the posterior distributions of `theta` with `MCMCplot()`:
```{r}
MCMCplot(object = mcmc.output, 
         params = 'theta')
```

The point represents the posterior median, the thick line is the 50% credible interval and the thin line the 95% credible interval. 

The trace and posterior density of theta can be obtained with `MCMCtrace()`:
```{r}
MCMCtrace(object = mcmc.output,
          pdf = FALSE, # no export to PDF
          ind = TRUE, # separate density lines per chain
          params = "theta")
```

You can also add the diagnostics of convergence we discussed in the previous chapter:
```{r}
MCMCtrace(object = mcmc.output,
          pdf = FALSE,
          ind = TRUE,
          Rhat = TRUE, # add Rhat
          n.eff = TRUE, # add eff sample size
          params = "theta")
```

We calculated lifespan directly in our model with `lifespan <- -1/log(theta)`. But you can also calculate this quantity from outside NIMBLE. This is a nice by-product of using MCMC simulations: you can obtain the posterior distribution of any quantity that is function of your model parameters by applying this function to samples from the posterior distribution of these parameters. In our example, all you need is samples from the posterior distribution of `theta`, which we pool between the three chains with:
```{r}
theta_samples <- c(mcmc.output$chain1[,'theta'], 
                   mcmc.output$chain2[,'theta'],
                   mcmc.output$chain3[,'theta'])
```

To get samples from the posterior distribution of lifespan, we apply the function to calculate lifespan to the samples from the posterior distribution of survival:
```{r}
lifespan <- -1/log(theta_samples)
```

As usual then, you can calculate the posterior mean and 95% credible interval:
```{r}
mean(lifespan)
quantile(lifespan, probs = c(2.5, 97.5)/100)
```

You can also visualise the posterior distribution of lifespan:
```{r}
lifespan %>%
  as_tibble() %>%
  ggplot() +
  geom_histogram(aes(x = value), color = "white") +
  labs(x = "lifespan")
```

Now you're good to go. The NIMBLE workflow provided with `nimbleMCMC()` allows you to build models and make inference. This is what you can achieve with other software like WinBUGS or JAGS. 

But NIMBLE is more than just another MCMC engine. It provides a programming environment so that you have full control when building models and estimating parameters. NIMBLE allows you to write your own functions and distributions to build models, and to choose alternative MCMC samplers or code new ones. This flexibility often comes with faster convergence. 

I have to be honest, learning these improvements over other software takes some reading and experimentation, and it might well be that you do not need to use any of these features. And it's fine. In the next sections, I cover some of this advanced material. You may skip these sections and go back to this material later if you need it.

## Functions in NIMBLE {#functions-in-nimble}

In NIMBLE you can write and use your own functions, or use existing R or C/C++ functions. This allows you to customize models the way you want. 

### Write nimbleFunctions

NIMBLE provides `nimbleFunctions` for programming. A `nimbleFunction` is like an R function, plus it can be compiled for faster computation. Going back to our animal survival example, we can write a `nimbleFunction` to compute lifespan:
```{r}
computeLifespan <- nimbleFunction(
    run = function(theta = double(0)) { # type declarations
        ans <- -1/log(theta)
        return(ans)
        returnType(double(0))  # return type declaration
    } )
```

Within the nimbleFunction, the `run` section gives the function to be executed. It is written in the NIMBLE language. The `theta = double(0)` and `returnType(double(0))` arguments tell NIMBLE that the input and output are single numeric values (scalars). Alternatively, `double(1)` and `double(2)` are for vectors and matrices, while `logical()`, `integer()` and `character()` are for logical, integer and character values. 

You can use your `nimbleFunction` in R:
```{r}
computeLifespan(0.8)
```

You can compile it and use the C++ code for faster computation: 
```{r}
CcomputeLifespan <- compileNimble(computeLifespan)
CcomputeLifespan(0.8)
```

You can also use your `nimbleFunction` in a model:
```{r}
model <- nimbleCode({
  # likelihood
  survived ~ dbinom(theta, released)
  # prior
  theta ~ dunif(0, 1)
  # derived quantity
  lifespan <- computeLifespan(theta)
})
```

The rest of the workflow remains the same: 
```{r}
my.data <- list(survived = 19, released = 57)
parameters.to.save <- c("theta", "lifespan")
initial.values <- function() list(theta = runif(1,0,1))
n.iter <- 5000
n.burnin <- 1000
n.chains <- 3
mcmc.output <- nimbleMCMC(code = model,
                          data = my.data,
                          inits = initial.values,
                          monitors = parameters.to.save,
                          niter = n.iter,
                          nburnin = n.burnin,
                          nchains = n.chains)
MCMCsummary(object = mcmc.output, round = 2)
```

With `nimbleFunctions`, you can mimic basic R syntax, do linear algebra (e.g. compute eigenvalues), operate on vectors and matrices (e.g. inverse a matrix), use logical operators (e.g. and/or) and flow control (e.g. if-else). There is also a long list of common and less common distributions that can be used with `nimbleFunctions`. 

To learn everything you need to know on writing `nimbleFunctions`, make sure to read chapter 11 of the NIMBLE manual at <https://r-nimble.org/html_manual/cha-RCfunctions.html#cha-RCfunctions>. 

### Call existing R/C++ functions from NIMBLE

If you're like me, and too lazy to write your own functions, you can rely on the scientific community and use existing C, C++ or R code. The trick is to write a `nimbleFunction` that wraps access to that code which can then be used by NIMBLE. As an example, imagine you'd like to use an R function `myfunction()`, either a function you wrote yourself, or a function available in your favorite R package:
```{r}
myfunction <- function(x) {
  -1/log(x)
}
```

Now wrap this function using `nimbleRcall()` or `nimbleExternalCall()` for a C or C++ function:
```{r}
Rmyfunction <- nimbleRcall(prototype = function(x = double(0)){}, 
                           Rfun = 'myfunction',
                           returnType = double(0))
```

In the call to `nimbleRcall()` above, the argument `prototype` specifies inputs (a single numeric value `double(0)`) of the R function `Rfun` that generates outputs `returnType` (a single numeric value `double(0)`).  

Now you can call your R function from a model (or any `nimbleFunctions`):
```{r}
model <- nimbleCode({
  # likelihood
  survived ~ dbinom(theta, released)
  # prior
  theta ~ dunif(0, 1)
  lifespan <- Rmyfunction(theta)
})
```

The rest of the workflow remains the same: 
```{r}
my.data <- list(survived = 19, released = 57)
parameters.to.save <- c("theta", "lifespan")
initial.values <- function() list(theta = runif(1,0,1))
n.iter <- 5000
n.burnin <- 1000
n.chains <- 3
mcmc.output <- nimbleMCMC(code = model,
                          data = my.data,
                          inits = initial.values,
                          monitors = parameters.to.save,
                          niter = n.iter,
                          nburnin = n.burnin,
                          nchains = n.chains)
MCMCsummary(object = mcmc.output, round = 2)
```

Evaluating an R function from within NIMBLE slows MCMC sampling down, but if you can live with it, the cost is easily offset by the convenience of being able to use existing R functions. 

Another advantage of using `nimbleRcall()` (or `nimbleExternalCall()`) is that you can keep large objects out of your model, so that NIMBLE does not have to handle them in MCMC sampling. These objects should be constants and not change when you run NIMBLE. Letting R manipulating these objects will save you time, usually more than the time you lose by calling R from within NIMBLE.

Having `nimbleFunctions` offer infinite possibilities which I do not cover exhaustively here. Worth mentionning is that you can write your own distributions and use them in your model with NIMBLE. See the NIMBLE manual at <https://r-nimble.org/html_manual/cha-user-defined.html#sec:user-distributions> for details and examples. Another possibility worth mentionning is that you can write your own samplers. We will see an example in a minute, but I first need to tell you more about the NIMBLE workflow. 

## Under the hood {#under-the-hood}

So far, you have used `nimbleMCMC()` which runs the default MCMC workflow. This is perfecly fine for most applications. However, in some situations you need to customize the MCMC samplers to improve or fasten convergence. NIMBLE allows you to look under the hood by using a detailed workflow in several steps: `nimbleModel()`, `configureMCMC()`, `buildMCMC()`, `compileNimble()` and `runMCMC()`. Note that `nimbleMCMC()` does all of this at once.

We write the model code, read in data and pick initial values as before:
```{r}
model <- nimbleCode({
  # likelihood
  survived ~ dbinom(theta, released)
  # prior
  theta ~ dunif(0, 1)
  # derived quantity
  lifespan <- -1/log(theta)
})
my.data <- list(survived = 19, released = 57)
initial.values <- list(theta = 0.5)
```

First step is to create the model as an R object (uncompiled model) with `nimbleModel()`:
```{r}
survival <- nimbleModel(code = model,
                        data = my.data,
                        inits = initial.values)
```

You can look at its nodes:
```{r}
survival$getNodeNames()
```

You can look at the values stored at each node. 
```{r}
survival$theta
survival$survived
survival$lifespan 
# this is -1/log(0.5)
```

We can also look at the log-likelihood evaluated at the initial value for `theta`.
```{r}
survival$logProb_survived
# this is dbinom(x = 19, size = 57, prob = 0.5, log = TRUE)
```

You can obtain the graph of the model as in Figure \@ref(fig:dag-survival) with:
```{r}
survival$plotGraph()
```

Second we compile the model with `compileNimble()`:
```{r}
Csurvival <- compileNimble(survival)
```

With `compileNimble()`, the C++ code is generated, compiled and loaded back into R so that it can be used in R (compiled model):
```{r}
Csurvival$theta
```

Now you have two versions of the model, `survival` is in R and `Csurvival` in C++. Being able to separate the steps of model building and parameter estimation is a strength of NIMBLE. This gives you a lot of flexibility at both steps. For example, imagine you would like to fit your model with maximum likelihood, then you can do it by wrapping your model in an R function that gets the likelihood and maximise this function. Using the C version of the model, you can write:
```{r}
# function for negative log-likelihood to minimize
f <- function(par) {
    Csurvival[['theta']] <- par # assign par to theta 
    ll <- Csurvival$calculate() # update log-likelihood with par value
    return(-ll) # return negative log-likelihood
}
# evaluate function at 0.5 and 0.9
f(0.5)
f(0.9)
# minimize function
out <- optimize(f, interval = c(0,1))
round(out$minimum, 2)
```

By maximising the likelihood (or minimising the negative log-lieklihood), you obtain the maximum likelihood estimate of animal survival, which is exactly 19 surviving animals over 57 released animals or `r round(19/57, 2)`.

Third we create a MCMC configuration for our model with `configureMCMC()`:
```{r}
survivalConf <- configureMCMC(survival)
```

This steps tells you the nodes that are monitored by default, and the MCMC samplers than have been assigned to them. Here `theta` is monitored, and samples from its posterior distribution are simulated with a random walk sampler similar to the sampler we coded in the previous chapter in Section \@ref(metropolis-algorithm). 

To monitor `lifespan` in addition to `theta`, you write:
```{r}
survivalConf$addMonitors(c("lifespan"))
survivalConf
```

Third, we create a MCMC function with `buildMCMC()` and compile it with `compileNimble()`:
```{r}
survivalMCMC <- buildMCMC(survivalConf)
CsurvivalMCMC <- compileNimble(survivalMCMC, project = survival)
```

Fourth, we run NIMBLE with `runMCMC()`:
```{r}
n.iter <- 5000
n.burnin <- 1000
samples <- runMCMC(CsurvivalMCMC, 
                   niter = n.iter,
                   nburnin = n.burnin)
```

We run a single chain but `runMCMC()` allows you to use multiple chains as with `nimbleMCMC()`. 

You can look into `samples` which containts values simulated from the posterior distribution of the parameters we monitor:
```{r}
head(samples)
```

From here, you can obtain numerical summaries:
```{r}
summary(samples)
```

At first glance, using several steps instead of doing all these at once with `nimbleMCMC()` seem odds. Why is it useful? First, knowing what is under the hood can help you debug your code which is something I have always find hard to do with other software. Second, mastering the whole sequence of steps allows you to play around with samplers, and the default options or even write your own.

## Debugging

## Playing around with MCMC samplers

### Changing default sampler

What is the default sampler used by NIMBLE in our example? You can answer this question by inspecting the MCMC configuration obtained with `configureMCMC()`:
```{r}
#survivalConf <- configureMCMC(survival)
survivalConf$printSamplers()
```

Now that we have control on the MCMC configure, let's change it a little. We start by removing the default sampler:
```{r}
survivalConf$removeSamplers(c('theta'))
survivalConf$printSamplers()
```

And we whange it for a slice sampler:
```{r}
survivalConf$addSampler(target = c('theta'),
                        type = 'slice')
survivalConf$printSamplers()
```

Now you can resume the workflow:
```{r}
# create a new MCMC function and compile it:
survivalMCMC2 <- buildMCMC(survivalConf)
CsurvivalMCMC2 <- compileNimble(survivalMCMC2, 
                                project = survival,
                                resetFunctions = TRUE) # to compile new functions 
                                                       # into existing project, 
                                                       # need to reset nimbleFunctions
# run NIMBLE:
samples2 <- runMCMC(CsurvivalMCMC2, 
                   niter = n.iter,
                   nburnin = n.burnin)
# obtain numerical summaries:
summary(samples2)
```

NIMBLE has many samplers available, and a list is available with `?samplers`. For example, high correlation in (regression) parameters can make independent samplers inefficient. In that situation, block sampling might help which consists in proposing candidate values from a multivariate distribution that acknowledges correlation between parameters. Another example 

### Coding your own sampler

Allowing you to code your own sampler is another topic on which NIMBLE thrives. As an example, we focus on the Metropolis algorithm of Section \@ref(metropolis-algorithm) which we coded in R. In this section, we make it a `nimbleFunction` so that we can compile it for improved performance: 
```{r}
my_metropolis <- nimbleFunction(
  name = 'my_metropolis', # fancy name for our MCMC sampler
  contains = sampler_BASE, # has to be included
  setup = function(model, mvSaved, target, control) { # has to be included
    calcNodes <- model$getDependencies(target) # get dependencies for 'target' = param of interest in 'model'
    scale <- control$scale # SD of proposal distribution
  },
  run = function() {
    initialLP <- model$getLogProb(calcNodes) # log-lik at current value
    current <- model[[target]] # current parameter value
    lcurrent <- log(current / (1 - current)) # logit transform
    lproposal <- lcurrent  + rnorm(1, mean = 0, scale) # propose candidate value
    proposal <- plogis(lproposal) # back-transform
    model[[target]] <<- proposal 
    proposalLP <- model$calculate(calcNodes) # log-lik at candidate value
    lMHR <- proposalLP - initialLP # compute lik ratio on log scale
    if(runif(1,0,1) < exp(lMHR)) { # spin continuous spinner
      # if candidate value is accepted, update current value
      copy(from = model, to = mvSaved, nodes = calcNodes, logProb = TRUE, row = 1)
    } else {
      ## if candidate value is accepted, keep current value
      copy(from = mvSaved, to = model, nodes = calcNodes, logProb = TRUE, row = 1)
    }
  },
  methods = list(
    reset = function() {}
  )
)
```

**Explain sections of the code.**

<!-- Suppose we want a sampler for theta[4] in the pump model. An example specialization -->
<!-- step would be theta4sampler <- simple MH(model = pumpModel, currentState -->
<!-- = mvState, targetNode = "theta[4]"). Here mvState is a modelValues object with -->
<!-- variables that match those in the model, with only one of each. This is used to store the -->
<!-- current state of the model. We assume that on entry to the run function, mvState will -->
<!-- contain a copy of all model variables and log probabilities, and on exit the run function -->
<!-- must ensure that the same is true, re -->
<!-- ecting any updates to those states. As in the importance -->
<!-- sampling example, the only real work to be done in the setup function is to -->
<!-- query the model to determine the stochastic dependencies of the targetNode. In this case -->
<!-- calculationNodes will be (theta[4], lambda[4], x[4]). -->
<!-- The run function illustrates the compactness of expressing a Metropolis-Hastings algorithm -->
<!-- using language elements like calculate, getLogProb, copy, and list-like access to -->
<!-- a model node. The scale run-time argument is the standard deviation for the normally -->
<!-- distributed proposal value. In the full, released version of this algorithm (sampler RW), -->
<!-- the setup code includes some error trapping, and there is additional code to implement -->
<!-- adaptation of the scale parameter (Haario et al., 2001) rather than taking it as a runtime -->
<!-- argument. The simplified version here is less cluttered for illustration. In addition -->
<!-- the full version is more efficient by using calculateDiff instead of both getLogProb and -->
<!-- calculate, but here we use the latter to illustrate the steps more clearly. -->
<!-- The lines of run (1) obtain the current sum of log probabilities of the stochastic dependents -->
<!-- of the target node (including itself); (2) simulate a new value centered on the current -->
<!-- value (model[[targetNode]]); (3) put that value in the model; (4) calculate the new sum -->
<!-- of log probabilities of the same stochastic dependents; (5) determine the log acceptance -->
<!-- probability; (6) call the utility function decide that determines the accept/reject decision; -->
<!-- and (7) copy from the model to the currentState for (7a) an acceptance or (7b) vice-versa -->
<!-- for a rejection. Again, the setup and run functions are fully model-generic. -->
<!-- This example illustrates natural R-like access to nodes and variables in models, such -->
<!-- as model[[targetNode]], but making this model-generic leads to some surprising syntax. -->
<!-- Every node has a unique character name that includes indices, such as "theta[4]". This -->
<!-- leads to the syntax model[["theta[4]"]], rather than model[["theta"]][4]. The latter -->
<!-- is also valid, but it is not model-generic because, in another specialization of simple MH, -->
<!-- targetNode may have a different number of indices. For example, if targetNode is "y[2, -->
<!-- 3]", model[[targetNode]] accesses the same value as model[["y"]][2,3]. The NIMBLE -->
<!-- DSL also provides vectorized access to groups of nodes and/or variables. -->

<!-- To be used within the MCMC engine, sampler functions definitions must adhere exactly to the following: -->

<!-- + The nimbleFunction must include the contains statement contains = sampler_BASE. -->
<!-- + The setup function must have the four arguments model, mvSaved, target, control, in that order. -->
<!-- + The run function must accept no arguments, and have no return value. Further, after execution it must leave the mvSaved modelValues object as an up-to-date copy of the values and logProb values in the model object. -->
<!-- + The nimbleFunction must have a member method called reset, which takes no arguments and has no return value. -->

<!-- The purpose of the setup function is generally two-fold. First, to extract control parameters from the control list; in the example, the proposal standard deviation scale. It is good practice to specify default values for any control parameters that are not provided in the control argument, as done in the example. Second, to generate any sets of nodes needed in the run function. In many sampling algorithms, as here, calcNodes is used to represent the target node(s) and dependencies up to the first layer of stochastic nodes, as this is precisely what is required for calculating the Metropolis-Hastings acceptance probability. These probability calculations are done using model$calculate(calcNodes). -->

<!-- The purpose of the mvSaved modelValues object is to store the state of the model, including both node values and log probability values, as it exists before any changes are made by the sampler. This allows restoration of the state when a proposal is rejected, as can be seen in the example above. When a proposal is accepted, one should copy from the model into the mvSaved object. NIMBLE’s MCMC engine expects that mvSaved contains an up-to-date copy of model values and logProb values at the end of the run code of a sampler. -->

<!-- Note that NIMBLE generally expects the user-defined sampler to be defined in the global environment. If you define it in a function (which would generally be the case if you are using it in the context of parallelization), one approach would be to assign the user-defined sampler to the global environment in your function: -->


```{r}
model <- nimbleCode({
  # likelihood
  survived ~ dbinom(theta, released)
  # prior
  theta ~ dunif(0, 1)
})
my.data <- list(survived = 19, released = 57)
parameters.to.save <- c("theta")
initial.values <- function() list(theta = runif(1,0,1))
#mcmc.output <- nimbleMCMC(code = model,
#                          data = my.data,
#                          inits = initial.values,
#                          monitors = parameters.to.save,
#                          niter = n.iter,
#                          nburnin = n.burnin,
#                          nchains = n.chains)
#MCMCsummary(object = mcmc.output, round = 2)
```

Parameter to tune. Seek equivalent in previous chapter.
```{r}
scale <- 0.1
```

Detailed workflow.
```{r}
Rmodel <- nimbleModel(code = model, data = my.data, inits = initial.values())
conf <- configureMCMC(Rmodel, monitors = c('theta'))
conf$printSamplers()
conf$removeSamplers(c('theta'))
conf$addSampler(target = 'theta', type = 'my_metropolis', control = list(scale = scale))
conf$printSamplers()
conf$printMonitors()
Rmcmc <- buildMCMC(conf)
out <- compileNimble(list(model = Rmodel, mcmc = Rmcmc))
Cmcmc <- out$mcmc
samples <- runMCMC(Cmcmc, niter = 10000, nburnin = 9000)
samplesSummary(samples)
#basicMCMCplots::chainsPlot(samples)
```

Re-run by changing scale of proposal with 1 and 10, and compare to traceplots in previous chapter Figure \@ref(fig:tracechainlength):
```{r traceown, echo = FALSE, fig.align="center", out.width="100%", fig.cap = "Trace plots for different values of the standard deviation (scale) of the proposal distribution. "}
scale <- 1
Rmodel <- nimbleModel(code = model, data = my.data, inits = initial.values())
conf <- configureMCMC(Rmodel, monitors = c('theta'), print = FALSE)
conf$removeSamplers(c('theta'))
conf$addSampler(target = 'theta', type = 'my_metropolis', control = list(scale = scale))
Rmcmc <- buildMCMC(conf)
out <- compileNimble(list(model = Rmodel, mcmc = Rmcmc))
Cmcmc <- out$mcmc
samples_sd1 <- runMCMC(Cmcmc, niter = 10000, nburnin = 9000)
scale <- 10
Rmodel <- nimbleModel(code = model, data = my.data, inits = initial.values())
conf <- configureMCMC(Rmodel, monitors = c('theta'), print = FALSE)
conf$removeSamplers(c('theta'))
conf$addSampler(target = 'theta', type = 'my_metropolis', control = list(scale = scale))
Rmcmc <- buildMCMC(conf)
out <- compileNimble(list(model = Rmodel, mcmc = Rmcmc))
Cmcmc <- out$mcmc
samples_sd10 <- runMCMC(Cmcmc, niter = 10000, nburnin = 9000)
plot01 <- samples %>%
  as_tibble() %>%
  ggplot() + 
  aes(x = 9001:10000, y = theta) +
  geom_line() + 
  labs(x = "iterations", title = "scale = 0.1")
plot1 <- samples_sd1 %>%
  as_tibble() %>%
  ggplot() + 
  aes(x = 9001:10000, y = theta) +
  geom_line() + 
  labs(x = "iterations", title = "scale = 1")
plot10 <- samples_sd10 %>%
  as_tibble() %>%
  ggplot() + 
  aes(x = 9001:10000, y = theta) +
  geom_line() + 
  labs(x = "iterations", title = "scale = 10")
library(patchwork)
plot01 + plot1 + plot10
```

## Tip and tricks

Things to know. Grab from NIMBLE user's list. NIMBLE users what's yours? 

### Flexible specification of distributions

```{r, eval = FALSE}
# JAGS (& Nimble)
for(t in 1:Tmax){
  epsilon[t] ~ dnorm(0, tau)
}
tau <- pow(sigma, -2)
sigma ~ dunif(0, 5)

# Nimble
for(t in 1:Tmax){
  epsilon[t] ~ dnorm(0, sd = sigma)
}
sigma ~ dunif(0, 5)
```

### The end of empty indices

```{r, eval = FALSE}
# JAGS
sum.x <- sum(x[])

# Nimble
sum.x <- sum(x[1:Tmax])
```

### Make compilation faster

Try using "calculate=FALSE" in the call to nimbleModel. For such a large model, the uncompiled calculation can be a substantial portion of the time. When you get to configureMCMC, try using "useConjugacy=FALSE" to make that faster.

By default (calculate = TRUE), nimbleModel iterates through every node of the model, in order, trying to calculate, which means calculating values for deterministic nodes and log probabilities for stochastic nodes. For large models, this can take a long time because it is uncompiled and also doing some laborious keeping track of model structure. It can be helpful for debugging or catching issues, but it isn't strictly necessary.

here a couple of tips. A first tip would be to use the argument calculate = FALSE when using the nimbleModel() function, which will build the nimble models faster. By default, calculate is set to TRUE, meaning that nimble calculates all deterministic nodes and logProbability values when creating the model, and with large models this can eat up a good amount of time and memory.

### Initialization

The NA or NaN messages are saying that the log probability calculation for a node is not valid. That usually means not that the input data for the node is invalid but that some initial values for its parent nodes are giving an invalid probability. This may or may not be a problem. The MCMC does try to initialize everything. A good way to diagnose issues is to run the MCMC for 0 iterations (which will trigger the initialization) and then inspect the compiled model object. For example, if it is called Cmodel, you can do:

Cmodel$calculate("y[5, 10]") # perhaps this is NA
Then inspect parent nodes to see how this is happening:
Cmodel$beta1[5, 10] # etc.

### Running NIMBLE a little bit longer

Tomas, if you've already used runMCMC(Cmcmc, ...), then yes, you can still extend the current MCMC chain, using:
Cmcmc$run(niter, reset = FALSE)
where niter is the number of additional MCMC iterations you wish to run. After that, you can extract the full maxtrix of MCMC samples (which will include the ones you previously executed in the first call of runMCMC), using:

full_matrix_of_samples <- as.matrix(Cmcmc$mvSamples)

### Compile, compile, compile

Models and nimbleFunctions need to be compiled before they can be used to specify a project. Once compiled you can use an R model or nimbleFunction to specify the project.


## Summary

+ Blabla.

+ Reblabla. 

## Suggested reading

+ only scratch the surface of what NIMBLE is capable of. 

+ Official website [https://r-nimble.org](https://r-nimble.org)

+ User Manual [https://r-nimble.org/html_manual/cha-welcome-nimble.html](https://r-nimble.org/html_manual/cha-welcome-nimble.html) and [cheatsheet](https://r-nimble.org/cheatsheets/NimbleCheatSheet.pdf).

+ Users mailing list [https://groups.google.com/forum/#!forum/nimble-users](https://groups.google.com/forum/#!forum/nimble-users)

+ Training material [https://github.com/nimble-training](https://github.com/nimble-training)

+ Reference to cite when using nimble in a publication:

> de Valpine, P., D. Turek, C. J. Paciorek, C. Anderson-Bergman, D. Temple Lang, and R. Bodik (2017). [Programming With Models: Writing Statistical Algorithms for General Model Structures With NIMBLE](https://arxiv.org/pdf/1505.05093.pdf). *Journal of Computational and Graphical Statistics* **26** (2): 403–13.


<!-- ## A dire quelque part? -->

<!-- Pourquoi Nimble plutôt que Stan? Syntaxe BUGS, also discrete latent states easier to deal with, no need to marginalise. In Stan you marginalise (ref forward to relevant section of the book), but difficult endeavour, and you do not need to do that with NIMBLE, it has algorithms that work fine with discrete latent states.  -->

<!-- What about vectorization? -->

<!-- And parallelization à la jagsUI? -->

<!-- Simulations also? Generate initial values, cf code Maud? -->
