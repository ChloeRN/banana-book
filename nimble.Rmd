# NIMBLE tutorial {#intronimble}

## Introduction

In this second chapter, you will get familiar with NIMBLE an R package that implements up-to-date MCMC algorithms for fitting complex models. NIMBLE spares you from coding the MCMC algorithms by hand, and only requires the specification of a likelihood and priors for model parameters. You will go through a simple example to illustrate NIMBLE main features, but the ideas hold for other problems.  

## What is NIMBLE?

NIMBLE stands for **N**umerical **I**nference for statistical **M**odels using **B**ayesian and **L**ikelihood **E**stimation. Briefly speaking, NIMBLE is an R package that implements for you MCMC algorithms to sample the posterior distribution of model parameters. Freed from the burden of coding your own MCMC algorithms, you only have to specify a likelihood and priors to apply the Bayes theorem. To do so, NIMBLE uses a syntax very similar to the R syntax, which makes your life easier. This so-called BUGS language is also used by other programs like WinBUGS, OpenBUGS, and JAGS. 

So why use NIMBLE you may ask? The short answer is that NIMBLE is capable of so much more! First, you will work from within R, but in the background NIMBLE will translate your code in C++ for faster computation (in general). Second, NIMBLE extends the BUGS language for writing new functions and statistical distributions of your own, or grab those written by others. Third, NIMBLE gives you full control of the MCMC samplers, and you may pick other algorithms than the defaults. Fourth, NIMBLE comes with a library of numerical methods other than MCMC, including sequential Monte Carlo (particle filtering) and Monte Carlo Expectation Maximization (maximum likelihood). Last but not least, the development team is friendly and helpful, and based on users' feedbacks, NIMBLE folks work constantly at improving the package capabilities. 

## NIMBLE workflow

To run NIMBLE, you will need to specify three things: (1) a model (likelihood and priors), (2) the data, (3) those parameters you want to say something about, (4) initial values and (5) MCMC details (number of chains, length of the burn-in period and number of iterations following burn-in). 

But first things firt, and do not forget to load the `nimble` package.
```{r}
library(nimble)
```

Now let's go back to our example on animal survival from previous chapter. First step is to build our model by specifying the binomial likelihood and a uniform prior on survival probability. We use the `nimbleCode()` function. 
```{r}
model <- nimbleCode({
  # likelihood
  survived ~ dbinom(theta, released)
  # prior
  theta ~ dunif(0, 1)
})
```

In the code above, the ~ means distributed as.

BUGS is a declarative language for graphical (or hierarchical) models. Most programming languages are imperative, which means a series of commands will be executed in the order they are written. A declarative language like BUGS is more like building a machine before using it. Each line declares that a component should be plugged into the machine, but it doesn’t matter in what order they are declared as long as all the right components are plugged in by the end of the code.

The machine in this case is a graphical model12. A node (sometimes called a vertex) holds one value, which may be a scalar or a vector. Edges define the relationships between nodes. A huge variety of statistical models can be thought of as graphs.

Here is the code to define and create a simple linear regression model with four observations

’ and deterministic relationships are declared with ‘<-’. For example, each y[i] follows a normal distribution with mean predicted.y[i] and standard deviation sigma. Each predicted.y[i] is the result of intercept + slope * x[i]. The for-loop yields the equivalent of writing four lines of code, each with a different value of i. It does not matter in what order the nodes are declared. Imagine that each line of code draws part of Figure 5.1, and all that matters is that the everything gets drawn in the end. Available distributions, default and alternative parameterizations, and functions are listed in Section 5.2.4.

NIMBLE calls non-stochastic nodes ‘deterministic’, whereas BUGS ca

The model definition consists of a series of relations inside a block delimited by curly brackets { and } and preceded by the keyword model. Here is a simple linear regression example:

Each relation defines a node in the model. The node on the left of a relation is defined in
terms of other nodes – referred to as parent nodes – that appear on the right hand side.
Taken together, the nodes in the model form a directed acyclic graph (with the parent/child
relationships represented as directed edges). The very top-level nodes in the graph, with no
parents, are constant nodes, which are defined either in the model definition (e.g. 1.0E-3),
or in the data when the model is compiled (e.g. x[1]).
Relations can be of two types. A stochastic relation (~) defines a stochastic node, repre-
senting a random variable in the model. A deterministic relation (<-) defines a deterministic
node, the value of which is determined exactly by the values of its parents. The equals sign
(=) can be used for a deterministic relation in place of the left arrow (<-).



R2jags is that we can specify the model by creating a special kind of function.6 The avoids the need to create temporary files (as rjags requires) and keeps things tidier in our R markdown documents.

Describe distributions. And also nodes. Stochastic. Deterministic. And distributed as. Provide list of built-in distributions? You can provide your own, see e.g. 

Read in data. 
```{r}
my.data <- list(released = 57, survived = 19)
```

Distinguish constants and data. To Nimble, not all "data" is data...
```{r}
my.constants <- list(released = 57)
my.data <- list(survived = 19)
```

**Constants**:
+ Can never be changed
+ Must be provided when a model is defined (part of the model structure)
+ E.g. vector of known index values, variables used to define for-loops, etc.

After defining the model code, we should define the constants, initial values and data list. Compared to WinBUGS and JAGS, data and initial values can be defined in the same way, while ‘constants’ is a new list that contains the values that would not change, including the variables that define for-loop indices. In our settings, the lists of data, constants and initial values are given as follows:

**Data**:
+ Can be changed without re-building the model
+ Can be (re-)simulated within a model
+ E.g. stuff that *only* appears to the left of a "~"

For computational efficiency, better to specify as much as possible as constants. NIMBLE will help you with this!

We can also control the starting point for the chains. Starting different chains and quite different parameter values can help

verify that the MCMC algorithm is not overly sensitive to where we are starting from, and
ensure that the MCMC algorithm has explored the posterior distribution sufficiently.

On the other hand, if we start a chain too far from the peak of the posterior distribution, the chain may have trouble converging.

We can provide either specific starting points for each chain or a function that generates random starting points.

Specify initial values. 
```{r}
initial.values <- function() list(theta = runif(1,0,1))
```

```{r}
initial.values()
```

Which parameters to save? Define parameters to keep track of (i.e., parameters of interest).
```{r}
parameters.to.save <- c("theta")
```

MCMC details
```{r}
n.iter <- 5000
n.burnin <- 1000
n.chains <- 2
```

Number of posterior samples per chain:
$$n.posterior = \frac{n.iter - n.burnin}{n.thin}$$

Run model, tadaa!
```{r, cache = TRUE}
mcmc.output <- nimbleMCMC(code = model,
                          data = my.data,
                          constants = my.constants,
                          inits = initial.values,
                          monitors = parameters.to.save,
                          niter = n.iter,
                          nburnin = n.burnin,
                          nchains = n.chains)
```

Details on messages received. 

Say we do not thin. But ok if you'd like, just think of $n.posterior = \frac{n.iter - n.burnin}{n.thin}$.

Proposer le même modèle avec la bernoulli pour montrer une boucle. The binomial is just a sum of Bernoulli outcomes. Like flipping a coin for each individual and get a survivor with prob phi. Comme dans annexe Hobbs. Vectorize also. 

### Post-process MCMC outputs by hand

```{r}
str(mcmc.output)
```

```{r}
head(mcmc.output$chain1)
```

```{r, echo = FALSE}
mcmc.output %>%
  as_tibble() %>%
  ggplot() +
  geom_histogram(aes(x = chain1[,"theta"]), color = "white") +
  labs(x = "survival probability")
```

### Post-process MCMC outputs without pain

We use MCMCvis, but there are other perfectly valid options out there like ggmcmc and basicMCMCplots.

Numerical summaries.
```{r}
library(MCMCvis)
MCMCsummary(mcmc.output, round = 2)
```

Trace and posterior density

```{r}
MCMCtrace(mcmc.output,
          pdf = FALSE)
```

```{r}
MCMCtrace(mcmc.output,
          pdf = FALSE,
          ind = TRUE,
          Rhat = TRUE,
          n.eff = TRUE)
```

## Syntax: what's new/better/different?

**basculer des trucs de speed up ici**

+ Vectorization
```{r, eval = FALSE}
# JAGS (& Nimble)
for(t in 1:Tmax){
  x[t] <- Mu.x + epsilon[t]
}

# Nimble
x[1:Tmax] <- Mu.x + epsilon[1:Tmax]
```

+ More flexible specification of distributions
```{r, eval = FALSE}
# JAGS (& Nimble)
for(t in 1:Tmax){
  epsilon[t] ~ dnorm(0, tau)
}
tau <- pow(sigma, -2)
sigma ~ dunif(0, 5)

# Nimble
for(t in 1:Tmax){
  epsilon[t] ~ dnorm(0, sd = sigma)
}
sigma ~ dunif(0, 5)
```

+ Your own functions and distributions
```{r, eval = FALSE}
x[1:Tmax] <- myNimbleFunction(a = Mu.x, b = epsilon[1:Tmax])
```

```{r, eval = FALSE}
sigma ~ dCustomDistr(c = 0.5, z = 10)
```

+ The end of empty indices
```{r, eval = FALSE}
# JAGS
sum.x <- sum(x[])

# Nimble
sum.x <- sum(x[1:Tmax])
```

+ & more...

## Our `nimble` workflow so far

```{r}
knitr::include_graphics("images/nimble_workflow_sofar.png")
```


But `nimble` gives full access to the MCMC engine

```{r}
knitr::include_graphics("images/nimble_workflow.png")
```

```{r}
knitr::include_graphics("images/I1bIY06.gif")
```

## Functions

Say we want an R function that adds 2 to every value in a vector.
```{r}
add2 <- function(x) {
   x + 2 
}
Radd2 <- nimbleRcall(function(x = double(0)){}, 
                     Rfun = 'add2',
                     returnType = double(0))
demoCode <- nimbleCode({
  mu ~ dnorm(0,1)
  for(i in 1:n) {
    x[i] ~ dnorm(mu, sd = 1)
    z[i] <- Radd2(x[i])
    } 
})

param_names <- c("mu", "z")
mcmc.out <- nimbleMCMC(code = demoCode, 
                      constants = list(n = 4),
                      data = list(x = c(-1, -2, 1, 2)), 
                      inits = list(mu = rnorm(1)),
                      monitors = param_names,
                      nchains = 2, 
                      niter = 1000,
                      nburnin = 500)
library(MCMCvis)
MCMCsummary(object = mcmc.out, round = 2)
```

Change format to vectorise.
```{r}
add2 <- function(x) {
   x + 2 
}
Radd2 <- nimbleRcall(function(x = double(1)){}, 
                     Rfun = 'add2',
                     returnType = double(1))
demoCode <- nimbleCode({
  mu ~ dnorm(0,1)
  for(i in 1:n) {
    x[i] ~ dnorm(mu, sd = 1)
    }
    z[1:4] <- Radd2(x[1:4])
})

param_names <- c("mu", "z")
mcmc.out <- nimbleMCMC(code = demoCode, 
                      constants = list(n = 4),
                      data = list(x = c(-1, -2, 1, 2)), 
                      inits = list(mu = rnorm(1)),
                      monitors = param_names,
                      nchains = 2, 
                      niter = 1000,
                      nburnin = 500)
library(MCMCvis)
MCMCsummary(object = mcmc.out, round = 2)
```

Now have paramater to estimate as parameter of your R function. 
```{r}
add2 <- function(x) {
   x + 2 
}
Radd2 <- nimbleRcall(function(x = double(0)){}, 
                     Rfun = 'add2',
                     returnType = double(0))
demoCode <- nimbleCode({
  mu ~ dnorm(0,1)
  for(i in 1:n) {x[i] ~ dnorm(mu, sd = 1)} 
  z <- Radd2(mu)
})

param_names <- c("mu", "z")
mcmc.out <- nimbleMCMC(code = demoCode, 
                      constants = list(n = 4),
                      data = list(x = c(-1, -2, 1, 2)), 
                      inits = list(mu = rnorm(1)),
                      monitors = param_names,
                      nchains = 2, 
                      niter = 1000,
                      nburnin = 500)
library(MCMCvis)
MCMCsummary(object = mcmc.out, round = 2)
```

In general if you do need a nimbleRcall like this, there are a couple of considerations. It is common to need to write a wrapper function, i.e. a function you access via nimbleRcall that calls your actual function of interest with arguments and then return value rearranged as needed. For example, if you just need the eigenvectors, a wrapper function could pick those out and return them.  The bigger issue is the returnType declaration: nimble type declarations do not include an R list of type declarations as a nimble type.  I think you could use a nimbleList data structure for this purpose.  You would have to create a nimbleList type and then use that as the declared returnType.  But you would still need to write a wrapper, so that you could convert the list returned from base::eigen into a nimbleList object to return from your wrapper.  I hope that makes sense.

https://kenkellner.com/blog/models-with-integrals.html

Same thing w/ global environment.

```{r eval = FALSE}
library(nimble)
add2 <- function(x) {
   x + 2 + globvar
}
add2(2)
globvar <- 2020
add2(2)
Radd2 <- nimbleRcall(function(x = double(0)){}, 
                     Rfun = 'add2',
                     returnType = double(0))
demoCode <- nimbleCode({
  mu ~ dnorm(0,1)
  for(i in 1:n) {x[i] ~ dnorm(mu, sd = 1)} 
  z <- Radd2(mu)
})

param_names <- c("mu", "z")
mcmc.out <- nimbleMCMC(code = demoCode, 
                       constants = list(n = 4),
                       data = list(x = c(-1, -2, 1, 2)), 
                       inits = list(mu = rnorm(1)),
                       monitors = param_names,
                       nchains = 2, 
                       niter = 1000,
                       nburnin = 500)
#printErrors()
# pb is y is not recognized
#ls()
# assign y to global env
# https://stackoverflow.com/questions/9726705/assign-multiple-objects-to-globalenv-from-within-a-function
#assign("globvar", 20, envir = .GlobalEnv)

library(MCMCvis)
MCMCsummary(object = mcmc.out, round = 2)
```

## Code your own sampler

```{r eval = FALSE}
library(nimble)
load('matos/ressources-chapters/nimble/dipper_data.Rdata')

dipperCode <- nimbleCode({
    logit.p ~ dnorm(0, 0.001)
    logit.phi ~ dnorm(0, 0.001)
    p <- expit(logit.p)
    phi <- expit(logit.phi)
    ##phi ~ dunif(0, 1)
    ##p ~ dunif(0, 1)
    for(i in 1:N) {
        x[i, first[i]] <- 1
        y[i, first[i]] <- 1
        for(t in (first[i]+1):T) {
            x[i, t] ~ dbern(phi * x[i, t-1])
            y[i, t] ~ dbern(p * x[i, t])
        }
    }
})

N <- dim(sightings)[1]
T <- dim(sightings)[2]
dipperConsts <- list(N = N, T = T, first = first)
dipperData <- list(y = sightings)
xInit <- ifelse(!is.na(sightings), 1, 0)
dipperInits <- list(logit.phi = 0, logit.p = 0, x = xInit)

samples <- nimbleMCMC(dipperCode, dipperConsts, dipperData, dipperInits,
                      niter = 10000, nburnin = 5000,
                      monitors = c('p', 'phi'))

my_MH <- nimbleFunction(
    name = 'my_MH',
    contains = sampler_BASE,
    setup = function(model, mvSaved, target, control) {
        calcNodes <- model$getDependencies(target)
        scale <- control$scale
    },
    run = function() {
        initialLP <- model$getLogProb(calcNodes)
        current <- model[[target]]
        proposal <- rnorm(1, current, scale)
        model[[target]] <<- proposal
        proposalLP <- model$calculate(calcNodes)
        lMHR <- proposalLP - initialLP
        if(runif(1,0,1) < exp(lMHR)) {
            ## accept
            copy(from = model, to = mvSaved, nodes = calcNodes, logProb = TRUE, row = 1)
        } else {
            ## reject
            copy(from = mvSaved, to = model, nodes = calcNodes, logProb = TRUE, row = 1)
        }
    },
    methods = list(
        reset = function() {}
    )
)

scale <- 0.05

Rmodel <- nimbleModel(dipperCode, dipperConsts, dipperData, dipperInits)
conf <- configureMCMC(Rmodel, monitors = c('p', 'phi'))
conf$printSamplers()
conf$printSamplers(byType = TRUE)
conf$removeSamplers(c('logit.p', 'logit.phi'))
conf$addSampler(target = 'logit.p', type = 'my_MH', control = list(scale = scale))
conf$addSampler(target = 'logit.phi', type = 'my_MH', control = list(scale = scale))
conf$printSamplers()
conf$printMonitors()
Rmcmc <- buildMCMC(conf)

out <- compileNimble(list(model=Rmodel, mcmc=Rmcmc))
Cmcmc <- out$mcmc

samples2 <- runMCMC(Cmcmc, niter = 10000, nburnin = 5000)

samplesSummary(samples2)

basicMCMCplots::chainsPlot(samples2)
```

## A dire quelque part?

Pourquoi Nimble plutôt que Stan? Syntaxe BUGS, also discrete latent states easier to deal with, no need to marginalise. In Stan you marginalise (ref forward to relevant section of the book), but difficult endeavour, and you do not need to do that with NIMBLE, it has algorithms that work fine with discrete latent states. 

## When things go wrong: Tip and tricks

## Summary

+ Blabla.

+ Reblabla. 

```{block2 fake, type='rmdnote', eval = FALSE}
The *hypothesis* is a working assumption about which you want to learn using *data*. In capture--recapture analyses, the hypothesis might be a parameter like detection probability, or regression parameters in a relationship between survival probability and a covariate. Bayes' theorem tells us how to obtain the probability of a hypothesis given the data we have. 
```

## Suggested reading

+ Official website [https://r-nimble.org](https://r-nimble.org)

+ User Manual [https://r-nimble.org/html_manual/cha-welcome-nimble.html](https://r-nimble.org/html_manual/cha-welcome-nimble.html) and [cheatsheet](https://r-nimble.org/cheatsheets/NimbleCheatSheet.pdf).

+ Users mailing list [https://groups.google.com/forum/#!forum/nimble-users](https://groups.google.com/forum/#!forum/nimble-users)

+ Training material [https://github.com/nimble-training](https://github.com/nimble-training)

+ Reference to cite when using nimble in a publication:

> de Valpine, P., D. Turek, C. J. Paciorek, C. Anderson-Bergman, D. Temple Lang, and R. Bodik (2017). [Programming With Models: Writing Statistical Algorithms for General Model Structures With NIMBLE](https://arxiv.org/pdf/1505.05093.pdf). *Journal of Computational and Graphical Statistics* **26** (2): 403–13.
