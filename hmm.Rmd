# Hidden Markov models {#hmmcapturerecapture}

## Introduction

In this third chapter, you will learn the basics on Markov models and how to fit them to longitudinal data using NIMBLE. In real life however, individuals may go undetected and their status be unknown. You will also learn how to manipulate the extension of Markov models to hidden states, so-called hidden Markov models. 

## Longitudinal data

Let's get back to our survival example, and denote $z_i$ the state of individual $i$ with $z_i = 1$ if alive and $z_i = 0$ if dead. We have a total of $z = \displaystyle{\sum_{i=1}^{n}{z_i}}$ survivors out of $n$ released animals with winter survival probability $\phi$. Our model so far is a combination of a binomial likelihood and a Beta prior with parameters 1 and 1, which is also a uniform distribution between 0 and 1. It can be written as^[I write models the way Richard McElreath does it in his book and video lectures [Statistical Rethinking](https://xcelab.net/rm/statistical-rethinking/).]:

\begin{align*}
   z &\sim \text{Binomial}(n, \phi) &\text{[likelihood]}
   \\
  \phi &\sim \text{Beta}(1, 1) &\text{[prior for }\phi \text{]} \\
\end{align*}

Because the binomial distribution is just a sum of independent Bernoulli outcomes, you can rewrite this model as:

\begin{align*}
   z_i &\sim \text{Bernoulli}(\phi), \; i = 1, \ldots, N &\text{[likelihood]}
   \\
  \phi &\sim \text{Beta}(1, 1) &\text{[prior for }\phi \text{]} \\
\end{align*}

It is like flipping a coin for each individual and get a survivor with probability $\phi$.

In this set up, we consider a single winter. But for many species, we need to collect data on the long term to get a representative estimate of survival. Therefore what if we had say $T = 5$ winters?

Let us denote $z_{i,t} = 1$ if individual $i$ alive at winter $t$, and $z_{i,t} = 2$ if dead. Then longitudinal data look like in the table below. Each row is an individual $i$, and columns are for winters $t$, or sampling occasions. Variable $z$ is indexed by both $i$ and $t$, and takes value 1 if individual $i$ is alive in winter $t$, and 2 otherwise.

```{r echo = FALSE}
# 1 = alive, 2 = dead
nind <- 57
nocc <- 5
phi <- 0.8 # survival probability
delta <- c(1,0) # (Pr(alive at t = 1), Pr(dead at t = 1))
Gamma <- matrix(NA, 2, 2) # transition matrix
Gamma[1,1] <- phi      # Pr(alive t -> alive t+1)
Gamma[1,2] <- 1 - phi  # Pr(alive t -> dead t+1)
Gamma[2,1] <- 0        # Pr(dead t -> alive t+1)
Gamma[2,2] <- 1        # Pr(dead t -> dead t+1)
z <- matrix(NA, nrow = nind, ncol = nocc)
set.seed(2022)
for (i in 1:nind){
  z[i,1] <- nimble::rcat(n = 1, prob = delta) # 1 for sure
  for (t in 2:nocc){
    z[i,t] <- nimble::rcat(n = 1, prob = Gamma[z[i,t-1],1:2]) 
  }
}
colnames(z) <- paste0("winter ", 1:nocc)
z %>%
  as_tibble() %>%
  add_column(id = 1:nind, .before = "winter 1") %>%
  kableExtra::kable() %>%
  kableExtra::scroll_box(width = "100%", height = "400px")
#  kableExtra::kable_styling(font_size = 8,
#                            latex_options = "scale_down")
```

## A Markov model for longitudinal data

Let's think of a model for these data. The objective remains the same, estimating survival. To build this model, we'll make assumptions, go through its components and write down its likelihood. Note that we already encountered Markov models in Section \@ref(markovmodelmcmc).

### Assumptions

First, we assume that the state of an animal in a given winter, alive or dead, is only dependent on its state the winter before. In other words, the future depends only on the present, not the past. This is a Markov process.

Second, if an animal is alive in a given winter, the probability it survives to the next winter is $\phi$. The probability it dies is $1 - \phi$.

Third, if an animal is dead a winter, it remains dead, unless you believe in zombies.

Our Markov process can be represented this way:

```{r, engine = 'tikz', echo = FALSE}
\usetikzlibrary{arrows, fit, positioning, automata}
\begin{tikzpicture}[node distance = 2cm]
\tikzset{state/.style = {circle, draw, minimum size = 30pt, scale = 3, line width=1pt}}
\node [state,fill=lightgray!75] (6) [] {$z_{i,t}$};
\node [state,fill=lightgray!75] (5) [left = 20mm of 6] {$z_{i,t-1}$};
\node [state,fill=lightgray!75] (4) [left = 20mm of 5] {$z_{i,t-2}$};
\node [state,fill=lightgray!75] (3) [left = 20mm of 4] {$\cdots$};
\node [state,fill=lightgray!75] (7) [right = 20mm of 6] {$z_{i,t+1}$};
\node [state,fill=lightgray!75] (8) [right = 20mm of 7] {$z_{i,t+2}$};
\node [state,fill=lightgray!75] (9) [right = 20mm of 8] {$\cdots$};
\draw[->,black, line width=0.25mm,-latex] (3) to (4);
\draw[->,black, line width=0.25mm,-latex] (4) to (5);
\draw[->,black, line width=0.25mm,-latex] (5) to (6);
\draw[->,black, line width=0.25mm,-latex] (6) to (7);
\draw[->,black, line width=0.25mm,-latex] (7) to (8);
\draw[->,black, line width=0.25mm,-latex] (8) to (9);
\end{tikzpicture}
```

An example of this Markov process is, for example:

```{r, engine = 'tikz', echo = FALSE}
\usetikzlibrary{arrows, fit, positioning, automata}
\begin{tikzpicture}[node distance = 2cm]
\tikzset{state/.style = {circle, draw, minimum size = 30pt, scale = 3, line width=1pt}}
\node [state,fill=lightgray!75] (6) [] {$1$};
\node [state,fill=lightgray!75] (5) [left = 20mm of 6] {$1$};
\node [state,fill=lightgray!75] (4) [left = 20mm of 5] {$1$};
\node [state,fill=lightgray!75] (3) [left = 20mm of 4] {$1$};
\node [state,fill=lightgray!75] (7) [right = 20mm of 6] {$2$};
\node [state,fill=lightgray!75] (8) [right = 20mm of 7] {$2$};
\node [state,fill=lightgray!75] (9) [right = 20mm of 8] {$\cdots$};
\draw[->,black, line width=0.25mm,-latex] (3) -- node[above=3mm, align=center] {\huge $\phi$} (4);
\draw[->,black, line width=0.25mm,-latex] (4) -- node[above=3mm, align=center] {\huge $\phi$} (5);
\draw[->,black, line width=0.25mm,-latex] (5) -- node[above=3mm, align=center] {\huge $\phi$} (6);
\draw[->,black, line width=0.25mm,-latex] (6) -- node[above=3mm, align=center] {\huge $1 - \phi$} (7);
\draw[->,black, line width=0.25mm,-latex] (7) -- node[above=3mm, align=center] {\huge 1} (8);
\draw[->,black, line width=0.25mm,-latex] (8) -- node[above=3mm, align=center] {\huge 1} (9);
\end{tikzpicture}
```

Here the animal remains alive over the three first time intervals $(z_{i,1} = z_{i,2} = z_{i,3} = 1)$ with probability $\phi$ until it dies over the fourth time interval $(z_{i,4} = 2)$ with probability $1-\phi$ then remains dead from then onwards $(z_{i,t \geq 5} = 2)$ with probability 1. 

### Transition matrix

You might have figured it out already, the core of our Markov process is made of transition probabilities between states alive and dead. 

For example, the probability of transitioning from state alive at $t-1$ to state alive at $t$ is $\Pr(z_{i,t} = 1 | z_{i,t-1} = 1) = \gamma_{1,1}$. It is the survival probability $\phi$.

The probability of dying over the interval $(t-1, t)$ is $\Pr(z_{i,t} = 2 | z_{i,t-1} = 1) = \gamma_{1,2} = 1 - \phi$.

Now if an animal is dead at $t-1$, then $\Pr(z_t = 1 | z_{t-1} = 2) = 0$ and $\Pr(z_{i,t} = 2 | z_{i,t-1} = 2) = 1$. 

We can gather these probabilities of transition between states from one occasion to the next in a matrix, say $\mathbf{\Gamma}$, which we will call the transition matrix:

\begin{align*}
\mathbf{\Gamma} =
\left(\begin{array}{cc}
\gamma_{1,1} & \gamma_{1,2}\\
\gamma_{2,1} & \gamma_{2,2}
\end{array}\right) =
\left(\begin{array}{cc}
\phi & 1 - \phi\\
0 & 1
\end{array}\right)
\end{align*}

To try and remember that the states at $t-1$ are in rows, and the states at $t$ are in columns, I will often write:

$$
\begin{matrix}
& \\
\mathbf{\Gamma} =
    \left ( \vphantom{ \begin{matrix} 12 \\ 12 \end{matrix} } \right .
\end{matrix}
\hspace{-1.2em}
\begin{matrix}
    z_t=A & z_t=D \\ \hdashline
\phi & 1-\phi \\
0 & 1
\end{matrix}
\hspace{-0.2em}
\begin{matrix}
& \\
\left . \vphantom{ \begin{matrix} 12 \\ 12 \end{matrix} } \right )
    \begin{matrix}
    z_{t-1}=A \\ z_{t-1}=D
    \end{matrix}
\end{matrix}
$$

Take the time you need to navigate through this matrix, and get familiar with it. For example, you may start alive at $t$ (first row) then end up dead at $t+1$ (first column) with probability $1-\phi$.

### Initial states

A Markov process has to start somewhere. We need the probabilities of initial states, i.e. the states of an individual at $t = 1$. 

We will gather the probability of being in each state (alive or 1 and dead or 2) in the first winter in a vector. We will use $\mathbf{\delta} = \left(\Pr(z_{i,1} = 1), \Pr(z_{i,1} = 2)\right)$.  

For simplicity, we will assume that all individuals are marked and released in the first winter, hence alive when first captured, which means that they are all in state alive or 1 for sure. Therefore we have $\mathbf{\delta} = \left(1, 0\right)$.

### Likelihood

Now that we have built a Markov model, we need its likelihood to apply the Bayes theorem. The likelihood is the probability of the data, given the model. Here the data are the $z$, therefore we need $\Pr(\mathbf{z}) = \Pr(z_1, z_2, \ldots, z_{T-2}, z_{T-1}, z_T)$.

We're gonna work backward, starting from the last sampling occasion. Using conditional probabilities, the likelihood can be written as the product of the probability of $z_T$ i.e. you're alive or not on the last occasion given your past history, that is the states at previous occasions, times the probability of your past history:

\begin{align*}
\Pr(\mathbf{z}) &= \Pr(z_T, z_{T-1}, z_{T-2}, \ldots, z_1) \color{white}{\Pr(z_{T-1}, z_{T-2},\ldots, z_1) \Pr(z_{T-2}, \ldots, z_1)}\\
                &= \Pr(z_T | z_{T-1}, z_{T-2},\ldots, z_1) \Pr(z_{T-1}, z_{T-2},\ldots, z_1) \\
\end{align*}

Then because we have a Markov model, we're memory less, that is the probabilty of next state, here $z_T$, depends only on the current state, that is $z_{T-1}$, and not the previous states:

\begin{align*}
\Pr(\mathbf{z}) &= \Pr(z_T, z_{T-1}, z_{T-2}, \ldots, z_1) \color{white}{\Pr(z_{T-1}, z_{T-2},\ldots, z_1) \Pr(z_{T-2}, \ldots, z_1)}\\
                &= \Pr(z_T | z_{T-1}, z_{T-2},\ldots, z_1) \Pr(z_{T-1}, z_{T-2},\ldots, z_1) \\
                &= \Pr(z_T | z_{T-1}) \Pr(z_{T-1}, z_{T-2},\ldots, z_1) \\
\end{align*}

You can apply the same reasoning to $T-1$. First use conditional probabilities: 

\begin{align*}
\Pr(\mathbf{z}) &= \Pr(z_T, z_{T-1}, z_{T-2}, \ldots, z_1) \color{white}{\Pr(z_{T-1}, z_{T-2},\ldots, z_1) \Pr(z_{T-2}, \ldots, z_1)}\\
                &= \Pr(z_T | z_{T-1}, z_{T-2},\ldots, z_1) \Pr(z_{T-1}, z_{T-2},\ldots, z_1) \\
                &= \Pr(z_T | z_{T-1}) \Pr(z_{T-1}, z_{T-2},\ldots, z_1) \\
                &= \Pr(z_T | z_{T-1}) \Pr(z_{T-1} | z_{T-2}, \ldots, z_1) \Pr(z_{T-2}, \ldots, z_1)\\
\end{align*}

Then apply the Markovian property:

\begin{align*}
\Pr(\mathbf{z}) &= \Pr(z_T, z_{T-1}, z_{T-2}, \ldots, z_1) \color{white}{\Pr(z_{T-1}, z_{T-2},\ldots, z_1) \Pr(z_{T-2}, \ldots, z_1)}\\
                &= \Pr(z_T | z_{T-1}, z_{T-2},\ldots, z_1) \Pr(z_{T-1}, z_{T-2},\ldots, z_1) \\
                &= \Pr(z_T | z_{T-1}) \Pr(z_{T-1}, z_{T-2},\ldots, z_1) \\
                &= \Pr(z_T | z_{T-1}) \Pr(z_{T-1} | z_{T-2}, \ldots, z_1) \Pr(z_{T-2}, \ldots, z_1)\\
                &= \Pr(z_T | z_{T-1}) \Pr(z_{T-1} | z_{T-2}) \Pr(z_{T-2}, \ldots, z_1)\\
\end{align*}

And so on up to $z_2$. You end up with this expression for the likelihood:

\begin{align*}
\Pr(\mathbf{z}) &= \Pr(z_T, z_{T-1}, z_{T-2}, \ldots, z_1) \color{white}{\Pr(z_{T-1}, z_{T-2},\ldots, z_1) \Pr(z_{T-2}, \ldots, z_1)}\\
                &= \Pr(z_T | z_{T-1}, z_{T-2},\ldots, z_1) \Pr(z_{T-1}, z_{T-2},\ldots, z_1) \\
                &= \Pr(z_T | z_{T-1}) \Pr(z_{T-1}, z_{T-2},\ldots, z_1) \\
                &= \Pr(z_T | z_{T-1}) \Pr(z_{T-1} | z_{T-2}, \ldots, z_1) \Pr(z_{T-2}, \ldots, z_1)\\
                &= \Pr(z_T | z_{T-1}) \Pr(z_{T-1} | z_{T-2}) \Pr(z_{T-2}, \ldots, z_1)\\
                &= \ldots \\
                &= \Pr(z_T | z_{T-1}) \Pr(z_{T-1} | z_{T-2}) \ldots \Pr(z_{2} | z_{1}) \Pr(z_{1})\\
\end{align*}

This is a product of conditional probabilities of states given previous states, and the probability of initial states $\Pr(z_1)$. Using a more compact notation for the product of conditional probabilities, we get:

\begin{align*}
\Pr(\mathbf{z}) &= \Pr(z_T, z_{T-1}, z_{T-2}, \ldots, z_1) \color{white}{\Pr(z_{T-1}, z_{T-2},\ldots, z_1) \Pr(z_{T-2}, \ldots, z_1)}\\
                &= \Pr(z_T | z_{T-1}, z_{T-2},\ldots, z_1) \Pr(z_{T-1}, z_{T-2},\ldots, z_1) \\
                &= \Pr(z_T | z_{T-1}) \Pr(z_{T-1}, z_{T-2},\ldots, z_1) \\
                &= \Pr(z_T | z_{T-1}) \Pr(z_{T-1} | z_{T-2}, \ldots, z_1) \Pr(z_{T-2}, \ldots, z_1)\\
                &= \Pr(z_T | z_{T-1}) \Pr(z_{T-1} | z_{T-2}) \Pr(z_{T-2}, \ldots, z_1)\\
                &= \ldots \\
                &= \Pr(z_T | z_{T-1}) \Pr(z_{T-1} | z_{T-2}) \ldots \Pr(z_{2} | z_{1}) \Pr(z_{1})\\
                &= \Pr(z_{1}) \prod_{t=2}^T{\Pr(z_{t} | z_{t-1})}\\
\end{align*}

In the product, you can recognize the transition parameters $\gamma$ we defined above, so that the likelihood of a Markov model can be written as:

\begin{align*}
\Pr(\mathbf{z}) &= \Pr(z_T, z_{T-1}, z_{T-2}, \ldots, z_1) \color{white}{\Pr(z_{T-1}, z_{T-2},\ldots, z_1) \Pr(z_{T-2}, \ldots, z_1)}\\
                &= \Pr(z_{1}) \prod_{t=2}^T{\gamma_{z_{t-1},z_{t}}}\\
\end{align*}


<!-- ## Matrix formulation of the likelihood -->

<!-- \begin{align*} -->
<!-- \Pr(\mathbf{z}) &= \Pr(z_T, z_{T-1}, z_{T-2}, \ldots, z_1) \color{white}{\Pr(z_{T-1}, z_{T-2},\ldots, z_1) \Pr(z_{T-2}, \ldots, z_1)}\\ -->
<!--                 &= \Pr(z_{1}) \prod_{t=2}^T{\gamma_{z_{t-1},z_{t}}}\\ -->
<!--                 &= \mathbf{\delta} \; \mathbf{\Gamma} \cdots \mathbf{\Gamma} -->
<!-- \end{align*} -->

### Example

I realise these calculations are a bit difficult to follow. Let's take an example to fix ideas. Let's assume an animal is alive, alive at time 2 then dies at time 3. We have $\mathbf{z} = (1, 1, 2)$. What is the contribution of this animal to the likelihood? Let's apply the formula we just derived:

\begin{align*}
\Pr(\mathbf{z} = (1, 1, 2)) &= \Pr(z_1 = 1) \; \gamma_{z_{1} = 1, z_{2} = 1} \; \gamma_{z_{2} = 1, z_{3} = 2}\\
                            &= 1 \; \phi \; (1 - \phi).
\end{align*}

The probability of having the sequence alive, alive and dead is the probability of being alive first, then to stay alive, eventually to die. The probability of being alive at first occasion being 1, we have that the contribution of this individual to the likelihood is $\phi (1 - \phi)$.

## Bayesian formulation

Now that we have the likelihood of a Markov model, we can complement it with a prior for survival to apply the Bayes theorem. 

Before implementing this model in NIMBLE, we note that the likelihood is a product of conditional probabilities of binary events (alive or dead). Usually binary events are associated with the Bernoulli distribution. Here however,  we will use its extension to several outcomes (from a coin to a dice) known as the categorical distribution^[The categorical distribution is a multinomial distribution with a single draw.]. To get a better idea of how the categorical distribution works, let's simulate from it. Consider for example a random value drawn from a categorical distribution with probability 0.1, 0.3 and 0.6. Think of a dice with three faces, face 1 has probability 0.1 of occurring, face 2 probability 0.3 and face 3 has probability 0.6, the sum of these probabilities being 1. We expect to get a 3 more often than a 2 and rarely a 1: 
```{r}
nimble::rcat(n = 1, prob = c(0.1, 0.3, 0.6))
```

Here is another example in which we sample 20 times in a categorical distribution with probabilities 0.1, 0.1, 0.4, 0.2 and 0.2, hence a dice with 5 faces:
```{r}
nimble::rcat(n = 20, prob = c(0.1, 0.1, 0.4, 0.2, 0.2))
```

In this chapter, you will familiarise yourself with the categorical distribution in binary situations, which should make the transition to more states than just alive and dead smoother in the next chapters. 

Initial state is a categorical random variable with probability $\delta$. That is you have a dice with two faces, or a coin, and you have some probability to be alive, and one minus that probability to be dead. Of course, it you want your Markov chain to start, you'd better say it's alive so that $\delta$ is just $(1,0)$:

\begin{align*}
   z_1 &\sim \text{Categorical}(\delta) &\text{[likelihood, }t = 1 \text{]}\\
\end{align*}

Now the main part is the dynamic of the states. The state $z_t$ at $t$ depends only on the known state $z_{t-1}$ at $t-1$, and is a categorical random variable which probabilities are given by row $z_{t-1}$ of the transition matrix $\mathbf{\Gamma} = \gamma_{z_{t-1},z_{t}}$:

\begin{align*}
   z_1 &\sim \text{Categorical}(\delta) &\text{[likelihood, }t = 1 \text{]}\\
   z_t | z_{t-1} &\sim \text{Categorical}(\gamma_{z_{t-1},z_{t}}) &\text{[likelihood, }t > 1 \text{]}\\
\end{align*}

For example, if individual $i$ is alive over $(t-1,t)$ i.e. $z_{t-1} = 1$, we need the first row in $\mathbf{\Gamma}$, 

\begin{align*}
\mathbf{\Gamma} =
\left(\begin{array}{cc}
\color{blue}{\phi} & \color{blue}{1 - \phi}\\
0 & 1
\end{array}\right)
\end{align*}

that is $\color{blue}{\gamma_{z_{t-1} = 1,z_{t}} = (\phi, 1-\phi)}$ and $z_t | z_{t-1} = 1 \sim \text{Categorical}((\phi, 1-\phi))$. 

Otherwise, if individual $i$ dies over $(t-1,t)$ i.e. $z_{t-1} = 2$, we need the second row in $\mathbf{\Gamma}$:

\begin{align*}
\mathbf{\Gamma} =
\left(\begin{array}{cc}
\phi & 1 - \phi\\
\color{blue}{0} & \color{blue}{1}
\end{array}\right)
\end{align*}

that is $\color{blue}{\gamma_{z_{t-1} = 2,z_{t}} = (0, 1)}$ and $z_t | z_{t-1} = 2 \sim \text{Categorical}((0, 1))$ (if the individual is dead, it remains dead with probability 1). 

We also need a prior on survival. Without surprise, we use a uniform distribution between 0 and 1, or a Beta distribution with parameters 1 and 1. Overall our model is: 

\begin{align*}
   z_1 &\sim \text{Categorical}(\delta) &\text{[likelihood, }t = 1 \text{]}\\
   z_t | z_{t-1} &\sim \text{Categorical}(\gamma_{z_{t-1},z_{t}}) &\text{[likelihood, }t > 1 \text{]}\\
  \phi &\sim \text{Beta}(1, 1) &\text{[prior for }\phi \text{]} \\
\end{align*}


## NIMBLE implementation

How to implement the Markov model we just built in NIMBLE? First things first, we need some bricks that are relatively easy to put in place. Let's start with the prior on survival, the vector of initial state probabilities and the transition matrix:
```{r eval = FALSE}
markov.survival <- nimbleCode({
  phi ~ dunif(0, 1) # prior
  delta[1] <- 1          # Pr(alive t = 1) = 1
  delta[2] <- 0          # Pr(dead t = 1) = 0
  gamma[1,1] <- phi      # Pr(alive t -> alive t+1)
  gamma[1,2] <- 1 - phi  # Pr(alive t -> dead t+1)
  gamma[2,1] <- 0        # Pr(dead t -> alive t+1)
  gamma[2,2] <- 1        # Pr(dead t -> dead t+1)
...
```

Alternatively, you can define vectors and matrices in NIMBLE like you would do it in R. You can write:
```{r eval = FALSE}
markov.survival <- nimbleCode({
  phi ~ dunif(0, 1) # prior
  delta[1:2] <- c(1, 0) # vector of initial state probabilities
  gamma[1:2,1:2] <- matrix( c(phi, 0, 1 - phi, 1), nrow = 2) # transition matrix
...
```

Now there are two important dimensions to our model, along which we need to repeat tasks, namely individual and time. As for time, we describe the successive events of survival, say for individual $i$:
```{r eval = FALSE}
z[i,1] ~ dcat(delta[1:2])           # t = 1
z[i,2] ~ dcat(gamma[z[i,1], 1:2])   # t = 2
z[i,3] ~ dcat(gamma[z[i,2], 1:2])   # t = 3
...
z[i,T] ~ dcat(gamma[z[i,T-1], 1:2]) # t = T
```

There is a more efficient way to write this piece of code by using a for loop, that is a sequence of instructions that we repeat. Here, we condense the previous code into:
```{r eval = FALSE}
z[i,1] ~ dcat(delta[1:2])             # t = 1
for (t in 2:T){ # loop over time t
  z[i,t] ~ dcat(gamma[z[i,t-1], 1:2]) # t = 2,...,T
}
```

Now we just need to do the same for all individuals. We use another loop:
```{r eval = FALSE}
for (i in 1:N){ # loop over individual i
  z[i,1] ~ dcat(delta[1:2]) # t = 1
  for (j in 2:T){ # loop over time t
    z[i,j] ~ dcat(gamma[z[i,j-1], 1:2]) # t = 2,...,T
  } # t
} # i
```

**Do I need to say anything here about `dcat()`?**

Puting everything together, the NIMBLE code for our Markov model is:
```{r}
markov.survival <- nimbleCode({
  phi ~ dunif(0, 1) # prior
  delta[1] <- 1          # Pr(alive t = 1) = 1
  delta[2] <- 0          # Pr(dead t = 1) = 0
  gamma[1,1] <- phi      # Pr(alive t -> alive t+1)
  gamma[1,2] <- 1 - phi  # Pr(alive t -> dead t+1)
  gamma[2,1] <- 0        # Pr(dead t -> alive t+1)
  gamma[2,2] <- 1        # Pr(dead t -> dead t+1)
  # likelihood
  for (i in 1:N){ # loop over individual i
    z[i,1] ~ dcat(delta[1:2]) # t = 1
    for (j in 2:T){ # loop over time t
      z[i,j] ~ dcat(gamma[z[i,j-1], 1:2]) # t = 2,...,T
    } # t
  } # i
})
```

**Do I need to say that vector $\delta$ is used as a placeholder for more complex models to come? Here, you could write `z[i,1] <- 1` (show some code?).**

```{r, eval = FALSE}
markov.survival <- nimbleCode({
  # likelihood
  for (i in 1:N){
    z[i,1] ~ dcat(delta[1:2])
    for (j in 2:T){
      z[i,j] ~ dcat(gamma[z[i,j-1], 1:2])
    }
  }})
```

Now we're ready to resume our standard NIMBLE workflow. First we read in data. Because we have loops and indices that do not change, we use constants as explained in Section \@ref{gettingstartedinnimble}:
```{r}
my.constants <- list(N = 57, T = 5)
my.data <- list(z = z)
```

We also specify initial values for survival with a function:
```{r}
initial.values <- function() list(phi = runif(1,0,1))
initial.values()
```

There is a single parameter to monitor:
```{r}
parameters.to.save <- c("phi")
parameters.to.save
```

We run 2 chains with 5000 iterations including 1000 iterations as burnin:
```{r}
n.iter <- 5000
n.burnin <- 1000
n.chains <- 2
```

Let's run NIMBLE:
```{r, eval=FALSE}
mcmc.output <- nimbleMCMC(code = markov.survival,
                          constants = my.constants,
                          data = my.data,
                          inits = initial.values,
                          monitors = parameters.to.save,
                          niter = n.iter,
                          nburnin = n.burnin,
                          nchains = n.chains)
```

```{r, message=FALSE, echo = FALSE, cache = TRUE}
mcmc.output <- nimbleMCMC(code = markov.survival,
                          constants = my.constants,
                          data = my.data,
                          inits = initial.values,
                          monitors = parameters.to.save,
                          niter = n.iter,
                          nburnin = n.burnin,
                          nchains = n.chains,
                          progressBar = FALSE)
```

Let's calculate the usual posterior numerical summaries for survival:
```{r}
MCMCsummary(mcmc.output, round = 2)
```

Posterior mean and median are close to $0.8$. This is fortunate since the data was simulated with (actual) survival $\phi = 0.8$. The code I used was:
```{r echo = TRUE, eval = TRUE}
# 1 = alive, 2 = dead
nind <- 57
nocc <- 5
phi <- 0.8 # survival probability
delta <- c(1,0) # (Pr(alive at t = 1), Pr(dead at t = 1))
Gamma <- matrix(NA, 2, 2) # transition matrix
Gamma[1,1] <- phi      # Pr(alive t -> alive t+1)
Gamma[1,2] <- 1 - phi  # Pr(alive t -> dead t+1)
Gamma[2,1] <- 0        # Pr(dead t -> alive t+1)
Gamma[2,2] <- 1        # Pr(dead t -> dead t+1)
z <- matrix(NA, nrow = nind, ncol = nocc)
set.seed(2022)
for (i in 1:nind){
  z[i,1] <- nimble::rcat(n = 1, prob = delta) # 1 for sure
  for (t in 2:nocc){
    z[i,t] <- nimble::rcat(n = 1, prob = Gamma[z[i,t-1],1:2]) 
  }
}
head(z) 
```

**Note also that dcat() everywhere could be replaced by dbern(). Does it make any difference? I think dcat() uses less efficient samplers than dbern(). To be checked. But dcat() convenient in terms of model building because allows more than two outcomes - forward reference to next chapters.**

## Hidden Markov models

### Capture-recapture data

Unfortunately, this is the data we wish we had. In real life, animals cannot be monitored exhaustively, like humans in a medical trial. Animals are captured, marked or identified then released alive. Then, these animals may be detected again, or go undetected <span>&#8212;</span> **capture-recapture**. Whenever animals go undetected, it might be that they were alive but missed, or because they were dead and therefore could not be detected <span>&#8212;</span> **imperfect detection**^[Nice video here on the basics principles of capture-recapture experiments <https://www.youtube.com/embed/tyX79mPm2xY>]. Whenever animals go undetected, it might be that they were alive but missed, or because they were dead and therefore could not be detected <span>&#8212;</span> **imperfect detection**. The Markov process for survival is only partially observed <span>&#8212;</span> **hidden Markov models**.

**Est-ce que je me passer d'introduire la matrice des observations at first encounter? Be dans les notations de Roger? On ne peut pas se passer d'expliquer que puisqu'on a une étape de marquage, il y a un conditionnement à cette première capture, à laquelle on sait que z = 1 et y = 1. Du coup pourquoi introduire delta si en fait on n'introduit pas Be?**

The truth is in $z$:

```{r echo = FALSE}
colnames(z) <- paste0("winter ", 1:nocc)
z %>%
  as_tibble() %>%
  add_column(id = 1:nind, .before = "winter 1") %>%
  kableExtra::kable() %>%
  kableExtra::scroll_box(width = "100%", height = "300px")
```

Unfortunately, we have only partial access to $z$. We do observe $y$ the detections and non-detections. How are $z$ and $y$ connected?

Dead animals go undetected. When an animal is dead i.e. $z = 2$, it cannot be detected, therefore $y = 0$.

```{r echo = FALSE}
colnames(z) <- paste0("winter ", 1:nocc)
z %>%
  as_tibble() %>%
  replace(. == 2, 0) %>%
  add_column(id = 1:nind, .before = "winter 1") %>%
  kableExtra::kable() %>%
  kableExtra::scroll_box(width = "100%", height = "300px")
```

Alive animals may be detected or not. If animal is alive $z = 1$, it is detected $y = 1$ w/ prob $p$ or not $y = 0$ w/ prob $1-p$. Before **first** detection, we know nothing, and we proceed conditional on it.

```{r echo = FALSE}
set.seed(2022)
nocc <- 5
p <- 0.6
phi <- 0.8
delta <- c(1,0)
Gamma <- matrix(NA, 2, 2)
Omega <- matrix(NA, 2, 2)
Gamma[1,1] <- phi      # Pr(alive t -> alive t+1)
Gamma[1,2] <- 1 - phi  # Pr(alive t -> dead t+1)
Gamma[2,1] <- 0        # Pr(dead t -> alive t+1)
Gamma[2,2] <- 1        # Pr(dead t -> dead t+1)
Omega[1,1] <- 1 - p      # Pr(alive t -> non-detected t)
Omega[1,2] <- p          # Pr(alive t -> detected t)
Omega[2,1] <- 1          # Pr(dead t -> non-detected t)
Omega[2,2] <- 0          # Pr(dead t -> detected t)
z <- matrix(NA, nrow = nind, ncol = nocc)
y <- z
y[,1] <- 1
for (i in 1:nind){
  z[i,1] <- nimble::rcat(n = 1, prob = delta) # 1 for sure
  for (t in 2:nocc){
    z[i,t] <- nimble::rcat(n = 1, prob = Gamma[z[i,t-1],1:2]) 
    y[i,t] <- nimble::rcat(n = 1, prob = Omega[z[i,t],1:2]) 
  }
}
colnames(y) <- paste0("winter ", 1:nocc)
nobs <- sum(apply(y,1,sum) != 0)
y <- y[apply(y,1,sum) !=0, ]
y %>%
  as_tibble() %>%
  add_column(id = 1:nobs, .before = "winter 1") %>%
  kableExtra::kable() %>%
  kableExtra::scroll_box(width = "100%", height = "300px")
```

Compare with previous table. Some 1s have become 0s. This table $y$ is what we observe in real life. To make the connection between the observations, the $y$, and the true states, the $z$.  We need to describe how observations are made from the states

### Observation matrix

The observation probabilities can be packed in an observation matrix $\mathbf{\Omega}$. In rows: the states alive $z = 1$ and dead $z = 2$. In columns: the observations non-detected $y = 1$ and detected $y = 2$ (previously coded 0 and 1 respectively).

\begin{align*}
\mathbf{\Omega} =
\left(\begin{array}{cc}
\omega_{1,1} & \omega_{1,2}\\
\omega_{2,1} & \omega_{2,2}
\end{array}\right) =
\left(\begin{array}{cc}
1 - p & p\\
1 & 0
\end{array}\right)
\end{align*}

Observation matrix:

$$
\begin{matrix}
& \\
\mathbf{\Omega} =
    \left ( \vphantom{ \begin{matrix} 12 \\ 12 \end{matrix} } \right .
\end{matrix}
\hspace{-1.2em}
\begin{matrix}
    y_t=1 & y_t=2 \\ \hdashline
1 - p & p\\
1 & 0\\
\end{matrix}
\hspace{-0.2em}
\begin{matrix}
& \\
\left . \vphantom{ \begin{matrix} 12 \\ 12 \end{matrix} } \right )
    \begin{matrix}
    z_{t}=A \\ z_{t}=D
    \end{matrix}
\end{matrix}
$$

### Hidden Markov model

Markov model:

```{r, engine = 'tikz', echo = FALSE}
\usetikzlibrary{arrows, fit, positioning, automata}
\begin{tikzpicture}[node distance = 2cm]
\tikzset{state/.style = {circle, draw, minimum size = 30pt, scale = 3, line width=1pt}}
\node [state,fill=lightgray!75] (6) [] {$z_{t}$};
\node [state,fill=lightgray!75] (5) [left = 20mm of 6] {$z_{t-1}$};
\node [state,fill=lightgray!75] (4) [left = 20mm of 5] {$z_{t-2}$};
\node [state,fill=lightgray!75] (3) [left = 20mm of 4] {$\cdots$};
\node [state,fill=lightgray!75] (7) [right = 20mm of 6] {$z_{t+1}$};
\node [state,fill=lightgray!75] (8) [right = 20mm of 7] {$z_{t+2}$};
\node [state,fill=lightgray!75] (9) [right = 20mm of 8] {$\cdots$};
\draw[->,black, line width=0.25mm,-latex] (3) to (4);
\draw[->,black, line width=0.25mm,-latex] (4) to (5);
\draw[->,black, line width=0.25mm,-latex] (5) to (6);
\draw[->,black, line width=0.25mm,-latex] (6) to (7);
\draw[->,black, line width=0.25mm,-latex] (7) to (8);
\draw[->,black, line width=0.25mm,-latex] (8) to (9);
\end{tikzpicture}
```

States $z$ are in gray. Remember the graphical representation of a Markov model. Hidden Markov model:

```{r, engine = 'tikz', echo = FALSE}
\usetikzlibrary{arrows, fit, positioning, automata}
\begin{tikzpicture}[node distance = 2cm]
\tikzset{state/.style = {circle, draw, minimum size = 30pt, scale = 3, line width=1pt}}
\node [state,fill=lightgray!75] (6) [] {$z_{t}$};
\node [state,fill=lightgray!75] (5) [left = 20mm of 6] {$z_{t-1}$};
\node [state,fill=lightgray!75] (4) [left = 20mm of 5] {$z_{t-2}$};
\node [state,fill=lightgray!75] (3) [left = 20mm of 4] {$\cdots$};
\node [state,fill=lightgray!75] (7) [right = 20mm of 6] {$z_{t+1}$};
\node [state,fill=lightgray!75] (8) [right = 20mm of 7] {$z_{t+2}$};
\node [state,fill=lightgray!75] (9) [right = 20mm of 8] {$\cdots$};
\node [state,fill=white] (16) [above = 20mm of 6] {$y_{t}$};
\node [state,fill=white] (15) [above = 20mm of 5] {$y_{t-1}$};
\node [state,fill=white] (14) [above = 20mm of 4] {$y_{t-2}$};
\node [state,fill=white] (17) [above = 20mm of 7] {$y_{t+1}$};
\node [state,fill=white] (18) [above = 20mm of 8] {$y_{t+2}$};
\draw[->,black, line width=0.25mm,-latex] (3) to (4);
\draw[->,black, line width=0.25mm,-latex] (4) to (5);
\draw[->,black, line width=0.25mm,-latex] (5) to (6);
\draw[->,black, line width=0.25mm,-latex] (6) to (7);
\draw[->,black, line width=0.25mm,-latex] (7) to (8);
\draw[->,black, line width=0.25mm,-latex] (8) to (9);
\draw[->,black, line width=0.25mm,-latex] (4) to (14);
\draw[->,black, line width=0.25mm,-latex] (5) to (15);
\draw[->,black, line width=0.25mm,-latex] (6) to (16);
\draw[->,black, line width=0.25mm,-latex] (7) to (17);
\draw[->,black, line width=0.25mm,-latex] (8) to (18);
\end{tikzpicture}
```

States $z$ are in gray. Observations $y$ are in white. A hidden Markov model is just two time series. One for the states with a Markovian property. The other of observations generated from the states. Run in parallel.

```{r, engine = 'tikz', echo = FALSE}
\usetikzlibrary{arrows, fit, positioning, automata}
\begin{tikzpicture}[node distance = 2cm]
\tikzset{state/.style = {circle, draw, minimum size = 30pt, scale = 3, line width=1pt}}
\node [state,fill=lightgray!75] (6) [] {$1$};
\node [state,fill=lightgray!75] (5) [left = 20mm of 6] {$1$};
\node [state,fill=lightgray!75] (4) [left = 20mm of 5] {$1$};
\node [state,fill=lightgray!75] (3) [left = 20mm of 4] {$1$};
\node [state,fill=lightgray!75] (7) [right = 20mm of 6] {$2$};
\node [state,fill=lightgray!75] (8) [right = 20mm of 7] {$2$};
\node [state,fill=lightgray!75] (9) [right = 20mm of 8] {$\cdots$};
\node [state,fill=white] (16) [above = 20mm of 6] {$1$};
\node [state,fill=white] (15) [above = 20mm of 5] {$2$};
\node [state,fill=white] (14) [above = 20mm of 4] {$1$};
\node [state,fill=white] (17) [above = 20mm of 7] {$1$};
\node [state,fill=white] (18) [above = 20mm of 8] {$1$};
\draw[->,black, line width=0.25mm,-latex] (3) -- node[above=3mm, align=center] {\huge $\varphi$} (4);
\draw[->,black, line width=0.25mm,-latex] (4) -- node[above=3mm, align=center] {\huge $\varphi$} (5);
\draw[->,black, line width=0.25mm,-latex] (5) -- node[above=3mm, align=center] {\huge $\varphi$} (6);
\draw[->,black, line width=0.25mm,-latex] (6) -- node[above=3mm, align=center] {\huge $1 - \varphi$} (7);
\draw[->,black, line width=0.25mm,-latex] (7) -- node[above=3mm, align=center] {\huge $1$} (8);
\draw[->,black, line width=0.25mm,-latex] (8) -- node[above=3mm, align=center] {\huge $1$} (9);
\draw[->,black, line width=0.25mm,-latex] (4) -- node[left=3mm, align=center] {\huge $1 - p$} (14);
\draw[->,black, line width=0.25mm,-latex] (5) -- node[left=3mm, align=center] {\huge $p$} (15);
\draw[->,black, line width=0.25mm,-latex] (6) -- node[left=3mm, align=center] {\huge $1 - p$} (16);
\draw[->,black, line width=0.25mm,-latex] (7) -- node[left=3mm, align=center] {\huge $1$} (17);
\draw[->,black, line width=0.25mm,-latex] (8) -- node[left=3mm, align=center] {\huge $1$} (18);
\end{tikzpicture}
```

For states (in gray), $z = 1$ is alive, $z = 2$ is dead. For observations (in white), $y = 1$ is non-detected, $y = 2$ is detected. Now add the states alive and dead, 1 and 2s. The observations, non-detected and detected, 1 and 2s. And the parameters, $\phi$ for transition from 1 to 1. And $p$ for probability of $y$ being 2 detected given $z$ is 1 alive.

### Likelihood

In the likelihood, we have observed $y$. Parameters are $\phi$, $p$ and unobserved or partially observed $z$. Shall we estimate the latent states $z$ or not? Treat them as parameters? 

Complexity high, but ok. Comment les z sont-ils estimés puisqu'on somme dessus? See comments in paper by Daniel Turek sur number of nodes. Et pb convergence. OK forward algorithm for marginalization. 

**Regarder dans la thèse de Lauriane aussi.** 

We can get rid of the states, so that likelihood is a function of $\phi$ and $p$ only. This is the function we would maximize in a Frequentist approach. Reference forward to section below (forward algorithm). 

The Bayesian approach with MCMC methods allows treating the latent states as if they were parameters, and to be estimated as such.

Infering the latent states $z$ can be useful to estimate prevalence, e.g. in animal epidemiology with [prevalence of a disease](https://veterinaryresearch.biomedcentral.com/articles/10.1186/1297-9716-45-39), in evolutionary ecology with [sex ratio](https://onlinelibrary.wiley.com/doi/abs/10.1002/cjs.5550360105) or in conservation biology with [prevalence of hybrids](https://onlinelibrary.wiley.com/doi/full/10.1002/ece3.4819?af=R).

Estimating the latent states is costly though, and if not required, marginalisation may speed up computations. Actually, you can estimate the states afterwards (Viterbi).

More about so-called marginalisation in [Yackulic et al. (2020)](https://esajournals.onlinelibrary.wiley.com/doi/abs/10.1002/eap.2112). And reference forward to section below. 

The neat thing with Nimble is that it provides marginalised models through nimbleEcology, we'll get back to it in section blabla.

Using the formula of total probability, then the likelihood of a Markov chain:

\begin{align*}
\Pr(\mathbf{y}) &= \Pr(y_1, y_{2}, \ldots, y_T)\\
                &= \sum_{z_1} \cdots \sum_{z_T} \Pr(y_1, y_{2}, \ldots, y_T | z_1, z_{2}, \ldots, z_T) \Pr(z_1, z_{2}, \ldots, z_T)\\
                &= \sum_{z_1} \cdots \sum_{z_T} \left(\prod_{t=1}^T{\omega_{z_{t}, y_t}}\right) \left(\Pr(z_{1}) \prod_{t=2}^T{\gamma_{z_{t-1},z_{t}}}\right)\\
\end{align*}

What is the likelihood of a HMM. The thing here is that we don't know the states. So we have to go through all possibilities, and sum over the possible states. Hence these sums here. Then this term is the likelihood of a Markov chain, we saw that before. And this component are the elements of the observation matrix. The likelihood has a matrix formulation that can be useful. It is delta, initial states, then observation, then transitions, and so on. There is a vector of ones at the end to get the sum all the terms.

<!-- It has a matrix formulation: -->
<!-- \begin{align*} -->
<!-- \Pr(\mathbf{y}) &= \mathbf{\delta} \; \mathbf{\Omega} \; \mathbf{\Gamma} \cdots \mathbf{\Omega} \; \mathbf{\Gamma} \; \mathbf{\Omega} \; \mathbb{1} -->
<!-- \end{align*} -->

### Example

Let assume an animal is detected, then missed. We have $\mathbf{y} = (2, 1)$. What is the contribution of this animal to the likelihood?

\begin{align*}
\Pr(\mathbf{y} = (2, 1)) &= \sum_{z_1 = 1}^2 \; \sum_{z_2 = 1}^2 w_{z_1, y_1 = 2} w_{z_2, y_2 = 1} \Pr(z_1) \gamma_{z_1, z_2} \color{white}{\Pr(z_{T-1}, z_{T-2},\ldots, z_1, z_1, z_1, z_1)}\\
\end{align*}

Let assume an animal is detected, then missed. We have $\mathbf{y} = (2, 1)$. What is the contribution of this animal to the likelihood?

\begin{align*}
\Pr(\mathbf{y} = (2, 1)) &= \sum_{z_1 = 1}^2 \; \sum_{z_2 = 1}^2 w_{z_1, y_1 = 2} w_{z_2, y_2 = 1} \Pr(z_1) \gamma_{z_1, z_2} \color{white}{\Pr(z_{T-1}, z_{T-2},\ldots, z_1, z_1, z_1, z_1)}\\
&= \sum_{z_1 = 1}^2 \left( w_{z_1, y_1 = 2} w_{z_2 = 1, y_2 = 1} \Pr(z_1) \gamma_{z_1, z_2 = 1} + w_{z_1, y_1 = 2} w_{z_2 = 2, y_2 = 1} \Pr(z_1) \gamma_{z_1, z_2 = 2} \right) \\
\end{align*}

Let assume an animal is detected, then missed. We have $\mathbf{y} = (2, 1)$. What is the contribution of this animal to the likelihood?

\begin{align*}
\Pr(\mathbf{y} = (2, 1)) &= \sum_{z_1 = 1}^2 \; \sum_{z_2 = 1}^2 w_{z_1, y_1 = 2} w_{z_2, y_2 = 1} \Pr(z_1) \gamma_{z_1, z_2} \color{white}{\Pr(z_{T-1}, z_{T-2},\ldots, z_1, z_1, z_1, z_1)}\\
&= \sum_{z_1 = 1}^2 \left( w_{z_1, y_1 = 2} w_{z_2 = 1, y_2 = 1} \Pr(z_1) \gamma_{z_1, z_2 = 1} + w_{z_1, y_1 = 2} w_{z_2 = 2, y_2 = 1} \Pr(z_1) \gamma_{z_1, z_2 = 2} \right) \\
&= w_{z_1 = 1, y_1 = 2} w_{z_2 = 1, y_2 = 1}\delta_1 \gamma_{z_1 = 1, z_2 = 1} + w_{z_1 = 1, y_1 = 2} w_{z_2 = 2, y_2 = 1} \delta_1 \gamma_{z_1 = 1, z_2 = 2}
\end{align*}

Note: $\Pr(z_1 = 1) = \delta_1 = 1$ and $\Pr(z_1 = 2) = 0$.

Let assume an animal is detected, then missed. We have $\mathbf{y} = (2, 1)$. What is the contribution of this animal to the likelihood?

\begin{align*}
\Pr(\mathbf{y} = (2, 1)) &= \sum_{z_1 = 1}^2 \; \sum_{z_2 = 1}^2 w_{z_1, y_1 = 2} w_{z_2, y_2 = 1} \Pr(z_1) \gamma_{z_1, z_2} \color{white}{\Pr(z_{T-1}, z_{T-2},\ldots, z_1, z_1, z_1, z_1)}\\
&= \sum_{z_1 = 1}^2 \left( w_{z_1, y_1 = 2} w_{z_2 = 1, y_2 = 1} \Pr(z_1) \gamma_{z_1, z_2 = 1} + w_{z_1, y_1 = 2} w_{z_2 = 2, y_2 = 1} \Pr(z_1) \gamma_{z_1, z_2 = 2} \right) \\
&= w_{z_1 = 1, y_1 = 2} w_{z_2 = 1, y_2 = 1} \delta_1 \gamma_{z_1 = 1, z_2 = 1} + w_{z_1 = 1, y_1 = 2} w_{z_2 = 2, y_2 = 1} \delta_1 \gamma_{z_1 = 1, z_2 = 2}\\
&= (1 - p) \phi + (1-\phi)
\end{align*}

Note: $w_{z_1 = 1, y_1 = 2} = \Pr(y_1 = 2 | z_1 = 1) = 1$ because we condition on first capture.

## Fitting HMM with NIMBLE

Our model so far:

\begin{align*}
   z_{\text{first}} &\sim \text{Categorical}(1, \delta) &\text{[likelihood]}\\
   z_t | z_{t-1} &\sim \text{Categorical}(1, \gamma_{z_{t-1},z_{t}}) &\text{[likelihood]}\\
   y_t | z_{t} &\sim \text{Categorical}(1, \omega_{z_{t}}) &\text{[likelihood]}\\
  \phi &\sim \text{Beta}(1, 1) &\text{[prior for }\phi \text{]} \\
  p &\sim \text{Beta}(1, 1) &\text{[prior for }p \text{]} \\
\end{align*}

Now our model has an observation layer for the $y$'s, conditional on the $z$. And we need a prior for the detection probability.

How to implement this model in Nimble?

```{r, echo=FALSE}
hmm.survival <- nimbleCode({
  phi ~ dunif(0, 1) # prior survival
  p ~ dunif(0, 1) # prior detection
  # likelihood
  gamma[1,1] <- phi      # Pr(alive t -> alive t+1)
  gamma[1,2] <- 1 - phi  # Pr(alive t -> dead t+1)
  gamma[2,1] <- 0        # Pr(dead t -> alive t+1)
  gamma[2,2] <- 1        # Pr(dead t -> dead t+1)
  delta[1] <- 1          # Pr(alive t = 1) = 1
  delta[2] <- 0          # Pr(dead t = 1) = 0
  omega[1,1] <- 1 - p    # Pr(alive t -> non-detected t)
  omega[1,2] <- p        # Pr(alive t -> detected t)
  omega[2,1] <- 1        # Pr(dead t -> non-detected t)
  omega[2,2] <- 0        # Pr(dead t -> detected t)
  for (i in 1:N){
    z[i,first[i]] ~ dcat(delta[1:2])
    for (j in (first[i]+1):T){
      z[i,j] ~ dcat(gamma[z[i,j-1], 1:2])
      y[i,j] ~ dcat(omega[z[i,j], 1:2])
    }
  }
})
```

Start with the priors:
```{r eval=FALSE}
hmm.survival <- nimbleCode({
  phi ~ dunif(0, 1) # prior survival
  p ~ dunif(0, 1) # prior detection
...
```

Then define initial states, transition and observation matrices:
```{r eval=FALSE}
...
  # parameters
  gamma[1,1] <- phi      # Pr(alive t -> alive t+1)
  gamma[1,2] <- 1 - phi  # Pr(alive t -> dead t+1)
  gamma[2,1] <- 0        # Pr(dead t -> alive t+1)
  gamma[2,2] <- 1        # Pr(dead t -> dead t+1)
  delta[1] <- 1          # Pr(alive t = 1) = 1
  delta[2] <- 0          # Pr(dead t = 1) = 0
  omega[1,1] <- 1 - p    # Pr(alive t -> non-detected t)
  omega[1,2] <- p        # Pr(alive t -> detected t)
  omega[2,1] <- 1        # Pr(dead t -> non-detected t)
  omega[2,2] <- 0        # Pr(dead t -> detected t)
...
```

Then the likelihood:
```{r eval=FALSE}
...
    # likelihood
    for (i in 1:N){
    z[i,first[i]] ~ dcat(delta[1:2])
    for (j in (first[i]+1):T){
      z[i,j] ~ dcat(gamma[z[i,j-1], 1:2])
      y[i,j] ~ dcat(omega[z[i,j], 1:2])
    }
  }
})
```

Overall, the code looks like:
```{r eval = FALSE}
hmm.survival <- nimbleCode({
  phi ~ dunif(0, 1) # prior survival
  p ~ dunif(0, 1) # prior detection
  # likelihood
  gamma[1,1] <- phi      # Pr(alive t -> alive t+1)
  gamma[1,2] <- 1 - phi  # Pr(alive t -> dead t+1)
  gamma[2,1] <- 0        # Pr(dead t -> alive t+1)
  gamma[2,2] <- 1        # Pr(dead t -> dead t+1)
  delta[1] <- 1          # Pr(alive t = 1) = 1
  delta[2] <- 0          # Pr(dead t = 1) = 0
  omega[1,1] <- 1 - p    # Pr(alive t -> non-detected t)
  omega[1,2] <- p        # Pr(alive t -> detected t)
  omega[2,1] <- 1        # Pr(dead t -> non-detected t)
  omega[2,2] <- 0        # Pr(dead t -> detected t)
  for (i in 1:N){
    z[i,first[i]] ~ dcat(delta[1:2])
    for (j in (first[i]+1):T){
      z[i,j] ~ dcat(gamma[z[i,j-1], 1:2])
      y[i,j] ~ dcat(omega[z[i,j], 1:2])
    }
  }
})
```


Now constants:
```{r}
first <- apply(y, 1, function(x) min(which(x !=0)))
my.constants <- list(N = nrow(y), T = 5, first = first)
my.constants
```

The data are made of 0s for non-detections and 1s for detections. To use the categorical distribution, we need to code 1, 2, etc. Value 0 is not accepted. Add 1 to get the correct format $y=1$ for non-detection and $y = 2$ for detection:
```{r}
my.data <- list(y = y + 1)
```

Initial values:
```{r}
zinits <- y + 1 # non-detection -> alive
zinits[zinits == 2] <- 1 # dead -> alive
initial.values <- function() list(phi = runif(1,0,1),
                                  p = runif(1,0,1),
                                  z = zinits)
```

Parameters to monitor:
```{r}
parameters.to.save <- c("phi", "p")
parameters.to.save
```

MCMC details:
```{r}
n.iter <- 5000
n.burnin <- 1000
n.chains <- 2
```

Run Nimble:
```{r, message=FALSE, eval = FALSE}
mcmc.output <- nimbleMCMC(code = hmm.survival,
                          constants = my.constants,
                          data = my.data,
                          inits = initial.values,
                          monitors = parameters.to.save,
                          niter = n.iter,
                          nburnin = n.burnin,
                          nchains = n.chains)
```

```{r, message=FALSE, cache = TRUE, echo = FALSE}
mcmc.output <- nimbleMCMC(code = hmm.survival,
                          constants = my.constants,
                          data = my.data,
                          inits = initial.values,
                          monitors = parameters.to.save,
                          niter = n.iter,
                          nburnin = n.burnin,
                          nchains = n.chains,
                          progressBar = FALSE)
```

Posterior distribution of survival:
```{r}
MCMCsummary(mcmc.output, round = 2)
```

The data is simulated, with true survival $\phi = 0.8$ and detection $p = 0.6$.

**Show simulation code. In all simulations, replace dbinom by dcat for coherence.**

## Marginalization

**Faire lien avec forward algorithm. Reprendre dHMM en la simplifiant au max et en faisant le lien avec l'algo. Match notation.**

**Faire une seule cohorte dans les simulations, tout le monde part de occasion 1, comme ça on ne gère pas la matrice d'observation at first encounter, ni le vecteur des first.**

**Il faut quand même ré-écrire dHMM et rHMM plus simplement, en enlevant tous les tests, et en reprenant le forward algorithm correctement, comme dans la vraisemblance que j'utilise en freq pour les HMM.**

**Ensuite, dans le chapitre sur la survie, les transitions et le multievent, on rentrera les first,  et obs matrix at first encounter.** 

### Theory

### Implementation in NIMBLE

Introduce NimbleEcology.

```{r eval = FALSE}
library(nimbleEcology)
```

Get the occasion of first capture for each individual:
```{r}
first <- apply(y, 1, function(x) min(which(x !=0)))
```

We filter out individuals that are first captured at last occasion. These individuals do not contribute to parameter estimation, and also they cause problems with nimbleEcology.
```{r}
mask <- which(first!=ncol(y)) # individuals that are not first encountered at last occasion
y <- y[mask, ]                # keep only these
first <- first[mask]
```

Model code:
```{r}
hmm.survival.new <- nimbleCode({
  phi ~ dunif(0, 1) # prior survival
  p ~ dunif(0, 1) # prior detection
  # likelihood
  gamma[1,1] <- phi      # Pr(alive t -> alive t+1)
  gamma[1,2] <- 1 - phi  # Pr(alive t -> dead t+1)
  gamma[2,1] <- 0        # Pr(dead t -> alive t+1)
  gamma[2,2] <- 1        # Pr(dead t -> dead t+1)
  delta[1] <- 1          # Pr(alive t = 1) = 1
  delta[2] <- 0          # Pr(dead t = 1) = 0
  omega[1,1] <- 1 - p    # Pr(alive t -> non-detected t)
  omega[1,2] <- p        # Pr(alive t -> detected t)
  omega[2,1] <- 1        # Pr(dead t -> non-detected t)
  omega[2,2] <- 0        # Pr(dead t -> detected t)
  for(i in 1:N) {
    init[i, 1:2] <- gamma[y[i, first[i] ] - 1, 1:2] # First state propagation
  }
  for (i in 1:N){
    y[i,(first[i]+1):T] ~ dHMM(init = init[i,1:2], 
                               probObs = omega[1:2,1:2], # observation matrix
                               probTrans = gamma[1:2,1:2], # transition matrix
                               len = T - first[i], # nb of sampling occasions
                               checkRowSums = 1) # do not check whether elements in a row sum to 1
  }
})
```

Rest is the same:
```{r cache = TRUE}
# Data and constants:
my.data <- list(y = y + 1)
my.constants <- list(N = nrow(y), T = ncol(y), first = first)

# Initial values:
zinits <- y + 1 # non-detection -> alive
zinits[zinits == 2] <- 1 # dead -> alive
initial.values <- function() list(phi = runif(1,0,1),
                                  p = runif(1,0,1))

# Parameters to monitor:
parameters.to.save <- c("phi", "p")
parameters.to.save

# MCMC details:
n.iter <- 5000
n.burnin <- 1000
n.chains <- 2

# Run NIMBLE:
mcmc.output <- nimbleMCMC(code = hmm.survival.new,
                          constants = my.constants,
                          data = my.data,
                          inits = initial.values,
                          monitors = parameters.to.save,
                          niter = n.iter,
                          nburnin = n.burnin,
                          nchains = n.chains)

# Numerical summaries:
MCMCsummary(mcmc.output, round = 2)
```

**Computer number of nodes. Show that the z are no longer in there.**

## Pooled encounter histories

In this section, we're gonna use NIMBLE functions to express the likelihood using pooled encounter histories. We use a vector mult that contains the number of individuals with a particular encounter history. We hacked the dHMM nimbleEcology function below. Courtesy of Chloé Nater. Implement of Turek et al. 

```{r}
dHMMweighted <- nimbleFunction(
  run = function (x = double(1), 
                  init = double(1), 
                  probObs = double(2),
                  probTrans = double(2), 
                  len = double(0),
                  mult = double(0), # NEWLY ADDED: argument stating number of occurrences 
                                    # of same encounter history in entire dataset 
                  log = integer(0, default = 0)) 
  {
    pi <- init
    logL <- 0
    for (t in 1:len) {
      Zpi <- probObs[, x[t]] * pi
      sumZpi <- sum(Zpi)
      logL <- logL + log(sumZpi) * mult # NEWLY ADDED
      if (t != len) 
        pi <- ((Zpi %*% probTrans)/sumZpi)[1, ]
    }
    if (log) 
      return(logL)
    return(exp(logL))
    returnType(double())
  })
```


Model code:
```{r}
hmm.survival.weighted <- nimbleCode({
  phi ~ dunif(0, 1) # prior survival
  p ~ dunif(0, 1) # prior detection
  # likelihood
  gamma[1,1] <- phi      # Pr(alive t -> alive t+1)
  gamma[1,2] <- 1 - phi  # Pr(alive t -> dead t+1)
  gamma[2,1] <- 0        # Pr(dead t -> alive t+1)
  gamma[2,2] <- 1        # Pr(dead t -> dead t+1)
  delta[1] <- 1          # Pr(alive t = 1) = 1
  delta[2] <- 0          # Pr(dead t = 1) = 0
  omega[1,1] <- 1 - p    # Pr(alive t -> non-detected t)
  omega[1,2] <- p        # Pr(alive t -> detected t)
  omega[2,1] <- 1        # Pr(dead t -> non-detected t)
  omega[2,2] <- 0        # Pr(dead t -> detected t)
  for(i in 1:N) {
    init[i, 1:2] <- gamma[y[i, first[i] ] - 1, 1:2] # First state propagation
  }
  for (i in 1:N){
    y[i,(first[i]+1):T] ~ dHMMweighted(init = init[i,1:2], 
                                       mult = mult[i],
                                       probObs = omega[1:2,1:2], # observation matrix
                                       probTrans = gamma[1:2,1:2], # transition matrix
                                       len = T - first[i], # nb of sampling occasions
                                       checkRowSums = 1) # do not check whether elements in a row sum to 1
  }
})
```


```{r}
set.seed(2022)
p <- 0.6
y <- z
y[z==2] <- 0
y[y==1] <- rbinom(n = sum(y==1), 1, p)
nobs <- sum(apply(y,1,sum) != 0)
y <- y[apply(y,1,sum) !=0, ]
first <- apply(y, 1, function(x) min(which(x !=0)))
for (i in 1:nobs){
  if(first[i] > 1) y[i, 1:(first[i]-1)] <- NA
}
y_weighted <- y %>% 
  as_tibble() %>% 
  group_by_all() %>% 
  summarise(mult = n()) %>% 
  relocate(mult) %>% 
  as.matrix()
head(y_weighted)
```

```{r}
mult <- y_weighted[,1] # nb of individuals w/ a particular encounter history
y <- y_weighted[,-1] # pooled data
```

There are XX individuals that were detected only once, XX individuals that were detected only once...

Get the occasion of first capture for each history:
```{r}
get.first <- function(x) min(which(x != 0))
first <- apply(y, 1, get.first)
```

Filter out individuals that are first captured at last occasion.
```{r}
mask <- which(first!=ncol(y))
y <- y[mask, ]
```

Apply filter on occasion of first capture and sample size.
```{r}
first <- first[mask]
mult <- mult[mask]
```

Rest is the same:
```{r}
# Data and constants:
my.data <- list(y = y + 1)
my.constants <- list(N = nrow(y), T = ncol(y), first = first, mult = mult)

# Initial values:
zinits <- y + 1 # non-detection -> alive
zinits[zinits == 2] <- 1 # dead -> alive
initial.values <- function() list(phi = runif(1,0,1),
                                  p = runif(1,0,1))

# Parameters to monitor:
parameters.to.save <- c("phi", "p")
parameters.to.save

# MCMC details:
n.iter <- 5000
n.burnin <- 1000
n.chains <- 2
```

This won't work. You need a rHMMweighted. 
```{r eval = FALSE}
# Run NIMBLE:
mcmc.output <- nimbleMCMC(code = hmm.survival.weighted,
                          constants = my.constants,
                          data = my.data,
                          inits = initial.values,
                          monitors = parameters.to.save,
                          niter = n.iter,
                          nburnin = n.burnin,
                          nchains = n.chains)
```

We need a `rHMMweighted` as well. 
```{r}
rHMMweighted <- nimbleFunction(
  run = function(n = integer(),    ## Observed capture (state) history
                 init = double(1),
                 probObs = double(2),
                 probTrans = double(2),
                 len = double(0),
                 mult = double(0),
                 checkRowSums = double(0, default = 1)) {
    returnType(double(1))
    if (dim(probObs)[1] != dim(probTrans)[1]) stop("In rHMM: Number of cols in probObs must equal number of cols in probTrans.")
    if (dim(probTrans)[1] != dim(probTrans)[2]) stop("In rHMM: probTrans must be a square matrix.")
    if (abs(sum(init) - 1) > 1e-06) stop("In rHMM: Initial probabilities must sum to 1.")
    if (checkRowSums) {
      transCheckPasses <- TRUE
      for (i in 1:dim(probTrans)[1]) {
        thisCheckSum <- sum(probTrans[i,])
        if (abs(thisCheckSum - 1) > 1e-6) {
          ## Compilation doesn't support more than a simple string for stop()
          ## so we provide more detail using a print().
          print("In rHMM: Problem with sum(probTrans[i,]) with i = ", i, ". The sum should be 1 but is ", thisCheckSum)
          transCheckPasses <- FALSE
        }
      }
      obsCheckPasses <- TRUE
      for (i in 1:dim(probObs)[1]) {
        thisCheckSum <- sum(probObs[i,])
        if (abs(thisCheckSum - 1) > 1e-6) {
          print("In rHMM: Problem with sum(probObs[i,]) with i = ", i, ". The sum should be 1 but is ", thisCheckSum)
          obsCheckPasses <- FALSE
        }
      }
      if(!(transCheckPasses | obsCheckPasses))
        stop("In rHMM: probTrans and probObs were not specified correctly.  Probabilities in each row (second dimension) must sum to 1.")
      if(!transCheckPasses)
        stop("In rHMM: probTrans was not specified correctly.  Probabilities in each row (second dimension) must sum to 1.")
      if(!obsCheckPasses)
        stop("In rHMM: probObs was not specified correctly. Probabilities in each row must sum to 1.")
    }
    ans <- numeric(len)
    probInit <- init
    trueInit <- 0
    r <- runif(1, 0, 1)
    j <- 1
    while (r > sum(probInit[1:j])) j <- j + 1
    trueInit <- j
    trueState <- trueInit
    for (i in 1:len) {
      # Transition to a new true state
      r <- runif(1, 0, 1)
      j <- 1
      while (r > sum(probTrans[trueState, 1:j])) j <- j + 1
      trueState <- j
      # Detect based on the true state
      r <- runif(1, 0, 1)
      j <- 1
      while (r > sum(probObs[trueState, 1:j])) j <- j + 1
      ans[i] <- j
    }
    return(ans)
  })
```

```{r eval = FALSE}
# Run NIMBLE:
mcmc.output <- nimbleMCMC(code = hmm.survival.weighted,
                          constants = my.constants,
                          data = my.data,
                          inits = initial.values,
                          monitors = parameters.to.save,
                          niter = n.iter,
                          nburnin = n.burnin,
                          nchains = n.chains)

# Numerical summaries:
MCMCsummary(mcmc.output, round = 2)
```

**Something doesn't work. I found a fix during the workshop, but can't make it work here. Try and simplify the fn first, and make sense of rHMM. Then resume.**

## Decoding after marginalization

https://discourse.mc-stan.org/t/how-to-get-the-best-state-sequence-using-viterbi-algorithm/18969

https://rdrr.io/bioc/STAN/man/getViterbi.html

## Summary

+ Blabla.

+ Blabla.

+ Recall 3 problems of HMM?

## Suggested reading

+ Zucchini, MacDonald and Langrock (2016) [Hidden Markov Models for Time Series: An Introduction Using R (2nd ed)](https://www.routledge.com/Hidden-Markov-Models-for-Time-Series-An-Introduction-Using-R-Second-Edition/Zucchini-MacDonald-Langrock/p/book/9781482253832). Chapman and Hall/CRC.

+ McClintock, B.T., Langrock, R., Gimenez, O., Cam, E., Borchers, D.L., Glennie, R. and Patterson, T.A. (2020), [Uncovering ecological state dynamics with hidden Markov models](https://onlinelibrary.wiley.com/doi/full/10.1111/ele.13610). Ecology Letters, 23: 1878-1903.

+  Yackulic, C. B. Dodrill, M., Dzul, M., Sanderlin, J. S., and Reid, J. A.. (2020). [A need for speed in Bayesian population models: a practical guide to marginalizing and recovering discrete latent states](https://esajournals.onlinelibrary.wiley.com/doi/abs/10.1002/eap.2112). Ecological Applications 30:e02112.

+ L. R. Rabiner (1989). [A tutorial on hidden Markov models and selected applications in speech recognition](https://web.ece.ucsb.edu/Faculty/Rabiner/ece259/Reprints/tutorial%20on%20hmm%20and%20applications.pdf). Proceedings of the IEEE, 77:257-286.

<!-- heller_novel_2021 -->
<!-- Gimenez et al. 2007, Royle 2008. Recall difference between SSM and HMM? -->