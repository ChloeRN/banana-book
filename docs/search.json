[{"path":"index.html","id":"welcome","chapter":"Welcome","heading":"Welcome","text":"Welcome website book Bayesian Analysis Capture-Recapture Data Hidden Markov Models – Theory Case Studies R Olivier Gimenez. Note book also available PDF format.’m currently writing book, welcome feedback requests content .Many thanks!Last updated: August 26, 2021","code":""},{"path":"preface.html","id":"preface","chapter":"Preface","heading":"Preface","text":"HMM framework gained much attention ecological literature last decade, suggested general modelling framework demography plant animal populations. particular, HMMs increasingly used analyse capture-recapture data estimate key population parameters (e.g., survival, dispersal, recruitment abundance) applications fields ecology. parallel, Bayesian statistics relatively well established fast growing ecology related disciplines, resonates scientific reasoning allows accommodating uncertainty smoothly. popularity Bayesian statistics also comes availability free pieces software (WinBUGS, OpenBUGS, JAGS, Stan, nimble) allow practitioners code analyses.However, knowledge, full Bayesian treatment HMMs applied capture-recapture data yet proposed book. propose book. Besides, popular software solutions come computational limitations ecologists deal complex models /big data. use Nimble seen many future ecological data modelling extends BUGS language writing new functions distributions, provides samplers can deal discrete latent states contrast Stan.book, cover theory HMMs capture-recapture data, applications models empower practitioners fit models confidence. important part book consist case studies presented tutorial style abide “learning ” philosophy.\nonline version book licensed Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. ","code":""},{"path":"preface.html","id":"why-read-this-book","chapter":"Preface","heading":"Why read this book","text":"","code":""},{"path":"preface.html","id":"structure-of-the-book","chapter":"Preface","heading":"Structure of the book","text":"Blabla.","code":""},{"path":"preface.html","id":"software-information-and-conventions","chapter":"Preface","heading":"Software information and conventions","text":"book uses primarily R package nimble, need least install R nimble package.R session information compiling book shown :add prompts (> +) R source code book, comment text output two hashes ## default, can see R session information . convenience want copy run code (text output ignored since commented ). Package names bold text (e.g., nimble), inline code filenames formatted typewriter font (e.g., knitr::knit('foo.Rmd')). Function names followed parentheses (e.g., nimble::nimbleCode()). double-colon operator :: means accessing object package.","code":"\nsessionInfo()\n## R version 4.1.0 (2021-05-18)\n## Platform: x86_64-apple-darwin17.0 (64-bit)\n## Running under: macOS Catalina 10.15.7\n## \n## Matrix products: default\n## BLAS:   /System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/vecLib.framework/Versions/A/libBLAS.dylib\n## LAPACK: /Library/Frameworks/R.framework/Versions/4.1/Resources/lib/libRlapack.dylib\n## \n## locale:\n## [1] fr_FR.UTF-8/fr_FR.UTF-8/fr_FR.UTF-8/C/C/fr_FR.UTF-8\n## \n## attached base packages:\n## [1] stats     graphics  grDevices utils     datasets \n## [6] methods   base     \n## \n## other attached packages:\n##  [1] pdftools_3.0.1   magick_2.7.3     MCMCvis_0.15.3  \n##  [4] nimble_0.11.1    forcats_0.5.1    stringr_1.4.0   \n##  [7] dplyr_1.0.7      purrr_0.3.4.9000 readr_2.0.0     \n## [10] tidyr_1.1.3      tibble_3.1.3     ggplot2_3.3.5   \n## [13] tidyverse_1.3.1 \n## \n## loaded via a namespace (and not attached):\n##  [1] Rcpp_1.0.7        lubridate_1.7.10 \n##  [3] lattice_0.20-44   assertthat_0.2.1 \n##  [5] digest_0.6.27     utf8_1.2.2       \n##  [7] R6_2.5.0          cellranger_1.1.0 \n##  [9] backports_1.2.1   reprex_2.0.1     \n## [11] evaluate_0.14     coda_0.19-4      \n## [13] httr_1.4.2        pillar_1.6.2     \n## [15] rlang_0.4.11      readxl_1.3.1     \n## [17] rstudioapi_0.13   jquerylib_0.1.4  \n## [19] qpdf_1.1          rmarkdown_2.10   \n## [21] igraph_1.2.6      munsell_0.5.0    \n## [23] broom_0.7.9       compiler_4.1.0   \n## [25] modelr_0.1.8      xfun_0.25        \n## [27] askpass_1.1       pkgconfig_2.0.3  \n## [29] htmltools_0.5.1.1 downlit_0.2.1    \n## [31] tidyselect_1.1.1  bookdown_0.23    \n## [33] fansi_0.5.0       crayon_1.4.1     \n## [35] tzdb_0.1.2        dbplyr_2.1.1     \n## [37] withr_2.4.2       grid_4.1.0       \n## [39] jsonlite_1.7.2    gtable_0.3.0     \n## [41] lifecycle_1.0.0   DBI_1.1.1        \n## [43] magrittr_2.0.1    scales_1.1.1     \n## [45] cli_3.0.1         stringi_1.7.3    \n## [47] fs_1.5.0          xml2_1.3.2       \n## [49] bslib_0.2.5.1     ellipsis_0.3.2   \n## [51] generics_0.1.0    vctrs_0.3.8      \n## [53] tools_4.1.0       glue_1.4.2       \n## [55] hms_1.1.0         parallel_4.1.0   \n## [57] yaml_2.2.1        colorspace_2.0-2 \n## [59] rvest_1.0.1       knitr_1.33       \n## [61] haven_2.4.3       sass_0.4.0"},{"path":"preface.html","id":"acknowledgments","chapter":"Preface","heading":"Acknowledgments","text":"CNRS. Jean-. Roger. Rémi. students. Chloé, Sarah, Perry, Daniel. Rob Chapman & Hall/CRC. Workshop attendees. Feedback . FIP radio. Marc Kéry support advice write book. Proofreading . family.\nOlivier Gimenez\nMontpellier, France\n","code":""},{"path":"about-the-author.html","id":"about-the-author","chapter":"About the Author","heading":"About the Author","text":"Je m’appelle Olivier Gimenez (https://oliviergimenez.github.io/). Je suis directeur de recherche au CNRS. Après des études universitaires en mathématiques, j’ai fait une thèse en statistiques pour l’écologie. J’ai passé mon Habilitation à Diriger des Recherches en écologie et évolution. Récemment, je suis retourné sur les bancs de l’université pour m’initier à la sociologie.J’ai écrit des articles scientifiques faisant appel à la statistique bayésienne, et co-écrit avec des collègues britanniques un livre sur les analyses bayésiennes pour l’écologie des populations.Vous pouvez retrouver sur Twitter (https://twitter.com/oaggimenez), ou bien contacter via mon adresse email qui s’écrit olivier suivi d’un point puis gimenez, ensuite arobase, puis cefe, suivi d’un point, puis cnrs, suivi d’un point et pour terminer fr.Tombé dedans quand j’étais petit. Obélix Roger et Astérix JD.","code":""},{"path":"crashcourse.html","id":"crashcourse","chapter":"1 Bayesian statistics & MCMC","heading":"1 Bayesian statistics & MCMC","text":"","code":""},{"path":"crashcourse.html","id":"bayes-theorem","chapter":"1 Bayesian statistics & MCMC","heading":"1.1 Bayes’ theorem","text":"theorem conditional probabilities.\\(\\Pr(B \\mid ) = \\displaystyle{\\frac{ \\Pr(\\mid B) \\; \\Pr(B)}{\\Pr()}}\\)\nFigure 1.1: Bayes’ theorem spelt blue neon offices Autonomy Cambridge. Source: Wikipedia\nalways forget letters mean.Might easier remember written like :\\[ \\Pr(\\text{hypothesis} \\mid \\text{data}) = \\frac{ \\Pr(\\text{data} \\mid \\text{hypothesis}) \\; \\Pr(\\text{hypothesis})}{\\Pr(\\text{data})} \\]“hypothesis” typically something unobserved unknown. ’s want learn using data.regression models, “hypothesis” parameter (intercept, slopes error terms).Bayes theorem tells probability hypothesis given data.Cool science ?plausible hypothesis given data?\\[ \\Pr(\\text{hypothesis} \\mid \\text{data}) = \\frac{ \\Pr(\\text{data} \\mid \\text{hypothesis}) \\; \\Pr(\\text{hypothesis})}{\\Pr(\\text{data})} \\]Bayesian reasoning echoes scientific reasoning. might ask , Bayesian statistics default?may ask: Bayesian statistics default?Due practical problems implementing Bayesian approach, futile wars (male) statisticians, little progress made two centuries.Recent advances computational power coupled development new methodology led great increase application Bayesian methods within last two decades.","code":""},{"path":"crashcourse.html","id":"frequentist-versus-bayesian","chapter":"1 Bayesian statistics & MCMC","heading":"1.2 Frequentist versus Bayesian","text":"Typical stats problems involve estimating parameter \\(\\theta\\) available data.frequentist approach (maximum likelihood estimation – MLE) assumes parameters fixed, unknown values estimated.Classical estimates generally point estimates parameters interest.Bayesian approach assumes parameters fixed fixed unknown distribution - distribution parameter.","code":""},{"path":"crashcourse.html","id":"what-is-the-bayesian-approach","chapter":"1 Bayesian statistics & MCMC","heading":"1.3 What is the Bayesian approach?","text":"approach based upon idea experimenter begins prior beliefs system.never start scratch.updates beliefs basis observed data.updating procedure based upon Bayes’ Theorem:\\[\\Pr(\\mid B) = \\frac{\\Pr(B \\mid ) \\; \\Pr()}{\\Pr(B)}\\]Schematically \\(= \\theta\\) \\(B = \\text{data}\\), thenThe Bayes’ theorem\\[\\Pr(\\mid B) = \\frac{\\Pr(B \\mid ) \\; \\Pr()}{\\Pr(B)}\\]Translates :\\[\\Pr(\\theta \\mid \\text{data}) = \\frac{\\Pr(\\text{data} \\mid \\theta) \\; \\Pr(\\theta)}{\\Pr(\\text{data})}\\]","code":""},{"path":"crashcourse.html","id":"bayes-theorem-1","chapter":"1 Bayesian statistics & MCMC","heading":"1.4 Bayes’ theorem","text":"\\[{\\color{red}{\\Pr(\\theta \\mid \\text{data})}} = \\frac{\\color{blue}{\\Pr(\\text{data} \\mid \\theta)} \\; \\color{green}{\\Pr(\\theta)}}{\\color{orange}{\\Pr(\\text{data})}}\\]\\(\\color{red}{\\text{Posterior distribution}}\\): Represents know seen data. basis inference, distribution, possibly multivariate one parameter.\\(\\color{blue}{\\text{Likelihood}}\\): quantity MLE approach.\\(\\color{green}{\\text{Prior distribution}}\\): Represents know seeing data. source much discussion Bayesian approach.\\(\\color{orange}{\\Pr(\\text{data}) = \\int{L(\\text{data} \\mid \\theta)\\Pr(\\theta) d\\theta}}\\) \\(N\\)-dimensional integral \\(\\theta = \\theta_1, \\ldots, \\theta_N\\).Difficult impossible calculate. one reasons need simulation (MCMC) methods.","code":""},{"path":"crashcourse.html","id":"brute-force-via-numerical-integration","chapter":"1 Bayesian statistics & MCMC","heading":"1.5 Brute force via numerical integration","text":"Say release \\(n\\) animals beginning winter, \\(y\\) survive, ’d like estimate winter survival \\(\\theta\\).model:\\[\\begin{align*}\ny &\\sim \\text{Binomial}(n, \\theta) &\\text{[likelihood]}\n\\\\\n\\theta &\\sim \\text{Beta}(1, 1) &\\text{[prior }\\theta \\text{]} \\\\ \n\\end{align*}\\]","code":"\ny <- 19 # nb of success\nn <- 57 # nb of attempts"},{"path":"crashcourse.html","id":"beta-prior","chapter":"1 Bayesian statistics & MCMC","heading":"1.6 Beta prior","text":"","code":""},{"path":"crashcourse.html","id":"apply-bayes-theorem","chapter":"1 Bayesian statistics & MCMC","heading":"1.7 Apply Bayes theorem","text":"Likelihood times prior: \\(\\Pr(\\text{data} \\mid \\theta) \\; \\Pr(\\theta)\\)Averaged likelihood: \\(\\Pr(\\text{data}) = \\int{L(\\theta \\mid \\text{data}) \\; \\Pr(\\theta) d\\theta}\\)","code":"\nnumerator <- function(p) dbinom(y,n,p) * dbeta(p,a,b)\ndenominator <- integrate(numerator,0,1)$value"},{"path":"crashcourse.html","id":"posterior-via-numerical-integration","chapter":"1 Bayesian statistics & MCMC","heading":"1.8 Posterior via numerical integration","text":"","code":""},{"path":"crashcourse.html","id":"superimpose-explicit-posterior","chapter":"1 Bayesian statistics & MCMC","heading":"1.9 Superimpose explicit posterior","text":"","code":""},{"path":"crashcourse.html","id":"and-the-prior","chapter":"1 Bayesian statistics & MCMC","heading":"1.10 And the prior","text":"","code":""},{"path":"crashcourse.html","id":"what-if-multiple-parameters","chapter":"1 Bayesian statistics & MCMC","heading":"1.11 What if multiple parameters?","text":"Example linear regression parameters \\(\\alpha\\), \\(\\beta\\) \\(\\sigma\\) estimated.Bayes’ theorem says:\\[ P(\\alpha, \\beta, \\sigma \\mid \\text{data}) = \\frac{ P(\\text{data} \\mid \\alpha, \\beta, \\sigma) \\, P(\\alpha, \\beta, \\sigma)}{\\iiint \\, P(\\text{data} \\mid \\alpha, \\beta, \\sigma) \\, P(\\alpha, \\beta, \\sigma) \\,d\\alpha \\,d\\beta \\,d\\sigma} \\]really wish calculate 3D integral?","code":""},{"path":"crashcourse.html","id":"bayesian-computation","chapter":"1 Bayesian statistics & MCMC","heading":"1.12 Bayesian computation","text":"early 1990s, statisticians rediscovered work 1950’s physics.Use stochastic simulation draw samples posterior distributions.Avoid explicit calculation integrals Bayes formula.Instead, approx. posterior w/ precision drawing large samples.Markov chain Monte Carlo (MCMC) gives boost Bayesian statistics!","code":"\nknitr::include_graphics(\"images/metropolis.png\")"},{"path":"crashcourse.html","id":"why-are-mcmc-methods-so-useful","chapter":"1 Bayesian statistics & MCMC","heading":"1.13 Why are MCMC methods so useful?","text":"MCMC stochastic algorithms produce sequence dependent random numbers Markov chain.MCMC stochastic algorithms produce sequence dependent random numbers Markov chain.Markov chain discrete sequence states, probability event depends state previous event.Markov chain discrete sequence states, probability event depends state previous event.Markov chain equilibrium (aka stationary) distribution.Markov chain equilibrium (aka stationary) distribution.Equilibrium distribution desired posterior distribution!Equilibrium distribution desired posterior distribution!Several ways constructing chains: e.g., Metropolis-Hastings, Gibbs sampler.Several ways constructing chains: e.g., Metropolis-Hastings, Gibbs sampler.implement practice?!implement practice?!","code":""},{"path":"crashcourse.html","id":"the-metropolis-algorithm","chapter":"1 Bayesian statistics & MCMC","heading":"1.14 The Metropolis algorithm","text":"Let’s go back animal survival estimation.Let’s go back animal survival estimation.illustrate sampling posterior distribution.illustrate sampling posterior distribution.write functions R likelihood, prior posterior.write functions R likelihood, prior posterior.","code":"\n# survival data, 19 \"success\" out of 57 \"attempts\"\nsurvived <- 19\nreleased <- 57\n\n# log-likelihood function\nloglikelihood <- function(x, p){\n  dbinom(x = x, size = released, prob = p, log = TRUE)\n}\n\n# prior density\nlogprior <- function(p){\n  dunif(x = p, min = 0, max = 1, log = TRUE)\n}\n\n# posterior density function (log scale)\nposterior <- function(x, p){\n  loglikelihood(x, p) + logprior(p) # - log(Pr(data))\n}"},{"path":"crashcourse.html","id":"metropolis-algorithm","chapter":"1 Bayesian statistics & MCMC","heading":"1.15 Metropolis algorithm","text":"start possible value parameter estimated.start possible value parameter estimated.decide visit next, propose move away current value parameter — candidate value. , add current value random value say normal distribution variance.decide visit next, propose move away current value parameter — candidate value. , add current value random value say normal distribution variance.compute ratio probabilities candidate current locations \\(R = \\text{posterior(candidate)/posterior(current)}\\). magic MCMC happens, \\(\\Pr(\\text{data})\\), denominator Bayes theorem, cancels .compute ratio probabilities candidate current locations \\(R = \\text{posterior(candidate)/posterior(current)}\\). magic MCMC happens, \\(\\Pr(\\text{data})\\), denominator Bayes theorem, cancels .spin continuous spinner lands anywhere 0 1 — call random spin \\(X\\). \\(X\\) smaller \\(R\\), move candidate location, otherwise remain current location.spin continuous spinner lands anywhere 0 1 — call random spin \\(X\\). \\(X\\) smaller \\(R\\), move candidate location, otherwise remain current location.repeat 2-4 number times — steps (many steps).repeat 2-4 number times — steps (many steps).Starting value \\(0.5\\) running algorithm \\(100\\) iterations.","code":"\n# propose candidate value\nmove <- function(x, away = .2){ \n  logitx <- log(x / (1 - x))\n  logit_candidate <- logitx + rnorm(1, 0, away)\n  candidate <- plogis(logit_candidate)\n  return(candidate)\n}\n\n# set up the scene\nsteps <- 100\ntheta.post <- rep(NA, steps)\nset.seed(1234)\n\n# pick starting value (step 1)\ninits <- 0.5\ntheta.post[1] <- inits\nfor (t in 2:steps){ # repeat steps 2-4 (step 5)\n  \n  # propose candidate value for prob of success (step 2)\n  theta_star <- move(theta.post[t-1])\n  \n  # calculate ratio R (step 3)\n  pstar <- posterior(survived, p = theta_star)  \n  pprev <- posterior(survived, p = theta.post[t-1])\n  logR <- pstar - pprev\n  R <- exp(logR)\n  \n  # decide to accept candidate value or to keep current value (step 4)\n  accept <- rbinom(1, 1, prob = min(R, 1))\n  theta.post[t] <- ifelse(accept == 1, theta_star, theta.post[t-1])\n}\nhead(theta.post)\n## [1] 0.5000 0.4399 0.4399 0.4577 0.4577 0.4577\ntail(theta.post)\n## [1] 0.4146 0.3772 0.3772 0.3861 0.3899 0.3624"},{"path":"crashcourse.html","id":"a-chain","chapter":"1 Bayesian statistics & MCMC","heading":"1.16 A chain","text":"","code":""},{"path":"crashcourse.html","id":"another-chain","chapter":"1 Bayesian statistics & MCMC","heading":"1.17 Another chain","text":"","code":""},{"path":"crashcourse.html","id":"with-5000-steps","chapter":"1 Bayesian statistics & MCMC","heading":"1.18 With 5000 steps","text":"yellow: posterior mean; red: maximum likelihood estimate.","code":""},{"path":"crashcourse.html","id":"animating-mcmc---1d-example-code-here","chapter":"1 Bayesian statistics & MCMC","heading":"1.18.1 Animating MCMC - 1D example (code here)","text":"","code":"\nknitr::include_graphics(\"images/112546886-56862f00-8dba-11eb-81a0-465434672bdd.gif\")"},{"path":"crashcourse.html","id":"animating-mcmc---2d-example","chapter":"1 Bayesian statistics & MCMC","heading":"1.18.2 Animating MCMC - 2D example","text":"Code .","code":"\nknitr::include_graphics(\"images/create-gif.gif\")"},{"path":"crashcourse.html","id":"the-mcmc-interactive-gallery-more-here","chapter":"1 Bayesian statistics & MCMC","heading":"1.18.3 The MCMC Interactive Gallery (more here)","text":"","code":"\nknitr::include_graphics(\"images/galery.png\")"},{"path":"crashcourse.html","id":"assessing-convergence","chapter":"1 Bayesian statistics & MCMC","heading":"1.19 Assessing convergence","text":"MCMC algorithms can used construct Markov chain given stationary distribution (set posterior distribution).MCMC algorithms can used construct Markov chain given stationary distribution (set posterior distribution).MCMC algorithm, posterior distribution needed known proportionality.MCMC algorithm, posterior distribution needed known proportionality.stationary distribution reached, can regard realisations chain (dependent) sample posterior distribution (obtain Monte Carlo estimates).stationary distribution reached, can regard realisations chain (dependent) sample posterior distribution (obtain Monte Carlo estimates).consider important implementation issues.consider important implementation issues.","code":""},{"path":"crashcourse.html","id":"mixing-and-autocorrelation","chapter":"1 Bayesian statistics & MCMC","heading":"1.20 Mixing and autocorrelation","text":"movement around parameter space often referred mixing.movement around parameter space often referred mixing.Traceplots small big moves provide (relatively) high correlations (known autocorrelations) successive observations Markov chain.Traceplots small big moves provide (relatively) high correlations (known autocorrelations) successive observations Markov chain.Strongly correlated observations require large sample sizes therefore longer simulations.Strongly correlated observations require large sample sizes therefore longer simulations.Autocorrelation function (ACF) plots convenient way displaying strength autocorrelation given sample values.Autocorrelation function (ACF) plots convenient way displaying strength autocorrelation given sample values.ACF plots provide autocorrelation successively sampled values separated \\(k\\) iterations, referred lag, (.e. \\(\\text{cor}(\\theta_t, \\theta_{t+k})\\)) increasing values \\(k\\).ACF plots provide autocorrelation successively sampled values separated \\(k\\) iterations, referred lag, (.e. \\(\\text{cor}(\\theta_t, \\theta_{t+k})\\)) increasing values \\(k\\).","code":""},{"path":"crashcourse.html","id":"how-do-good-chains-behave","chapter":"1 Bayesian statistics & MCMC","heading":"1.21 How do good chains behave?","text":"Converge target distribution; discard realisations Markov chain convergence achieved.Converge target distribution; discard realisations Markov chain convergence achieved., explore efficiently: post-convergence sample size required suitable numerical summaries., explore efficiently: post-convergence sample size required suitable numerical summaries.Therefore, looking determine long takes Markov chain converge stationary distribution.Therefore, looking determine long takes Markov chain converge stationary distribution.practice, must discard observations start chain just use observations chain converged.practice, must discard observations start chain just use observations chain converged.initial observations discard referred burn-.initial observations discard referred burn-.Simplest method determine length burn-period look trace plots.Simplest method determine length burn-period look trace plots.","code":""},{"path":"crashcourse.html","id":"burn-in","chapter":"1 Bayesian statistics & MCMC","heading":"1.22 Burn-in","text":"simulations cheap, conservative.","code":""},{"path":"crashcourse.html","id":"effective-sample-size-n.eff","chapter":"1 Bayesian statistics & MCMC","heading":"1.23 Effective sample size n.eff","text":"long chain needed produce stable estimates ?MCMC chains strongly autocorrelated.Successive steps near , independent.effective sample size (n.eff) measures chain length taking account autocorrelation chain.n.eff less number MCMC iterations.Check n.eff every parameter interest.Check n.eff interesting parameter combinations.need \\(\\text{n.eff} \\geq 100\\) independent steps.","code":""},{"path":"crashcourse.html","id":"potential-scale-reduction-factor","chapter":"1 Bayesian statistics & MCMC","heading":"1.24 Potential scale reduction factor","text":"Gelman-Rubin statistic \\(\\hat{R}\\)Measures ratio total variability combining multiple chains (-chain plus within-chain) within-chain variability.Asks question chain effect? much alike \\(F\\) test ANOVA.Values near \\(1\\) indicates likely convergence, value \\(\\leq 1.1\\) considered acceptable.Necessary condition, sufficient; words, diagnostics tell converged sure, .","code":""},{"path":"crashcourse.html","id":"to-sum-up","chapter":"1 Bayesian statistics & MCMC","heading":"1.25 To sum up","text":"Run multiple chains arbitrary starting places (initial values).Assume convergence chains reach regimeDiscard initial burn-phase.Proceed posterior inference.Use traceplot, effective sample size \\(\\hat{R}\\).","code":""},{"path":"crashcourse.html","id":"what-if-you-have-issues-of-convergence","chapter":"1 Bayesian statistics & MCMC","heading":"1.26 What if you have issues of convergence?","text":"Increase burn-, sample .Use informative priors.Pick better initial values (good guess), using e.g. estimates simpler models.Reparameterize:Standardize covariates.Non-centering: \\(\\alpha \\sim N(0,\\sigma)\\) becomes \\(\\alpha = z \\sigma\\) \\(z \\sim N(0,1)\\).Something wrong model?Start simpler model (remove complexities).Use simulations.Change sampler. later .","code":""},{"path":"crashcourse.html","id":"further-reading","chapter":"1 Bayesian statistics & MCMC","heading":"1.27 Further reading","text":"McCarthy, M. (2007). Bayesian Methods Ecology. Cambridge: Cambridge University Press.McCarthy, M. (2007). Bayesian Methods Ecology. Cambridge: Cambridge University Press.McElreath, R. (2020). Statistical Rethinking: Bayesian Course Examples R Stan (2nd ed.). CRC Press.McElreath, R. (2020). Statistical Rethinking: Bayesian Course Examples R Stan (2nd ed.). CRC Press.Gelman, . Hill, J. (2006). Data Analysis Using Regression Multilevel/Hierarchical Models (Analytical Methods Social Research). Cambridge: Cambridge University Press.Gelman, . Hill, J. (2006). Data Analysis Using Regression Multilevel/Hierarchical Models (Analytical Methods Social Research). Cambridge: Cambridge University Press.","code":""},{"path":"intronimble.html","id":"intronimble","chapter":"2 Introduction to Nimble","heading":"2 Introduction to Nimble","text":"","code":""},{"path":"intronimble.html","id":"what-is-nimble","chapter":"2 Introduction to Nimble","heading":"2.1 What is Nimble?","text":"\nFigure 2.1: (Meme created Todd Arnold’s wonderful students)\n\nFigure 2.2: (Meme created Todd Arnold’s wonderful students)\nNumerical Inference statistical Models using Bayesian Likelihood Estimation.Numerical Inference statistical Models using Bayesian Likelihood Estimation.framework hierarchical statistical models algorithms.framework hierarchical statistical models algorithms.Uses almost model code WinBUGS, OpenBUGS, JAGS.Uses almost model code WinBUGS, OpenBUGS, JAGS.extension BUGS language: additional syntax, custom functions distributions.extension BUGS language: additional syntax, custom functions distributions.configurable system MCMC.configurable system MCMC.library methods (SMC, MCEM).library methods (SMC, MCEM).Sequential Monte Carlo (particle filtering)Sequential Monte Carlo (particle filtering)Monte Carlo Expectation Maximization (maximum likelihood)Monte Carlo Expectation Maximization (maximum likelihood)model-generic programming system write new analysis methods.model-generic programming system write new analysis methods.","code":""},{"path":"intronimble.html","id":"load-nimble-package","chapter":"2 Introduction to Nimble","heading":"2.2 Load nimble package","text":"","code":"\nlibrary(nimble)"},{"path":"intronimble.html","id":"build-model-made-of-likelihood-and-priors","chapter":"2 Introduction to Nimble","heading":"2.3 Build model, made of likelihood and priors","text":"","code":"\nnaive.survival.model <- nimbleCode({\n  # prior\n  phi ~ dunif(0, 1)\n  # likelihood\n  y ~ dbinom(phi, n)\n})"},{"path":"intronimble.html","id":"syntax-whats-newbetterdifferent","chapter":"2 Introduction to Nimble","heading":"2.4 Syntax: what’s new/better/different?","text":"VectorizationMore flexible specification distributionsYour functions distributionsThe end empty indices& …","code":"\n# JAGS (& Nimble)\nfor(t in 1:Tmax){\n  x[t] <- Mu.x + epsilon[t]\n}\n\n# Nimble\nx[1:Tmax] <- Mu.x + epsilon[1:Tmax]\n# JAGS (& Nimble)\nfor(t in 1:Tmax){\n  epsilon[t] ~ dnorm(0, tau)\n}\ntau <- pow(sigma, -2)\nsigma ~ dunif(0, 5)\n\n# Nimble\nfor(t in 1:Tmax){\n  epsilon[t] ~ dnorm(0, sd = sigma)\n}\nsigma ~ dunif(0, 5)\nx[1:Tmax] <- myNimbleFunction(a = Mu.x, b = epsilon[1:Tmax])\nsigma ~ dCustomDistr(c = 0.5, z = 10)\n# JAGS\nsum.x <- sum(x[])\n\n# Nimble\nsum.x <- sum(x[1:Tmax])"},{"path":"intronimble.html","id":"read-in-data","chapter":"2 Introduction to Nimble","heading":"2.5 Read in data","text":"Back naive survival model:","code":"\nnaive.survival.model <- nimbleCode({\n  # prior\n  phi ~ dunif(0, 1)\n  # likelihood\n  y ~ dbinom(phi, n)\n})\nmy.data <- list(n = 57, y = 19)"},{"path":"intronimble.html","id":"distinguish-constants-and-data","chapter":"2 Introduction to Nimble","heading":"2.6 Distinguish constants and data","text":"Nimble, “data” data…Constants:\n+ Can never changed\n+ Must provided model defined (part model structure)\n+ E.g. vector known index values, variables used define -loops, etc.Nimble, “data” data…Data:\n+ Can changed without re-building model\n+ Can (re-)simulated within model\n+ E.g. stuff appears left “~”computational efficiency, better specify much possible constants.Nimble help !","code":"\nmy.constants <- list(n = 57)\nmy.data <- list(y = 19)\nmy.constants <- list(n = 57)\nmy.data <- list(y = 19)"},{"path":"intronimble.html","id":"specify-initial-values","chapter":"2 Introduction to Nimble","heading":"2.7 Specify initial values","text":"","code":"\ninitial.values <- function() list(phi = runif(1,0,1))\ninitial.values()\n## $phi\n## [1] 0.5332"},{"path":"intronimble.html","id":"which-parameters-to-save","chapter":"2 Introduction to Nimble","heading":"2.8 Which parameters to save?","text":"","code":"\nparameters.to.save <- c(\"phi\")"},{"path":"intronimble.html","id":"mcmc-details","chapter":"2 Introduction to Nimble","heading":"2.9 MCMC details","text":"Number posterior samples per chain:\\[n.posterior = \\frac{n.iter - n.burnin}{n.thin}\\]","code":"\nn.iter <- 5000\nn.burnin <- 1000\nn.chains <- 2\nn.thin <- 1"},{"path":"intronimble.html","id":"run-model-tadaa","chapter":"2 Introduction to Nimble","heading":"2.10 Run model, tadaa!","text":"","code":"\nmcmc.output <- nimbleMCMC(code = naive.survival.model,     \n                          data = my.data,  \n                          constants = my.constants,\n                          inits = initial.values,\n                          monitors = parameters.to.save,\n                          thin = n.thin,\n                          niter = n.iter, \n                          nburnin = n.burnin, \n                          nchains = n.chains)"},{"path":"intronimble.html","id":"explore-mcmc-outputs","chapter":"2 Introduction to Nimble","heading":"2.11 Explore MCMC outputs","text":"","code":"\nstr(mcmc.output)\n## List of 2\n##  $ chain1: num [1:4000, 1] 0.305 0.305 0.392 0.422 0.422 ...\n##   ..- attr(*, \"dimnames\")=List of 2\n##   .. ..$ : NULL\n##   .. ..$ : chr \"phi\"\n##  $ chain2: num [1:4000, 1] 0.352 0.416 0.416 0.416 0.416 ...\n##   ..- attr(*, \"dimnames\")=List of 2\n##   .. ..$ : NULL\n##   .. ..$ : chr \"phi\"\nhead(mcmc.output$chain1)\n##         phi\n## [1,] 0.3046\n## [2,] 0.3046\n## [3,] 0.3916\n## [4,] 0.4221\n## [5,] 0.4221\n## [6,] 0.4449"},{"path":"intronimble.html","id":"numerical-summaries","chapter":"2 Introduction to Nimble","heading":"2.12 Numerical summaries","text":"","code":"\nlibrary(MCMCvis)\nMCMCsummary(mcmc.output, round = 2)\n##     mean   sd 2.5%  50% 97.5% Rhat n.eff\n## phi 0.34 0.06 0.23 0.34  0.46    1  1931"},{"path":"intronimble.html","id":"trace-and-posterior-density","chapter":"2 Introduction to Nimble","heading":"2.13 Trace and posterior density","text":"","code":"\nMCMCtrace(mcmc.output,\n          pdf = FALSE) \nMCMCtrace(mcmc.output,\n          pdf = FALSE,\n          ind = TRUE,\n          Rhat = TRUE,\n          n.eff = TRUE) "},{"path":"intronimble.html","id":"our-nimble-workflow-so-far","chapter":"2 Introduction to Nimble","heading":"2.14 Our nimble workflow so far","text":"","code":"\nknitr::include_graphics(\"images/nimble_workflow_sofar.png\")"},{"path":"intronimble.html","id":"but-nimble-gives-full-access-to-the-mcmc-engine","chapter":"2 Introduction to Nimble","heading":"2.15 But nimble gives full access to the MCMC engine","text":"","code":"\nknitr::include_graphics(\"images/nimble_workflow.png\")\nknitr::include_graphics(\"images/I1bIY06.gif\")"},{"path":"intronimble.html","id":"useful-resources","chapter":"2 Introduction to Nimble","heading":"2.16 Useful resources","text":"Official website https://r-nimble.orgOfficial website https://r-nimble.orgUser Manual https://r-nimble.org/html_manual/cha-welcome-nimble.html cheatsheet.User Manual https://r-nimble.org/html_manual/cha-welcome-nimble.html cheatsheet.Users mailing list https://groups.google.com/forum/#!forum/nimble-usersUsers mailing list https://groups.google.com/forum/#!forum/nimble-usersTraining material https://github.com/nimble-trainingTraining material https://github.com/nimble-trainingReference cite using nimble publication:Reference cite using nimble publication:de Valpine, P., D. Turek, C. J. Paciorek, C. Anderson-Bergman, D. Temple Lang, R. Bodik (2017). Programming Models: Writing Statistical Algorithms General Model Structures NIMBLE. Journal Computational Graphical Statistics 26 (2): 403–13.","code":""},{"path":"hmmcapturerecapture.html","id":"hmmcapturerecapture","chapter":"3 Hidden Markov models","heading":"3 Hidden Markov models","text":"","code":""},{"path":"hmmcapturerecapture.html","id":"back-to-our-survival-example","chapter":"3 Hidden Markov models","heading":"3.1 Back to our survival example","text":"\\(z\\) survivors \\(n\\) released animals winter survival probability \\(\\phi\\)\\(z\\) survivors \\(n\\) released animals winter survival probability \\(\\phi\\)Let’s get back survival example.Let’s get back survival example.model far:model far:\\[\\begin{align*}\n   z &\\sim \\text{Binomial}(n, \\phi) &\\text{[likelihood]}\n   \\\\\n  \\phi &\\sim \\text{Beta}(1, 1) &\\text{[prior }\\phi \\text{]} \\\\ \n\\end{align*}\\]model far combinationOur model far combinationOf binomial likelihoodOf binomial likelihoodAnd Beta prior param 1 1, uniform 0 1.Beta prior param 1 1, uniform 0 1.also:also:\\[\\begin{align*}\n   z_i &\\sim \\text{Bernoulli}(\\phi), \\; = 1, \\ldots, N &\\text{[likelihood]}\n   \\\\\n  \\phi &\\sim \\text{Beta}(1, 1) &\\text{[prior }\\phi \\text{]} \\\\ \n\\end{align*}\\]binomial just sum Bernoulli outcomesThe binomial just sum Bernoulli outcomesLike flipping coin individual get survivor prob phi.Like flipping coin individual get survivor prob phi.several winters? Say \\(T = 5\\) winters.several winters? Say \\(T = 5\\) winters.design, single winter.\nmany species, ’ll need collect data long term get representative estimate survival.design, single winter.\nmany species, ’ll need collect data long term get representative estimate survival.Therefore say big T five winters?Therefore say big T five winters?","code":""},{"path":"hmmcapturerecapture.html","id":"longitudinal-data","chapter":"3 Hidden Markov models","heading":"3.2 Longitudinal data","text":"\\(z_{,t} = 1\\) individual \\(\\) alive winter \\(t\\), \\(z_{,t} = 2\\) dead.call longitudinal data.row individual , columns winters t, sampling occasions.z indexed t, takes value 1 ind alive winter t, 2 otherwise.","code":""},{"path":"hmmcapturerecapture.html","id":"a-model-for-longitudinal-survival-data","chapter":"3 Hidden Markov models","heading":"3.3 A model for longitudinal survival data","text":"model relies assumptions.model relies assumptions.Let’s think model data.Let’s think model data.objective remains , estimating survival.objective remains , estimating survival.build model, ’ll make assumptions.build model, ’ll make assumptions.state animal given winter, alive dead, dependent state winter .state animal given winter, alive dead, dependent state winter .First, assume state animal given winter, alive dead, dependent state winter .First, assume state animal given winter, alive dead, dependent state winter .future depends present, past: Markov process.future depends present, past: Markov process.others words, future depends present, pastIn others words, future depends present, pastThis Markov process.Markov process.animal alive given winter, probability survives next winter \\(\\phi\\).animal alive given winter, probability survives next winter \\(\\phi\\).animal alive given winter, probability survives next winter \\(\\phi\\).animal alive given winter, probability survives next winter \\(\\phi\\).probability dies \\(1 - \\phi\\).probability dies \\(1 - \\phi\\).probability dies \\(1 - \\phi\\).probability dies \\(1 - \\phi\\).animal dead winter, remains dead, unless believe zombies.animal dead winter, remains dead, unless believe zombies.animal dead winter, remains dead, unless believe zombies.animal dead winter, remains dead, unless believe zombies.","code":""},{"path":"hmmcapturerecapture.html","id":"markov-process","chapter":"3 Hidden Markov models","heading":"3.4 Markov process","text":"markov process can represented way.state t+1 depends state t.model, going winter next driven survival mortality processes.probability going alive 1 alive 1 phi.alive 1 dead 2 1 - phi.probability remain dead 1, go state 2 dead state 2 dead.","code":""},{"path":"hmmcapturerecapture.html","id":"transition-matrix","chapter":"3 Hidden Markov models","heading":"3.5 Transition matrix","text":"core Markov process made transition probabilities.core Markov process made transition probabilities.engine Markov model transition matrix.engine Markov model transition matrix.matrix table gathers probabilities transition states one occasion next.matrix table gathers probabilities transition states one occasion next.example, probability transitioning state alive \\(t-1\\) state alive \\(t\\) \\(\\Pr(z_t = 1 | z_{t-1} = 1) = \\gamma_{1,1}\\). survival probability \\(\\phi\\).example, probability transitioning state alive \\(t-1\\) state alive \\(t\\) \\(\\Pr(z_t = 1 | z_{t-1} = 1) = \\gamma_{1,1}\\). survival probability \\(\\phi\\).example, probability transitioning state alive \\(t-1\\) state alive \\(t\\) \\(\\Pr(z_t = 1 | z_{t-1} = 1) = \\gamma_{1,1}\\). survival probability \\(\\phi\\).example, probability transitioning state alive \\(t-1\\) state alive \\(t\\) \\(\\Pr(z_t = 1 | z_{t-1} = 1) = \\gamma_{1,1}\\). survival probability \\(\\phi\\).probability dying interval \\((t-1, t)\\) \\(\\Pr(z_t = 2 | z_{t-1} = 1) = \\gamma_{1,2} = 1 - \\phi\\).probability dying interval \\((t-1, t)\\) \\(\\Pr(z_t = 2 | z_{t-1} = 1) = \\gamma_{1,2} = 1 - \\phi\\).probability dying interval \\((t-1, t)\\) \\(\\Pr(z_t = 2 | z_{t-1} = 1) = \\gamma_{1,2} = 1 - \\phi\\).probability dying interval \\((t-1, t)\\) \\(\\Pr(z_t = 2 | z_{t-1} = 1) = \\gamma_{1,2} = 1 - \\phi\\).Now animal dead \\(t-1\\), \\(\\Pr(z_t = 1 | z_{t-1} = 2) = 0\\) \\(\\Pr(z_t = 2 | z_{t-1} = 2) = 1\\).Now animal dead \\(t-1\\), \\(\\Pr(z_t = 1 | z_{t-1} = 2) = 0\\) \\(\\Pr(z_t = 2 | z_{t-1} = 2) = 1\\).Now animal dead \\(t-1\\), \\(\\Pr(z_t = 1 | z_{t-1} = 2) = 0\\) \\(\\Pr(z_t = 2 | z_{t-1} = 2) = 1\\).Now animal dead \\(t-1\\), \\(\\Pr(z_t = 1 | z_{t-1} = 2) = 0\\) \\(\\Pr(z_t = 2 | z_{t-1} = 2) = 1\\).probabilities can packed transition matrix \\(\\mathbf{\\Gamma}\\):probabilities can packed transition matrix \\(\\mathbf{\\Gamma}\\):\\[\\begin{align*}\n\\mathbf{\\Gamma} = \n\\left(\\begin{array}{cc} \n\\gamma_{1,1} & \\gamma_{1,2}\\\\ \n\\gamma_{2,1} & \\gamma_{2,2}\n\\end{array}\\right) =\n\\left(\\begin{array}{cc} \n\\phi & 1 - \\phi\\\\ \n0 & 1\n\\end{array}\\right)\n\\end{align*}\\]probabilities can packed transition matrix \\(\\mathbf{\\Gamma}\\):Transition matrix:\\[\n\\begin{matrix}\n& \\\\\n\\mathbf{\\Gamma} = \n    \\left ( \\vphantom{ \\begin{matrix} 12 \\\\ 12 \\end{matrix} } \\right .\n\\end{matrix}\n\\hspace{-1.2em}\n\\begin{matrix}\n    z_t=& z_t=D \\\\ \\hdashline\n\\phi & 1-\\phi \\\\\n0 & 1\n\\end{matrix}\n\\hspace{-0.2em}\n\\begin{matrix}\n& \\\\\n\\left . \\vphantom{ \\begin{matrix} 12 \\\\ 12 \\end{matrix} } \\right )\n    \\begin{matrix}\n    z_{t-1}=\\\\ z_{t-1}=D\n    \\end{matrix}\n\\end{matrix}\n\\]\n+ Take time navigate matrix.\n+ rows, origin, columns, destination.\n+ example…","code":""},{"path":"hmmcapturerecapture.html","id":"initial-states","chapter":"3 Hidden Markov models","heading":"3.6 Initial states","text":"Markov process start somewhere.Markov process start somewhere.need probabilities initial states, .e. states \\(t = 1\\).need probabilities initial states, .e. states \\(t = 1\\).words, need probabilities initial statesIn words, need probabilities initial statesi.e. states \\(t = 1\\)..e. states \\(t = 1\\).use \\(\\mathbf{\\delta} = \\left(\\Pr(z_1 = 1), \\Pr(z_1 = 2)\\right)\\).use \\(\\mathbf{\\delta} = \\left(\\Pr(z_1 = 1), \\Pr(z_1 = 2)\\right)\\).denote delta vector.denote delta vector.gathers probability initial states.gathers probability initial states.alive 1 dead 2.alive 1 dead 2.assume animals alive first winter, .e. \\(\\Pr(z_1 = 1) = 1\\) \\(\\Pr(z_1 = 2) = 0\\).assume animals alive first winter, .e. \\(\\Pr(z_1 = 1) = 1\\) \\(\\Pr(z_1 = 2) = 0\\).individuals marked release first winter.individuals marked release first winter.Therefore alive first captured.Therefore alive first captured.means state 1 alive sure.means state 1 alive sure.","code":""},{"path":"hmmcapturerecapture.html","id":"likelihood","chapter":"3 Hidden Markov models","heading":"3.7 Likelihood","text":"\\[\\begin{align*}\n\\Pr(\\mathbf{z}) &= \\Pr(z_T, z_{T-1}, z_{T-2}, \\ldots, z_1) \\color{white}{\\Pr(z_{T-1}, z_{T-2},\\ldots, z_1) \\Pr(z_{T-2}, \\ldots, z_1)} \\\\\n\\end{align*}\\]OK now ’ve defined Markov model, need likelihood apply Bayes theorem.likelihood probability data, given model. data z.\\[\\begin{align*}\n\\Pr(\\mathbf{z}) &= \\Pr(z_T, z_{T-1}, z_{T-2}, \\ldots, z_1) \\color{white}{\\Pr(z_{T-1}, z_{T-2},\\ldots, z_1) \\Pr(z_{T-2}, \\ldots, z_1)}\\\\\n                &= \\Pr(z_T | z_{T-1}, z_{T-2},\\ldots, z_1) \\Pr(z_{T-1}, z_{T-2},\\ldots, z_1) \\\\\n\\end{align*}\\]’re gonna work backward, starting last sampling occasion.Now likelihood can written product probability zT ie ’re alive last occasion given past history, states previous occasions, times prob past history, y definition cond prob.\\[\\begin{align*}\n\\Pr(\\mathbf{z}) &= \\Pr(z_T, z_{T-1}, z_{T-2}, \\ldots, z_1) \\color{white}{\\Pr(z_{T-1}, z_{T-2},\\ldots, z_1) \\Pr(z_{T-2}, \\ldots, z_1)}\\\\\n                &= \\Pr(z_T | z_{T-1}, z_{T-2},\\ldots, z_1) \\Pr(z_{T-1}, z_{T-2},\\ldots, z_1) \\\\\n                &= \\Pr(z_T | z_{T-1}) \\Pr(z_{T-1}, z_{T-2},\\ldots, z_1) \\\\\n\\end{align*}\\]Markov model, ’re memory less, prob next state, zT, depends current state, zT-1, previous states.\\[\\begin{align*}\n\\Pr(\\mathbf{z}) &= \\Pr(z_T, z_{T-1}, z_{T-2}, \\ldots, z_1) \\color{white}{\\Pr(z_{T-1}, z_{T-2},\\ldots, z_1) \\Pr(z_{T-2}, \\ldots, z_1)}\\\\\n                &= \\Pr(z_T | z_{T-1}, z_{T-2},\\ldots, z_1) \\Pr(z_{T-1}, z_{T-2},\\ldots, z_1) \\\\\n                &= \\Pr(z_T | z_{T-1}) \\Pr(z_{T-1}, z_{T-2},\\ldots, z_1) \\\\\n                &= \\Pr(z_T | z_{T-1}) \\Pr(z_{T-1} | z_{T-2}, \\ldots, z_1) \\Pr(z_{T-2}, \\ldots, z_1)\\\\\n\\end{align*}\\]can apply reasoning T-1.First conditional prob.\\[\\begin{align*}\n\\Pr(\\mathbf{z}) &= \\Pr(z_T, z_{T-1}, z_{T-2}, \\ldots, z_1) \\color{white}{\\Pr(z_{T-1}, z_{T-2},\\ldots, z_1) \\Pr(z_{T-2}, \\ldots, z_1)}\\\\\n                &= \\Pr(z_T | z_{T-1}, z_{T-2},\\ldots, z_1) \\Pr(z_{T-1}, z_{T-2},\\ldots, z_1) \\\\\n                &= \\Pr(z_T | z_{T-1}) \\Pr(z_{T-1}, z_{T-2},\\ldots, z_1) \\\\\n                &= \\Pr(z_T | z_{T-1}) \\Pr(z_{T-1} | z_{T-2}, \\ldots, z_1) \\Pr(z_{T-2}, \\ldots, z_1)\\\\\n                &= \\Pr(z_T | z_{T-1}) \\Pr(z_{T-1} | z_{T-2}) \\Pr(z_{T-2}, \\ldots, z_1)\\\\\n\\end{align*}\\]markovian property.\\[\\begin{align*}\n\\Pr(\\mathbf{z}) &= \\Pr(z_T, z_{T-1}, z_{T-2}, \\ldots, z_1) \\color{white}{\\Pr(z_{T-1}, z_{T-2},\\ldots, z_1) \\Pr(z_{T-2}, \\ldots, z_1)}\\\\\n                &= \\Pr(z_T | z_{T-1}, z_{T-2},\\ldots, z_1) \\Pr(z_{T-1}, z_{T-2},\\ldots, z_1) \\\\\n                &= \\Pr(z_T | z_{T-1}) \\Pr(z_{T-1}, z_{T-2},\\ldots, z_1) \\\\\n                &= \\Pr(z_T | z_{T-1}) \\Pr(z_{T-1} | z_{T-2}, \\ldots, z_1) \\Pr(z_{T-2}, \\ldots, z_1)\\\\\n                &= \\Pr(z_T | z_{T-1}) \\Pr(z_{T-1} | z_{T-2}) \\Pr(z_{T-2}, \\ldots, z_1)\\\\\n                &= \\ldots \\\\\n\\end{align*}\\].\\[\\begin{align*}\n\\Pr(\\mathbf{z}) &= \\Pr(z_T, z_{T-1}, z_{T-2}, \\ldots, z_1) \\color{white}{\\Pr(z_{T-1}, z_{T-2},\\ldots, z_1) \\Pr(z_{T-2}, \\ldots, z_1)}\\\\\n                &= \\Pr(z_T | z_{T-1}, z_{T-2},\\ldots, z_1) \\Pr(z_{T-1}, z_{T-2},\\ldots, z_1) \\\\\n                &= \\Pr(z_T | z_{T-1}) \\Pr(z_{T-1}, z_{T-2},\\ldots, z_1) \\\\\n                &= \\Pr(z_T | z_{T-1}) \\Pr(z_{T-1} | z_{T-2}, \\ldots, z_1) \\Pr(z_{T-2}, \\ldots, z_1)\\\\\n                &= \\Pr(z_T | z_{T-1}) \\Pr(z_{T-1} | z_{T-2}) \\Pr(z_{T-2}, \\ldots, z_1)\\\\\n                &= \\ldots \\\\\n                &= \\Pr(z_T | z_{T-1}) \\Pr(z_{T-1} | z_{T-2}) \\ldots \\Pr(z_{2} | z_{1}) \\Pr(z_{1})\\\\\n\\end{align*}\\]end expression likelihood.\\[\\begin{align*}\n\\Pr(\\mathbf{z}) &= \\Pr(z_T, z_{T-1}, z_{T-2}, \\ldots, z_1) \\color{white}{\\Pr(z_{T-1}, z_{T-2},\\ldots, z_1) \\Pr(z_{T-2}, \\ldots, z_1)}\\\\\n                &= \\Pr(z_T | z_{T-1}, z_{T-2},\\ldots, z_1) \\Pr(z_{T-1}, z_{T-2},\\ldots, z_1) \\\\\n                &= \\Pr(z_T | z_{T-1}) \\Pr(z_{T-1}, z_{T-2},\\ldots, z_1) \\\\\n                &= \\Pr(z_T | z_{T-1}) \\Pr(z_{T-1} | z_{T-2}, \\ldots, z_1) \\Pr(z_{T-2}, \\ldots, z_1)\\\\\n                &= \\Pr(z_T | z_{T-1}) \\Pr(z_{T-1} | z_{T-2}) \\Pr(z_{T-2}, \\ldots, z_1)\\\\\n                &= \\ldots \\\\\n                &= \\Pr(z_T | z_{T-1}) \\Pr(z_{T-1} | z_{T-2}) \\ldots \\Pr(z_{2} | z_{1}) \\Pr(z_{1})\\\\\n                &= \\Pr(z_{1}) \\prod_{t=2}^T{\\Pr(z_{t} | z_{t-1})}\\\\\n\\end{align*}\\]product cond probabilities. prob initial states Pr(z1).\\[\\begin{align*}\n\\Pr(\\mathbf{z}) &= \\Pr(z_T, z_{T-1}, z_{T-2}, \\ldots, z_1) \\color{white}{\\Pr(z_{T-1}, z_{T-2},\\ldots, z_1) \\Pr(z_{T-2}, \\ldots, z_1)}\\\\\n                &= \\Pr(z_T | z_{T-1}, z_{T-2},\\ldots, z_1) \\Pr(z_{T-1}, z_{T-2},\\ldots, z_1) \\\\\n                &= \\Pr(z_T | z_{T-1}) \\Pr(z_{T-1}, z_{T-2},\\ldots, z_1) \\\\\n                &= \\Pr(z_T | z_{T-1}) \\Pr(z_{T-1} | z_{T-2}, \\ldots, z_1) \\Pr(z_{T-2}, \\ldots, z_1)\\\\\n                &= \\Pr(z_T | z_{T-1}) \\Pr(z_{T-1} | z_{T-2}) \\Pr(z_{T-2}, \\ldots, z_1)\\\\\n                &= \\ldots \\\\\n                &= \\Pr(z_T | z_{T-1}) \\Pr(z_{T-1} | z_{T-2}) \\ldots \\Pr(z_{2} | z_{1}) \\Pr(z_{1})\\\\\n                &= \\Pr(z_{1}) \\prod_{t=2}^T{\\Pr(z_{t} | z_{t-1})}\\\\\n                &= \\Pr(z_{1}) \\prod_{t=2}^T{\\gamma_{z_{t-1},z_{t}}}\\\\\n\\end{align*}\\]recognize gammas defined earlier.transition probabilities.","code":""},{"path":"hmmcapturerecapture.html","id":"example","chapter":"3 Hidden Markov models","heading":"3.8 Example","text":"Let’s assume animal alive, alive dies.Let’s assume animal alive, alive dies.realise calculations bit difficult follow.realise calculations bit difficult follow.Let’s take example.Let’s take example.\\(\\mathbf{z} = (1, 1, 2)\\). contribution animal likelihood?\\(\\mathbf{z} = (1, 1, 2)\\). contribution animal likelihood?\\(\\mathbf{z} = (1, 1, 2)\\). contribution animal likelihood?\\(\\mathbf{z} = (1, 1, 2)\\). contribution animal likelihood?Let’s apply formula just derived.Let’s apply formula just derived.\\[\\begin{align*}\n\\Pr(\\mathbf{z} = (1, 1, 2)) &= \\Pr(z_1 = 1) \\; \\gamma_{z_{1} = 1, z_{2} = 1} \\; \\gamma_{z_{2} = 1, z_{3} = 2}\\\\\n                            &= 1 \\; \\phi \\; (1 - \\phi).\n\\end{align*}\\]prob sequence alive, alive dead isThe prob sequence alive, alive dead isThe prob alive first, stay alive, die.prob alive first, stay alive, die.prob alive first occasion 1, contribution individual likelihood phi times 1 - phi.prob alive first occasion 1, contribution individual likelihood phi times 1 - phi.Remember:Remember:\\[\\begin{align*}\n\\mathbf{\\Gamma} = \n\\left(\\begin{array}{cc} \n\\gamma_{1,1} & \\gamma_{1,2}\\\\ \n\\gamma_{2,1} & \\gamma_{2,2}\n\\end{array}\\right) =\n\\left(\\begin{array}{cc} \n\\phi & 1 - \\phi\\\\ \n0 & 1\n\\end{array}\\right)\n\\end{align*}\\]","code":""},{"path":"hmmcapturerecapture.html","id":"our-model","chapter":"3 Hidden Markov models","heading":"3.9 Our model","text":"\\[\\begin{align*}\n   z_1 &\\sim \\text{Multinomial}(1, \\delta) &\\text{[likelihood, }t = 1 \\text{]}\\\\\n   \\color{white}{z_t | z_{t-1}} & \\color{white}{\\sim} \\color{white}{\\text{Multinomial}(1, \\gamma_{z_{t-1},z_{t}})} & \\color{white}{\\text{[likelihood, }t > 1 \\text{]}}\\\\\n  \\color{white}{\\phi} & \\color{white}{\\sim} \\color{white}{\\text{Beta}(1, 1)} & \\color{white}{\\text{[prior }\\phi \\text{]}} \\\\ \n\\end{align*}\\]OK let’s wrap .OK let’s wrap .model far one.model far one.Initial state multinomial one trial, probability delta.Initial state multinomial one trial, probability delta.dice two faces, coin, prob alive, 1 - prob dead. + course, want Markov chain start, ’d better say ’s alive delta just (1,0).dice two faces, coin, prob alive, 1 - prob dead. + course, want Markov chain start, ’d better say ’s alive delta just (1,0).","code":""},{"path":"hmmcapturerecapture.html","id":"our-model-1","chapter":"3 Hidden Markov models","heading":"3.10 Our model","text":"\\[\\begin{align*}\n   z_1 &\\sim \\text{Multinomial}(1, \\delta) &\\text{[likelihood, }t = 1 \\text{]}\\\\\n   \\color{white}{z_t | z_{t-1}} & \\color{white}{\\sim} \\color{white}{\\text{Multinomial}(1, \\gamma_{z_{t-1},z_{t}})} & \\color{white}{\\text{[likelihood, }t > 1 \\text{]}}\\\\\n  \\phi &\\sim \\text{Beta}(1, 1) &\\text{[prior }\\phi \\text{]} \\\\ \n\\end{align*}\\]also need prior survival.usual take uniform distribution 0 1, beta parameters 1 1.\\[\\begin{align*}\n   z_1 &\\sim \\text{Multinomial}(1, \\delta) &\\text{[likelihood, }t = 1 \\text{]}\\\\\n   z_t | z_{t-1} &\\sim \\text{Multinomial}(1, \\gamma_{z_{t-1},z_{t}}) &\\text{[likelihood, }t > 1 \\text{]}\\\\\n  \\phi &\\sim \\text{Beta}(1, 1) &\\text{[prior }\\phi \\text{]} \\\\ \n\\end{align*}\\]Now main part dynamic states.state t depends state t-1, multinomial random variable, one trial.probabilities given rows transition matrix.\\[\\begin{align*}\n   z_1 &\\sim \\text{Multinomial}(1, \\delta) &\\text{[likelihood, }t = 1 \\text{]}\\\\\n   z_t | z_{t-1} &\\sim \\text{Multinomial}(1, \\gamma_{z_{t-1},z_{t}}) &\\text{[likelihood, }t > 1 \\text{]}\\\\\n  \\phi &\\sim \\text{Beta}(1, 1) &\\text{[prior }\\phi \\text{]} \\\\ \n\\end{align*}\\]\\[\\begin{align*}\n\\mathbf{\\Gamma} = \n\\left(\\begin{array}{cc} \n\\color{blue}{\\phi} & \\color{blue}{1 - \\phi}\\\\ \n0 & 1\n\\end{array}\\right)\n\\end{align*}\\]\\[\\color{blue}{\\gamma_{z_{t-1} = 1,z_{t}} = (\\phi, 1-\\phi)}\\]z t-1 alive, first row, phi 1-phi.\\[\\begin{align*}\n   z_1 &\\sim \\text{Multinomial}(1, \\delta) &\\text{[likelihood, }t = 1 \\text{]}\\\\\n   z_t | z_{t-1} &\\sim \\text{Multinomial}(1, \\gamma_{z_{t-1},z_{t}}) &\\text{[likelihood, }t > 1 \\text{]}\\\\\n  \\phi &\\sim \\text{Beta}(1, 1) &\\text{[prior }\\phi \\text{]} \\\\ \n\\end{align*}\\]\\[\\begin{align*}\n\\mathbf{\\Gamma} = \n\\left(\\begin{array}{cc} \n\\phi & 1 - \\phi\\\\ \n\\color{blue}{0} & \\color{blue}{1}\n\\end{array}\\right)\n\\end{align*}\\]\\[\\color{blue}{\\gamma_{z_{t-1} = 2,z_{t}} = (0, 1)}\\]Otherwise, z t-1 dead 2, second row gamma, 0 1.dead remain dead.","code":""},{"path":"hmmcapturerecapture.html","id":"nimble-implementation","chapter":"3 Hidden Markov models","heading":"3.11 Nimble implementation","text":"Nimble, use categorical distribution dcat().Nimble, use categorical distribution dcat().categorical distribution multinomial distribution single draw.categorical distribution multinomial distribution single draw.https://en.wikipedia.org/wiki/Categorical_distributionThe categorical distribution generalization Bernoulli distribution categorical random variable, .e. discrete variable two possible outcomes, roll dice. hand, categorical distribution special case multinomial distribution, gives probabilities potential outcomes single drawing rather multiple drawings.","code":"\nnimble::rcat(n = 20, prob = c(0.1, 0.3, 0.6))\n##  [1] 2 3 2 1 2 2 1 3 3 3 3 2 2 2 1 3 2 1 1 3\nnimble::rcat(n = 20, prob = c(0.1, 0.1, 0.4, 0.2, 0.2))\n##  [1] 3 1 3 2 5 2 3 4 1 3 2 4 1 3 5 1 3 3 3 3"},{"path":"hmmcapturerecapture.html","id":"nimble-code","chapter":"3 Hidden Markov models","heading":"3.12 Nimble code","text":"]","code":"\nmarkov.survival <- nimbleCode({\n  phi ~ dunif(0, 1) # prior\n  gamma[1,1] <- phi      # Pr(alive t -> alive t+1)\n  gamma[1,2] <- 1 - phi  # Pr(alive t -> dead t+1)\n  gamma[2,1] <- 0        # Pr(dead t -> alive t+1)\n  gamma[2,2] <- 1        # Pr(dead t -> dead t+1)\n  delta[1] <- 1          # Pr(alive t = 1) = 1\n  delta[2] <- 0          # Pr(dead t = 1) = 0\n  # likelihood\n  for (i in 1:N){\n    z[i,1] ~ dcat(delta[1:2])\n    for (j in 2:T){\n      z[i,j] ~ dcat(gamma[z[i,j-1], 1:2])\n    }\n  }})\nmarkov.survival <- nimbleCode({\n  phi ~ dunif(0, 1) # prior #<<\n  gamma[1,1] <- phi      # Pr(alive t -> alive t+1)\n  gamma[1,2] <- 1 - phi  # Pr(alive t -> dead t+1)\n  gamma[2,1] <- 0        # Pr(dead t -> alive t+1)\n  gamma[2,2] <- 1        # Pr(dead t -> dead t+1)\n  delta[1] <- 1          # Pr(alive t = 1) = 1\n  delta[2] <- 0          # Pr(dead t = 1) = 0\n  # likelihood\n  for (i in 1:N){\n    z[i,1] ~ dcat(delta[1:2])\n    for (j in 2:T){\n      z[i,j] ~ dcat(gamma[z[i,j-1], 1:2])\n    }\n  }})\nmarkov.survival <- nimbleCode({\n  phi ~ dunif(0, 1) # prior\n  gamma[1,1] <- phi      # Pr(alive t -> alive t+1) #<<\n  gamma[1,2] <- 1 - phi  # Pr(alive t -> dead t+1) #<<\n  gamma[2,1] <- 0        # Pr(dead t -> alive t+1) #<<\n  gamma[2,2] <- 1        # Pr(dead t -> dead t+1) #<<\n  delta[1] <- 1          # Pr(alive t = 1) = 1\n  delta[2] <- 0          # Pr(dead t = 1) = 0\n  # likelihood\n  for (i in 1:N){\n    z[i,1] ~ dcat(delta[1:2])\n    for (j in 2:T){\n      z[i,j] ~ dcat(gamma[z[i,j-1], 1:2])\n    }\n  }})\nmarkov.survival <- nimbleCode({\n  phi ~ dunif(0, 1) # prior\n  gamma[1,1] <- phi      # Pr(alive t -> alive t+1)\n  gamma[1,2] <- 1 - phi  # Pr(alive t -> dead t+1)\n  gamma[2,1] <- 0        # Pr(dead t -> alive t+1)\n  gamma[2,2] <- 1        # Pr(dead t -> dead t+1)\n  delta[1] <- 1          # Pr(alive t = 1) = 1 #<<\n  delta[2] <- 0          # Pr(dead t = 1) = 0 #<<\n  # likelihood\n  for (i in 1:N){\n    z[i,1] ~ dcat(delta[1:2])\n    for (j in 2:T){\n      z[i,j] ~ dcat(gamma[z[i,j-1], 1:2])\n    }\n  }})\nmarkov.survival <- nimbleCode({\n  phi ~ dunif(0, 1) # prior\n  gamma[1,1] <- phi      # Pr(alive t -> alive t+1)\n  gamma[1,2] <- 1 - phi  # Pr(alive t -> dead t+1)\n  gamma[2,1] <- 0        # Pr(dead t -> alive t+1)\n  gamma[2,2] <- 1        # Pr(dead t -> dead t+1)\n  delta[1] <- 1          # Pr(alive t = 1) = 1\n  delta[2] <- 0          # Pr(dead t = 1) = 0\n  # likelihood\n  for (i in 1:N){ #<<\n    z[i,1] ~ dcat(delta[1:2]) \n    for (j in 2:T){\n      z[i,j] ~ dcat(gamma[z[i,j-1], 1:2])\n    } \n  } #<<\n  })\nmarkov.survival <- nimbleCode({\n  phi ~ dunif(0, 1) # prior\n  gamma[1,1] <- phi      # Pr(alive t -> alive t+1)\n  gamma[1,2] <- 1 - phi  # Pr(alive t -> dead t+1)\n  gamma[2,1] <- 0        # Pr(dead t -> alive t+1)\n  gamma[2,2] <- 1        # Pr(dead t -> dead t+1)\n  delta[1] <- 1          # Pr(alive t = 1) = 1\n  delta[2] <- 0          # Pr(dead t = 1) = 0\n  # likelihood\n  for (i in 1:N){\n    z[i,1] ~ dcat(delta[1:2]) #<<\n    for (j in 2:T){\n      z[i,j] ~ dcat(gamma[z[i,j-1], 1:2])\n    }\n  }})\nmarkov.survival <- nimbleCode({\n  phi ~ dunif(0, 1) # prior\n  gamma[1,1] <- phi      # Pr(alive t -> alive t+1)\n  gamma[1,2] <- 1 - phi  # Pr(alive t -> dead t+1)\n  gamma[2,1] <- 0        # Pr(dead t -> alive t+1)\n  gamma[2,2] <- 1        # Pr(dead t -> dead t+1)\n  delta[1] <- 1          # Pr(alive t = 1) = 1\n  delta[2] <- 0          # Pr(dead t = 1) = 0\n  # likelihood\n  for (i in 1:N){\n    z[i,1] ~ dcat(delta[1:2])\n    for (j in 2:T){ #<<\n      z[i,j] ~ dcat(gamma[z[i,j-1], 1:2])\n    } #<<\n  }})\nmarkov.survival <- nimbleCode({\n  phi ~ dunif(0, 1) # prior\n  gamma[1,1] <- phi      # Pr(alive t -> alive t+1)\n  gamma[1,2] <- 1 - phi  # Pr(alive t -> dead t+1)\n  gamma[2,1] <- 0        # Pr(dead t -> alive t+1)\n  gamma[2,2] <- 1        # Pr(dead t -> dead t+1)\n  delta[1] <- 1          # Pr(alive t = 1) = 1\n  delta[2] <- 0          # Pr(dead t = 1) = 0\n  # likelihood\n  for (i in 1:N){\n    z[i,1] ~ dcat(delta[1:2])\n    for (j in 2:T){\n      z[i,j] ~ dcat(gamma[z[i,j-1], 1:2]) #<<\n    }\n  }})"},{"path":"hmmcapturerecapture.html","id":"note","chapter":"3 Hidden Markov models","heading":"3.13 Note","text":"Vector \\(\\delta\\) used placeholder complex models come Class 7.Vector \\(\\delta\\) used placeholder complex models come Class 7., write z[,1] <- 1., write z[,1] <- 1.","code":""},{"path":"hmmcapturerecapture.html","id":"nimble-awesomness","chapter":"3 Hidden Markov models","heading":"3.14 Nimble awesomness","text":"able define vectors matrices like R.","code":"\nmarkov.survival <- nimbleCode({\n  phi ~ dunif(0, 1) # prior\n  gamma[1:2,1:2] <- matrix( c(phi, 0, 1 - phi, 1), nrow = 2) #<<\n  delta[1:2] <- c(1, 0) #<<\n  # likelihood\n  for (i in 1:N){\n    z[i,1] ~ dcat(delta[1:2])\n    for (j in 2:T){\n      z[i,j] ~ dcat(gamma[z[i,j-1], 1:2])\n    }\n  }})"},{"path":"hmmcapturerecapture.html","id":"converting-to-nimble-from-jags-openbugs-or-winbugs","chapter":"3 Hidden Markov models","heading":"3.15 Converting to Nimble from Jags, OpenBUGS or WinBUGS","text":"Main difference Nimble guess.Main difference Nimble guess.need specify dimensions vectors matrices.need specify dimensions vectors matrices.write x[] x[,]. Just provide index ranges x[1:n] x[,1:m].write x[] x[,]. Just provide index ranges x[1:n] x[,1:m].tips .tips .","code":""},{"path":"hmmcapturerecapture.html","id":"constants-and-data","chapter":"3 Hidden Markov models","heading":"3.16 Constants and data","text":"","code":"\nmy.constants <- list(N = 57, T = 5)\nmy.constants\n## $N\n## [1] 57\n## \n## $T\n## [1] 5\n\nmy.data <- list(z = z)"},{"path":"hmmcapturerecapture.html","id":"initial-values","chapter":"3 Hidden Markov models","heading":"3.17 Initial values","text":"","code":"\ninitial.values <- function() list(phi = runif(1,0,1))\ninitial.values()\n## $phi\n## [1] 0.5749"},{"path":"hmmcapturerecapture.html","id":"parameters-to-monitor","chapter":"3 Hidden Markov models","heading":"3.18 Parameters to monitor","text":"","code":"\nparameters.to.save <- c(\"phi\")\nparameters.to.save\n## [1] \"phi\""},{"path":"hmmcapturerecapture.html","id":"mcmc-details-1","chapter":"3 Hidden Markov models","heading":"3.19 MCMC details","text":"","code":"\nn.iter <- 5000\nn.burnin <- 1000\nn.chains <- 2"},{"path":"hmmcapturerecapture.html","id":"run-nimble","chapter":"3 Hidden Markov models","heading":"3.20 Run Nimble","text":"","code":"\nmcmc.output <- nimbleMCMC(code = markov.survival, \n                          constants = my.constants,\n                          data = my.data,              \n                          inits = initial.values,\n                          monitors = parameters.to.save,\n                          niter = n.iter, \n                          nburnin = n.burnin, \n                          nchains = n.chains)\n## |-------------|-------------|-------------|-------------|\n## |-------------------------------------------------------|\n## |-------------|-------------|-------------|-------------|\n## |-------------------------------------------------------|"},{"path":"hmmcapturerecapture.html","id":"posterior-distribution-of-survival","chapter":"3 Hidden Markov models","heading":"3.21 Posterior distribution of survival","text":"Posterior mean median close \\(0.8\\).Posterior mean median close \\(0.8\\).Cool! data simulated, (true) survival \\(\\phi = 0.8\\).Cool! data simulated, (true) survival \\(\\phi = 0.8\\).","code":"\nlibrary(MCMCvis)\nMCMCsummary(mcmc.output, round = 2)\n##     mean   sd 2.5%  50% 97.5% Rhat n.eff\n## phi 0.81 0.03 0.74 0.81  0.86 1.01  1979"},{"path":"hmmcapturerecapture.html","id":"unfortunately-this-is-the-data-we-wish-we-had.","chapter":"3 Hidden Markov models","heading":"3.22 Unfortunately, this is the data we wish we had.","text":"","code":""},{"path":"hmmcapturerecapture.html","id":"in-real-life","chapter":"3 Hidden Markov models","heading":"3.23 In real life","text":"Animals monitored exhaustively, like humans medical trial.Animals monitored exhaustively, like humans medical trial.Animals captured, marked identified released alive.Animals captured, marked identified released alive., animals may detected , go undetected — capture-recaptureThen, animals may detected , go undetected — capture-recaptureWhenever animals go undetected, might alive missed, dead therefore detected — imperfect detection.Whenever animals go undetected, might alive missed, dead therefore detected — imperfect detection.https://www.youtube.com/embed/tyX79mPm2xYWhenever animals go undetected, might alive missed, dead therefore detected — imperfect detection.Whenever animals go undetected, might alive missed, dead therefore detected — imperfect detection.Markov process survival partially observed — hidden Markov models.Markov process survival partially observed — hidden Markov models.","code":""},{"path":"hmmcapturerecapture.html","id":"the-truth-is-in-z","chapter":"3 Hidden Markov models","heading":"3.24 The truth is in \\(z\\)","text":"Unfortunately, partial access \\(z\\).Unfortunately, partial access \\(z\\).observe \\(y\\) detections non-detections.observe \\(y\\) detections non-detections.\\(z\\) \\(y\\) connected?\\(z\\) \\(y\\) connected?","code":""},{"path":"hmmcapturerecapture.html","id":"dead-animals-go-undetected","chapter":"3 Hidden Markov models","heading":"3.25 Dead animals go undetected","text":"animal dead .e. \\(z = 2\\), detected, therefore \\(y = 0\\).","code":""},{"path":"hmmcapturerecapture.html","id":"alive-animals-may-be-detected-or-not","chapter":"3 Hidden Markov models","heading":"3.26 Alive animals may be detected or not","text":"animal alive \\(z = 1\\), detected \\(y = 1\\) w/ prob \\(p\\) \\(y = 0\\) w/ prob \\(1-p\\).animal alive \\(z = 1\\), detected \\(y = 1\\) w/ prob \\(p\\) \\(y = 0\\) w/ prob \\(1-p\\).first detection, know nothing, proceed conditional .first detection, know nothing, proceed conditional .Compare previous tableCompare previous tableSome 1s become 0s.1s become 0s.table \\(y\\) observe real life.table \\(y\\) observe real life.make connection observations, y, true states, zTo make connection observations, y, true states, zWe need describe observations made statesWe need describe observations made states","code":""},{"path":"hmmcapturerecapture.html","id":"observation-matrix","chapter":"3 Hidden Markov models","heading":"3.27 Observation matrix","text":"observation probabilities can packed observation matrix \\(\\mathbf{\\Omega}\\).observation probabilities can packed observation matrix \\(\\mathbf{\\Omega}\\).rows: states alive \\(z = 1\\) dead \\(z = 2\\).rows: states alive \\(z = 1\\) dead \\(z = 2\\).columns: observations non-detected \\(y = 1\\) detected \\(y = 2\\) (previously coded 0 1 respectively).columns: observations non-detected \\(y = 1\\) detected \\(y = 2\\) (previously coded 0 1 respectively).\\[\\begin{align*}\n\\mathbf{\\Omega} = \n\\left(\\begin{array}{cc} \n\\omega_{1,1} & \\omega_{1,2}\\\\ \n\\omega_{2,1} & \\omega_{2,2}\n\\end{array}\\right) =\n\\left(\\begin{array}{cc} \n1 - p & p\\\\ \n1 & 0\n\\end{array}\\right)\n\\end{align*}\\]Observation matrix:\\[\n\\begin{matrix}\n& \\\\\n\\mathbf{\\Omega} = \n    \\left ( \\vphantom{ \\begin{matrix} 12 \\\\ 12 \\end{matrix} } \\right .\n\\end{matrix}\n\\hspace{-1.2em}\n\\begin{matrix}\n    y_t=1 & y_t=2 \\\\ \\hdashline\n1 - p & p\\\\ \n1 & 0\\\\\n\\end{matrix}\n\\hspace{-0.2em}\n\\begin{matrix}\n& \\\\\n\\left . \\vphantom{ \\begin{matrix} 12 \\\\ 12 \\end{matrix} } \\right )\n    \\begin{matrix}\n    z_{t}=\\\\ z_{t}=D\n    \\end{matrix}\n\\end{matrix}\n\\]","code":""},{"path":"hmmcapturerecapture.html","id":"markov-model","chapter":"3 Hidden Markov models","heading":"3.28 Markov model","text":"States \\(z\\) gray.States \\(z\\) gray.Remember graphical repres Markov model.Remember graphical repres Markov model.","code":""},{"path":"hmmcapturerecapture.html","id":"hidden-markov-model","chapter":"3 Hidden Markov models","heading":"3.29 Hidden Markov model","text":"States \\(z\\) gray.States \\(z\\) gray.Observations \\(y\\) white.Observations \\(y\\) white.hidden Markov model just two time series.hidden Markov model just two time series.One states Markovian property.One states Markovian property.observations generated states.observations generated states.Run parallel.Run parallel.","code":""},{"path":"hmmcapturerecapture.html","id":"hidden-markov-model-for-survival","chapter":"3 Hidden Markov models","heading":"3.30 Hidden Markov model for survival","text":"states (gray), \\(z = 1\\) alive, \\(z = 2\\) dead.states (gray), \\(z = 1\\) alive, \\(z = 2\\) dead.observations (white), \\(y = 1\\) non-detected, \\(y = 2\\) detectedFor observations (white), \\(y = 1\\) non-detected, \\(y = 2\\) detectedNow add states alive dead, 1 2s.Now add states alive dead, 1 2s.observations, non-detected detected, 1 2s.observations, non-detected detected, 1 2s.parameters, phi transition 1 1.parameters, phi transition 1 1.p prob y 2 detected given z 1 alive.p prob y 2 detected given z 1 alive.","code":""},{"path":"hmmcapturerecapture.html","id":"hmm-likelihood","chapter":"3 Hidden Markov models","heading":"3.31 HMM likelihood","text":"Using formula total probability, likelihood Markov chain:\\[\\begin{align*}\n\\Pr(\\mathbf{y}) &= \\Pr(y_1, y_{2}, \\ldots, y_T)\\\\\n                &= \\sum_{z_1} \\cdots \\sum_{z_T} \\Pr(y_1, y_{2}, \\ldots, y_T | z_1, z_{2}, \\ldots, z_T) \\Pr(z_1, z_{2}, \\ldots, z_T)\\\\\n                &= \\sum_{z_1} \\cdots \\sum_{z_T} \\left(\\prod_{t=1}^T{\\omega_{z_{t}, y_t}}\\right) \\left(\\Pr(z_{1}) \\prod_{t=2}^T{\\gamma_{z_{t-1},z_{t}}}\\right)\\\\\n\\end{align*}\\]likelihood HMM.likelihood HMM.thing don’t know states.thing don’t know states.go possibilities, sum possible states.go possibilities, sum possible states.Hence sums .Hence sums .term likelihood Markov chain, saw .term likelihood Markov chain, saw .component elements observation matrix.component elements observation matrix.likelihood matrix formulation can useful.likelihood matrix formulation can useful.delta, initial states, observation, transitions, . vector ones end get sum terms.delta, initial states, observation, transitions, . vector ones end get sum terms.matrix formulation:\n\\[\\begin{align*}\n\\Pr(\\mathbf{y}) &= \\mathbf{\\delta} \\; \\mathbf{\\Omega} \\; \\mathbf{\\Gamma} \\cdots \\mathbf{\\Omega} \\; \\mathbf{\\Gamma} \\; \\mathbf{\\Omega} \\; \\mathbb{1}\n\\end{align*}\\]matrix formulation:\n\\[\\begin{align*}\n\\Pr(\\mathbf{y}) &= \\mathbf{\\delta} \\; \\mathbf{\\Omega} \\; \\mathbf{\\Gamma} \\cdots \\mathbf{\\Omega} \\; \\mathbf{\\Gamma} \\; \\mathbf{\\Omega} \\; \\mathbb{1}\n\\end{align*}\\]","code":""},{"path":"hmmcapturerecapture.html","id":"example-1","chapter":"3 Hidden Markov models","heading":"3.32 Example","text":"Let assume animal detected, missed.Let assume animal detected, missed.\\(\\mathbf{y} = (2, 1)\\). contribution animal likelihood?\\(\\mathbf{y} = (2, 1)\\). contribution animal likelihood?\\[\\begin{align*}\n\\Pr(\\mathbf{y} = (2, 1)) &= \\sum_{z_1 = 1}^2 \\; \\sum_{z_2 = 1}^2 w_{z_1, y_1 = 2} w_{z_2, y_2 = 1} \\Pr(z_1) \\gamma_{z_1, z_2} \\color{white}{\\Pr(z_{T-1}, z_{T-2},\\ldots, z_1, z_1, z_1, z_1)}\\\\\n\\end{align*}\\]","code":""},{"path":"hmmcapturerecapture.html","id":"example-2","chapter":"3 Hidden Markov models","heading":"3.33 Example","text":"Let assume animal detected, missed.Let assume animal detected, missed.\\(\\mathbf{y} = (2, 1)\\). contribution animal likelihood?\\(\\mathbf{y} = (2, 1)\\). contribution animal likelihood?\\[\\begin{align*}\n\\Pr(\\mathbf{y} = (2, 1)) &= \\sum_{z_1 = 1}^2 \\; \\sum_{z_2 = 1}^2 w_{z_1, y_1 = 2} w_{z_2, y_2 = 1} \\Pr(z_1) \\gamma_{z_1, z_2} \\color{white}{\\Pr(z_{T-1}, z_{T-2},\\ldots, z_1, z_1, z_1, z_1)}\\\\\n&= \\sum_{z_1 = 1}^2 \\left( w_{z_1, y_1 = 2} w_{z_2 = 1, y_2 = 1} \\Pr(z_1) \\gamma_{z_1, z_2 = 1} + w_{z_1, y_1 = 2} w_{z_2 = 2, y_2 = 1} \\Pr(z_1) \\gamma_{z_1, z_2 = 2} \\right) \\\\\n\\end{align*}\\]Let assume animal detected, missed.Let assume animal detected, missed.\\(\\mathbf{y} = (2, 1)\\). contribution animal likelihood?\\(\\mathbf{y} = (2, 1)\\). contribution animal likelihood?\\[\\begin{align*}\n\\Pr(\\mathbf{y} = (2, 1)) &= \\sum_{z_1 = 1}^2 \\; \\sum_{z_2 = 1}^2 w_{z_1, y_1 = 2} w_{z_2, y_2 = 1} \\Pr(z_1) \\gamma_{z_1, z_2} \\color{white}{\\Pr(z_{T-1}, z_{T-2},\\ldots, z_1, z_1, z_1, z_1)}\\\\\n&= \\sum_{z_1 = 1}^2 \\left( w_{z_1, y_1 = 2} w_{z_2 = 1, y_2 = 1} \\Pr(z_1) \\gamma_{z_1, z_2 = 1} + w_{z_1, y_1 = 2} w_{z_2 = 2, y_2 = 1} \\Pr(z_1) \\gamma_{z_1, z_2 = 2} \\right) \\\\\n&= w_{z_1 = 1, y_1 = 2} w_{z_2 = 1, y_2 = 1}\\delta_1 \\gamma_{z_1 = 1, z_2 = 1} + w_{z_1 = 1, y_1 = 2} w_{z_2 = 2, y_2 = 1} \\delta_1 \\gamma_{z_1 = 1, z_2 = 2}\n\\end{align*}\\]Note: \\(\\Pr(z_1 = 1) = \\delta_1 = 1\\) \\(\\Pr(z_1 = 2) = 0\\).Let assume animal detected, missed.Let assume animal detected, missed.\\(\\mathbf{y} = (2, 1)\\). contribution animal likelihood?\\(\\mathbf{y} = (2, 1)\\). contribution animal likelihood?\\[\\begin{align*}\n\\Pr(\\mathbf{y} = (2, 1)) &= \\sum_{z_1 = 1}^2 \\; \\sum_{z_2 = 1}^2 w_{z_1, y_1 = 2} w_{z_2, y_2 = 1} \\Pr(z_1) \\gamma_{z_1, z_2} \\color{white}{\\Pr(z_{T-1}, z_{T-2},\\ldots, z_1, z_1, z_1, z_1)}\\\\\n&= \\sum_{z_1 = 1}^2 \\left( w_{z_1, y_1 = 2} w_{z_2 = 1, y_2 = 1} \\Pr(z_1) \\gamma_{z_1, z_2 = 1} + w_{z_1, y_1 = 2} w_{z_2 = 2, y_2 = 1} \\Pr(z_1) \\gamma_{z_1, z_2 = 2} \\right) \\\\\n&= w_{z_1 = 1, y_1 = 2} w_{z_2 = 1, y_2 = 1} \\delta_1 \\gamma_{z_1 = 1, z_2 = 1} + w_{z_1 = 1, y_1 = 2} w_{z_2 = 2, y_2 = 1} \\delta_1 \\gamma_{z_1 = 1, z_2 = 2}\\\\\n&= (1 - p) \\phi + (1-\\phi)\n\\end{align*}\\]Note: \\(w_{z_1 = 1, y_1 = 2} = \\Pr(y_1 = 2 | z_1 = 1) = 1\\) condition first capture.","code":""},{"path":"hmmcapturerecapture.html","id":"estimating-the-latent-states-z-or-not","chapter":"3 Hidden Markov models","heading":"3.34 Estimating the latent states \\(z\\) or not?","text":"Next question , shall estimate latent states ?Next question , shall estimate latent states ?previous example, got rid states, likelihood function \\(\\phi\\) \\(p\\) . function maximize Frequentist approach.previous example, got rid states, likelihood function \\(\\phi\\) \\(p\\) . function maximize Frequentist approach.Bayesian approach MCMC methods allows treating latent states parameters, estimated .Bayesian approach MCMC methods allows treating latent states parameters, estimated .Infering latent states \\(z\\) can useful estimate prevalence, e.g. animal epidemiology prevalence disease, evolutionary ecology sex ratio conservation biology prevalence hybrids.Infering latent states \\(z\\) can useful estimate prevalence, e.g. animal epidemiology prevalence disease, evolutionary ecology sex ratio conservation biology prevalence hybrids.Estimating latent states costly though, required, marginalisation may speed computations. Actually, can estimate states afterwards (Viterbi).Estimating latent states costly though, required, marginalisation may speed computations. Actually, can estimate states afterwards (Viterbi).-called marginalisation Yackulic et al. (2020).-called marginalisation Yackulic et al. (2020).neat thing Nimble provides marginalised models nimbleEcology, ’ll get back Class 8.neat thing Nimble provides marginalised models nimbleEcology, ’ll get back Class 8.","code":""},{"path":"hmmcapturerecapture.html","id":"our-model-2","chapter":"3 Hidden Markov models","heading":"3.35 Our model","text":"\\[\\begin{align*}\n   z_{\\text{first}} &\\sim \\text{Multinomial}(1, \\delta) &\\text{[likelihood]}\\\\\n   z_t | z_{t-1} &\\sim \\text{Multinomial}(1, \\gamma_{z_{t-1},z_{t}}) &\\text{[likelihood]}\\\\\n   y_t | z_{t} &\\sim \\text{Multinomial}(1, \\omega_{z_{t}}) &\\text{[likelihood]}\\\\\n  \\phi &\\sim \\text{Beta}(1, 1) &\\text{[prior }\\phi \\text{]} \\\\ \n  p &\\sim \\text{Beta}(1, 1) &\\text{[prior }p \\text{]} \\\\ \n\\end{align*}\\]Now model observation layer ys, conditional z.need prior detection probability.","code":""},{"path":"hmmcapturerecapture.html","id":"nimble-implementation-1","chapter":"3 Hidden Markov models","heading":"3.36 Nimble implementation","text":"implement model Nimble?","code":""},{"path":"hmmcapturerecapture.html","id":"priors","chapter":"3 Hidden Markov models","heading":"3.37 Priors","text":"","code":"hmm.survival <- nimbleCode({\n  phi ~ dunif(0, 1) # prior survival\n  p ~ dunif(0, 1) # prior detection\n..."},{"path":"hmmcapturerecapture.html","id":"hmm-ingredients","chapter":"3 Hidden Markov models","heading":"3.38 HMM ingredients","text":"","code":"\n...\n  # parameters\n  gamma[1,1] <- phi      # Pr(alive t -> alive t+1)\n  gamma[1,2] <- 1 - phi  # Pr(alive t -> dead t+1)\n  gamma[2,1] <- 0        # Pr(dead t -> alive t+1)\n  gamma[2,2] <- 1        # Pr(dead t -> dead t+1)\n  delta[1] <- 1          # Pr(alive t = 1) = 1\n  delta[2] <- 0          # Pr(dead t = 1) = 0\n  omega[1,1] <- 1 - p    # Pr(alive t -> non-detected t)\n  omega[1,2] <- p        # Pr(alive t -> detected t)\n  omega[2,1] <- 1        # Pr(dead t -> non-detected t)\n  omega[2,2] <- 0        # Pr(dead t -> detected t)\n..."},{"path":"hmmcapturerecapture.html","id":"likelihood-1","chapter":"3 Hidden Markov models","heading":"3.39 Likelihood","text":"","code":"...\n    # likelihood\n    for (i in 1:N){\n    z[i,first[i]] ~ dcat(delta[1:2])\n    for (j in (first[i]+1):T){\n      z[i,j] ~ dcat(gamma[z[i,j-1], 1:2])\n      y[i,j] ~ dcat(omega[z[i,j], 1:2])\n    }\n  }\n})"},{"path":"hmmcapturerecapture.html","id":"constants","chapter":"3 Hidden Markov models","heading":"3.40 Constants","text":"","code":"\nfirst <- apply(y, 1, function(x) min(which(x !=0)))\nmy.constants <- list(N = nrow(y), T = 5, first = first)\nmy.constants\n## $N\n## [1] 48\n## \n## $T\n## [1] 5\n## \n## $first\n##  [1] 3 1 3 3 1 1 1 1 1 1 1 1 2 2 2 1 1 1 1 1 2 2 1 1 1 1 1 1\n## [29] 2 2 2 2 1 1 1 2 2 1 2 1 1 1 2 3 2 1 1 3"},{"path":"hmmcapturerecapture.html","id":"data","chapter":"3 Hidden Markov models","heading":"3.41 Data","text":"data made 0s non-detections 1s detections.data made 0s non-detections 1s detections.use categorical distribution, need code 1, 2, etc. Value 0 accepted.use categorical distribution, need code 1, 2, etc. Value 0 accepted.Add 1 get correct format \\(y=1\\) non-detection \\(y = 2\\) detection.Add 1 get correct format \\(y=1\\) non-detection \\(y = 2\\) detection.","code":"\nmy.data <- list(y = y + 1)"},{"path":"hmmcapturerecapture.html","id":"initial-values-1","chapter":"3 Hidden Markov models","heading":"3.42 Initial values","text":"","code":"\nzinits <- y + 1 # non-detection -> alive\nzinits[zinits == 2] <- 1 # dead -> alive\ninitial.values <- function() list(phi = runif(1,0,1),\n                                  p = runif(1,0,1),\n                                  z = zinits)"},{"path":"hmmcapturerecapture.html","id":"parameters-to-monitor-1","chapter":"3 Hidden Markov models","heading":"3.43 Parameters to monitor","text":"","code":"\nparameters.to.save <- c(\"phi\", \"p\")\nparameters.to.save\n## [1] \"phi\" \"p\""},{"path":"hmmcapturerecapture.html","id":"mcmc-details-2","chapter":"3 Hidden Markov models","heading":"3.44 MCMC details","text":"","code":"\nn.iter <- 5000\nn.burnin <- 1000\nn.chains <- 2"},{"path":"hmmcapturerecapture.html","id":"run-nimble-1","chapter":"3 Hidden Markov models","heading":"3.45 Run Nimble","text":"","code":"\nmcmc.output <- nimbleMCMC(code = hmm.survival, \n                          constants = my.constants,\n                          data = my.data,              \n                          inits = initial.values,\n                          monitors = parameters.to.save,\n                          niter = n.iter, \n                          nburnin = n.burnin, \n                          nchains = n.chains)\n## |-------------|-------------|-------------|-------------|\n## |-------------------------------------------------------|\n## |-------------|-------------|-------------|-------------|\n## |-------------------------------------------------------|\nmcmc.output <- nimbleMCMC(code = hmm.survival, \n                          constants = my.constants,\n                          data = my.data,              \n                          inits = initial.values,\n                          monitors = parameters.to.save,\n                          niter = n.iter, \n                          nburnin = n.burnin, \n                          nchains = n.chains,\n                          progressBar = FALSE)"},{"path":"hmmcapturerecapture.html","id":"posterior-distribution-of-survival-1","chapter":"3 Hidden Markov models","heading":"3.46 Posterior distribution of survival","text":"data simulated, true survival \\(\\phi = 0.8\\) detection \\(p = 0.6\\).","code":"\nlibrary(MCMCvis)\nMCMCsummary(mcmc.output, round = 2)\n##     mean   sd 2.5%  50% 97.5% Rhat n.eff\n## p   0.56 0.06 0.43 0.56  0.67 1.11   360\n## phi 0.88 0.04 0.80 0.88  0.97 1.14   327"},{"path":"hmmcapturerecapture.html","id":"further-reading-1","chapter":"3 Hidden Markov models","heading":"3.47 Further reading","text":"Zucchini, MacDonald Langrock (2016) Hidden Markov Models Time Series: Introduction Using R (2nd ed). Chapman Hall/CRC.Zucchini, MacDonald Langrock (2016) Hidden Markov Models Time Series: Introduction Using R (2nd ed). Chapman Hall/CRC.McClintock, B.T., Langrock, R., Gimenez, O., Cam, E., Borchers, D.L., Glennie, R. Patterson, T.. (2020), Uncovering ecological state dynamics hidden Markov models. Ecology Letters, 23: 1878-1903.McClintock, B.T., Langrock, R., Gimenez, O., Cam, E., Borchers, D.L., Glennie, R. Patterson, T.. (2020), Uncovering ecological state dynamics hidden Markov models. Ecology Letters, 23: 1878-1903.Yackulic, C. B. Dodrill, M., Dzul, M., Sanderlin, J. S., Reid, J. .. (2020). need speed Bayesian population models: practical guide marginalizing recovering discrete latent states. Ecological Applications 30:e02112.Yackulic, C. B. Dodrill, M., Dzul, M., Sanderlin, J. S., Reid, J. .. (2020). need speed Bayesian population models: practical guide marginalizing recovering discrete latent states. Ecological Applications 30:e02112.L. R. Rabiner (1989). tutorial hidden Markov models selected applications speech recognition. Proceedings IEEE, 77:257-286.L. R. Rabiner (1989). tutorial hidden Markov models selected applications speech recognition. Proceedings IEEE, 77:257-286.Heller Pogaru (2021)","code":""},{"path":"survival.html","id":"survival","chapter":"4 Survival","heading":"4 Survival","text":"","code":""},{"path":"transition.html","id":"transition","chapter":"5 Transition","heading":"5 Transition","text":"","code":""},{"path":"covariates.html","id":"covariates","chapter":"6 Covariates","heading":"6 Covariates","text":"","code":""},{"path":"uncertainty.html","id":"uncertainty","chapter":"7 Uncertainty in state assignment","heading":"7 Uncertainty in state assignment","text":"","code":""},{"path":"abundance.html","id":"abundance","chapter":"8 Abundance","heading":"8 Abundance","text":"","code":""},{"path":"hsmm.html","id":"hsmm","chapter":"9 Hidden semi-Markov models","heading":"9 Hidden semi-Markov models","text":"","code":""},{"path":"states.html","id":"states","chapter":"10 Hidden states","heading":"10 Hidden states","text":"","code":""},{"path":"speed.html","id":"speed","chapter":"11 Speed up MCMC","heading":"11 Speed up MCMC","text":"","code":""},{"path":"senescence.html","id":"senescence","chapter":"12 Actuarial senescence","heading":"12 Actuarial senescence","text":"Choquet et al. (2011), Péron et al. (2016)","code":""},{"path":"heterogeneity.html","id":"heterogeneity","chapter":"13 Individual heterogeneity","heading":"13 Individual heterogeneity","text":"Cubaynes et al. (2010), Gimenez Choquet (2010), Turek, Wehrhahn, Gimenez (2021)","code":""},{"path":"tradeoffs.html","id":"tradeoffs","chapter":"14 Life-history tradeoffs","heading":"14 Life-history tradeoffs","text":"Morano et al. (2013), Shefferson et al. (2003), Cruz-Flores et al. (n.d.)","code":""},{"path":"breeding.html","id":"breeding","chapter":"15 Breeding dynamics","heading":"15 Breeding dynamics","text":"Pradel, Choquet, Béchet (2012), Desprez et al. (2011), Desprez et al. (2013), Pacoureau et al. (2019)","code":""},{"path":"rd.html","id":"rd","chapter":"16 Robust design","heading":"16 Robust design","text":"Karamanlidis et al. (2015), Santostasi et al. (2016), Gibson et al. (2018), Rankin et al. (2016)","code":""},{"path":"stopover.html","id":"stopover","chapter":"17 Stopover duration","heading":"17 Stopover duration","text":"Guérin et al. (2017)","code":""},{"path":"disease.html","id":"disease","chapter":"18 Disease dynamics","heading":"18 Disease dynamics","text":"Marescot et al. (2018) Santoro et al. (2014)","code":""},{"path":"sex.html","id":"sex","chapter":"19 Sex uncertainty","heading":"19 Sex uncertainty","text":"Pradel et al. (2008) Genovart, Pradel, Oro (2012)","code":""},{"path":"dependence.html","id":"dependence","chapter":"20 Dependence among individuals","heading":"20 Dependence among individuals","text":"Culina et al. (2013) Cubaynes et al. (2021)","code":""},{"path":"covariateselection.html","id":"covariateselection","chapter":"21 Individual and temporal variability","heading":"21 Individual and temporal variability","text":"Grosbois et al. (2008), Cubaynes et al. (2012), Gimenez et al. (2006), Bonner, Morgan, King (2010)","code":""},{"path":"mortalities.html","id":"mortalities","chapter":"22 Cause-specific mortalities","heading":"22 Cause-specific mortalities","text":"Fernández-Chacón et al. (2016) Ruette et al. (2015)","code":""},{"path":"prevalence.html","id":"prevalence","chapter":"23 Prevalence","heading":"23 Prevalence","text":"(Santostasi et al. 2019)","code":""},{"path":"faq.html","id":"faq","chapter":"FAQ","heading":"FAQ","text":"complete list frequently asked questions (FAQ). Yes, one question . Personally like FAQs. often mean surprises, surprises good software users.Q: bookdown features X, Y, Z?\n: short answer , asked three times “really need ” answer still “yes,” please feel free file feature request https://github.com/rstudio/bookdown/issues.\nUsers asking features often come LaTeX world. case , answer question yes, Pandoc’s Markdown supports raw LaTeX code. Whenever feel Markdown job , always option apply raw LaTeX code Markdown document. example, can create glossaries using glossaries package, embed complicated LaTeX table, long know LaTeX syntax. However, please keep mind LaTeX content portable. work LaTeX/PDF output, ignored types output. Depending request, may port LaTeX features bookdown future, general philosophy Markdown kept simple possible.Q: bookdown features X, Y, Z?: short answer , asked three times “really need ” answer still “yes,” please feel free file feature request https://github.com/rstudio/bookdown/issues.Users asking features often come LaTeX world. case , answer question yes, Pandoc’s Markdown supports raw LaTeX code. Whenever feel Markdown job , always option apply raw LaTeX code Markdown document. example, can create glossaries using glossaries package, embed complicated LaTeX table, long know LaTeX syntax. However, please keep mind LaTeX content portable. work LaTeX/PDF output, ignored types output. Depending request, may port LaTeX features bookdown future, general philosophy Markdown kept simple possible.challenging thing world learn fancy technologies, control wild heart.","code":""},{"path":"references.html","id":"references","chapter":"References","heading":"References","text":"","code":""}]
