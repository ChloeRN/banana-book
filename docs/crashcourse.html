<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Chapter 1 Bayesian statistics &amp; MCMC | Bayesian Analysis of Capture-Recapture Data with Hidden Markov Models – Theory and Case Studies in R</title>
<meta name="author" content="Olivier Gimenez">
<meta name="description" content="1.1 Bayes’ theorem A theorem about conditional probabilities. \(\Pr(B \mid A) = \displaystyle{\frac{ \Pr(A \mid B) \; \Pr(B)}{\Pr(A)}}\)  Figure 1.1: Bayes’ theorem spelt out in blue neon at the...">
<meta name="generator" content="bookdown 0.23 with bs4_book()">
<meta property="og:title" content="Chapter 1 Bayesian statistics &amp; MCMC | Bayesian Analysis of Capture-Recapture Data with Hidden Markov Models – Theory and Case Studies in R">
<meta property="og:type" content="book">
<meta property="og:url" content="https://oliviergimenez.github.io/bayesian-cr-workshop/crashcourse.html">
<meta property="og:image" content="https://oliviergimenez.github.io/bayesian-cr-workshop//images/satellite.png">
<meta property="og:description" content="1.1 Bayes’ theorem A theorem about conditional probabilities. \(\Pr(B \mid A) = \displaystyle{\frac{ \Pr(A \mid B) \; \Pr(B)}{\Pr(A)}}\)  Figure 1.1: Bayes’ theorem spelt out in blue neon at the...">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Chapter 1 Bayesian statistics &amp; MCMC | Bayesian Analysis of Capture-Recapture Data with Hidden Markov Models – Theory and Case Studies in R">
<meta name="twitter:description" content="1.1 Bayes’ theorem A theorem about conditional probabilities. \(\Pr(B \mid A) = \displaystyle{\frac{ \Pr(A \mid B) \; \Pr(B)}{\Pr(A)}}\)  Figure 1.1: Bayes’ theorem spelt out in blue neon at the...">
<meta name="twitter:image" content="https://oliviergimenez.github.io/bayesian-cr-workshop//images/satellite.png">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/header-attrs/header-attrs.js"></script><script src="libs/jquery/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap/bootstrap.bundle.min.js"></script><script src="libs/bs3compat/tabs.js"></script><script src="libs/bs3compat/bs3compat.js"></script><link href="libs/bs4_book/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book/bs4_book.js"></script><script src="libs/kePrint/kePrint.js"></script><link href="libs/lightable/lightable.css" rel="stylesheet">
<script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- CSS --><link rel="stylesheet" href="bs4_style.css">
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">Bayesian Analysis of Capture-Recapture Data with Hidden Markov Models – Theory and Case Studies in R</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html">Welcome</a></li>
<li><a class="" href="preface.html">Preface</a></li>
<li><a class="" href="about-the-author.html">About the Author</a></li>
<li class="book-part">Theory</li>
<li><a class="active" href="crashcourse.html"><span class="header-section-number">1</span> Bayesian statistics &amp; MCMC</a></li>
<li><a class="" href="intronimble.html"><span class="header-section-number">2</span> Introduction to Nimble</a></li>
<li><a class="" href="hmmcapturerecapture.html"><span class="header-section-number">3</span> Hidden Markov models</a></li>
<li><a class="" href="survival.html"><span class="header-section-number">4</span> Survival</a></li>
<li><a class="" href="transition.html"><span class="header-section-number">5</span> Transition</a></li>
<li><a class="" href="covariates.html"><span class="header-section-number">6</span> Covariates</a></li>
<li><a class="" href="uncertainty.html"><span class="header-section-number">7</span> Uncertainty in state assignment</a></li>
<li><a class="" href="abundance.html"><span class="header-section-number">8</span> Abundance</a></li>
<li><a class="" href="hsmm.html"><span class="header-section-number">9</span> Hidden semi-Markov models</a></li>
<li><a class="" href="states.html"><span class="header-section-number">10</span> Hidden states</a></li>
<li><a class="" href="speed.html"><span class="header-section-number">11</span> Speed up MCMC</a></li>
<li><a class="" href="conclusions.html"><span class="header-section-number">12</span> Conclusions</a></li>
<li class="book-part">Case studies</li>
<li><a class="" href="senescence.html"><span class="header-section-number">13</span> Actuarial senescence</a></li>
<li><a class="" href="heterogeneity.html"><span class="header-section-number">14</span> Individual heterogeneity</a></li>
<li><a class="" href="tradeoffs.html"><span class="header-section-number">15</span> Life-history tradeoffs</a></li>
<li><a class="" href="breeding.html"><span class="header-section-number">16</span> Breeding dynamics</a></li>
<li><a class="" href="rd.html"><span class="header-section-number">17</span> Robust design</a></li>
<li><a class="" href="stopover.html"><span class="header-section-number">18</span> Stopover duration</a></li>
<li><a class="" href="disease.html"><span class="header-section-number">19</span> Disease dynamics</a></li>
<li><a class="" href="sex.html"><span class="header-section-number">20</span> Sex uncertainty</a></li>
<li><a class="" href="dependence.html"><span class="header-section-number">21</span> Dependence among individuals</a></li>
<li><a class="" href="covariateselection.html"><span class="header-section-number">22</span> Individual and temporal variability</a></li>
<li><a class="" href="mortalities.html"><span class="header-section-number">23</span> Cause-specific mortalities</a></li>
<li><a class="" href="prevalence.html"><span class="header-section-number">24</span> Prevalence</a></li>
<li><a class="" href="faq.html">FAQ</a></li>
<li><a class="" href="references.html">References</a></li>
</ul>

        <div class="book-extra">
          <p><a id="book-repo" href="https://github.com/oliviergimenez/banana-book">View book source <i class="fab fa-github"></i></a></p>
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="crashcourse" class="section level1" number="1">
<h1>
<span class="header-section-number">1</span> Bayesian statistics &amp; MCMC<a class="anchor" aria-label="anchor" href="#crashcourse"><i class="fas fa-link"></i></a>
</h1>
<div id="bayes-theorem" class="section level2" number="1.1">
<h2>
<span class="header-section-number">1.1</span> Bayes’ theorem<a class="anchor" aria-label="anchor" href="#bayes-theorem"><i class="fas fa-link"></i></a>
</h2>
<p>A theorem about conditional probabilities.</p>
<p><span class="math inline">\(\Pr(B \mid A) = \displaystyle{\frac{ \Pr(A \mid B) \; \Pr(B)}{\Pr(A)}}\)</span></p>
<div class="figure">
<span style="display:block;" id="fig:unnamed-chunk-6"></span>
<img src="images/bayes_neon.jpeg" alt="Bayes' theorem spelt out in blue neon at the offices of Autonomy in Cambridge. Source: Wikipedia" width="400"><p class="caption">
Figure 1.1: Bayes’ theorem spelt out in blue neon at the offices of Autonomy in Cambridge. Source: Wikipedia
</p>
</div>
<p>I always forget what the letters mean.</p>
<p>Might be easier to remember when written like this:</p>
<p><span class="math display">\[ \Pr(\text{hypothesis} \mid \text{data}) = \frac{ \Pr(\text{data} \mid \text{hypothesis}) \; \Pr(\text{hypothesis})}{\Pr(\text{data})} \]</span></p>
<p>The “hypothesis” is typically something unobserved or unknown. It’s what you want to learn about using the data.</p>
<p>For regression models, the “hypothesis” is a parameter (intercept, slopes or error terms).</p>
<p>Bayes theorem tells you the probability of the hypothesis given the data.</p>
<p>Cool because what is doing science after all?</p>
<p>How plausible is some hypothesis given the data?</p>
<p><span class="math display">\[ \Pr(\text{hypothesis} \mid \text{data}) = \frac{ \Pr(\text{data} \mid \text{hypothesis}) \; \Pr(\text{hypothesis})}{\Pr(\text{data})} \]</span></p>
<p>The Bayesian reasoning echoes the scientific reasoning. You might ask then, why is Bayesian statistics not the default?</p>
<p>You may ask: Why is Bayesian statistics not the default?</p>
<p>Due to practical problems of implementing the Bayesian approach, and futile wars between (male) statisticians, little progress was made for over two centuries.</p>
<p>Recent advances in computational power coupled with the development of new methodology have led to a great increase in the application of Bayesian methods within the last two decades.</p>
</div>
<div id="frequentist-versus-bayesian" class="section level2" number="1.2">
<h2>
<span class="header-section-number">1.2</span> Frequentist versus Bayesian<a class="anchor" aria-label="anchor" href="#frequentist-versus-bayesian"><i class="fas fa-link"></i></a>
</h2>
<p>Typical stats problems involve estimating parameter <span class="math inline">\(\theta\)</span> with available data.</p>
<p>The frequentist approach (maximum likelihood estimation – MLE) assumes that the parameters are fixed, but have unknown values to be estimated.</p>
<p>Classical estimates are generally point estimates of the parameters of interest.</p>
<p>The Bayesian approach assumes that the parameters are not fixed but have some fixed unknown distribution - a distribution for the parameter.</p>
</div>
<div id="what-is-the-bayesian-approach" class="section level2" number="1.3">
<h2>
<span class="header-section-number">1.3</span> What is the Bayesian approach?<a class="anchor" aria-label="anchor" href="#what-is-the-bayesian-approach"><i class="fas fa-link"></i></a>
</h2>
<p>The approach is based upon the idea that the experimenter begins with some prior beliefs about the system.</p>
<p>You never start from scratch.</p>
<p>And then updates these beliefs on the basis of observed data.</p>
<p>This updating procedure is based upon the Bayes’ Theorem:</p>
<p><span class="math display">\[\Pr(A \mid B) = \frac{\Pr(B \mid A) \; \Pr(A)}{\Pr(B)}\]</span></p>
<p>Schematically if <span class="math inline">\(A = \theta\)</span> and <span class="math inline">\(B = \text{data}\)</span>, then</p>
<p>The Bayes’ theorem</p>
<p><span class="math display">\[\Pr(A \mid B) = \frac{\Pr(B \mid A) \; \Pr(A)}{\Pr(B)}\]</span></p>
<p>Translates into:</p>
<p><span class="math display">\[\Pr(\theta \mid \text{data}) = \frac{\Pr(\text{data} \mid \theta) \; \Pr(\theta)}{\Pr(\text{data})}\]</span></p>
</div>
<div id="bayes-theorem-1" class="section level2" number="1.4">
<h2>
<span class="header-section-number">1.4</span> Bayes’ theorem<a class="anchor" aria-label="anchor" href="#bayes-theorem-1"><i class="fas fa-link"></i></a>
</h2>
<p><span class="math display">\[{\color{red}{\Pr(\theta \mid \text{data})}} = \frac{\color{blue}{\Pr(\text{data} \mid \theta)} \; \color{green}{\Pr(\theta)}}{\color{orange}{\Pr(\text{data})}}\]</span></p>
<p><span class="math inline">\(\color{red}{\text{Posterior distribution}}\)</span>: Represents what you know after having seen the data. The basis for inference, a distribution, possibly multivariate if more than one parameter.</p>
<p><span class="math inline">\(\color{blue}{\text{Likelihood}}\)</span>: This quantity is the same as in the MLE approach.</p>
<p><span class="math inline">\(\color{green}{\text{Prior distribution}}\)</span>: Represents what you know before seeing the data. The source of much discussion about the Bayesian approach.</p>
<p><span class="math inline">\(\color{orange}{\Pr(\text{data}) = \int{L(\text{data} \mid \theta)\Pr(\theta) d\theta}}\)</span> is a <span class="math inline">\(N\)</span>-dimensional integral if <span class="math inline">\(\theta = \theta_1, \ldots, \theta_N\)</span>.</p>
<p>Difficult if not impossible to calculate. This is one of the reasons why we need simulation (MCMC) methods.</p>
</div>
<div id="brute-force-via-numerical-integration" class="section level2" number="1.5">
<h2>
<span class="header-section-number">1.5</span> Brute force via numerical integration<a class="anchor" aria-label="anchor" href="#brute-force-via-numerical-integration"><i class="fas fa-link"></i></a>
</h2>
<p>Say we release <span class="math inline">\(n\)</span> animals at the beginning of the winter, out of which <span class="math inline">\(y\)</span> survive, and we’d like to estimate winter survival <span class="math inline">\(\theta\)</span>.</p>
<div class="sourceCode" id="cb2"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">y</span> <span class="op">&lt;-</span> <span class="fl">19</span> <span class="co"># nb of success</span>
<span class="va">n</span> <span class="op">&lt;-</span> <span class="fl">57</span> <span class="co"># nb of attempts</span></code></pre></div>
<p>Our model:</p>
<p><span class="math display">\[\begin{align*}
y &amp;\sim \text{Binomial}(n, \theta) &amp;\text{[likelihood]}
\\
\theta &amp;\sim \text{Beta}(1, 1) &amp;\text{[prior for }\theta \text{]} \\ 
\end{align*}\]</span></p>
</div>
<div id="beta-prior" class="section level2" number="1.6">
<h2>
<span class="header-section-number">1.6</span> Beta prior<a class="anchor" aria-label="anchor" href="#beta-prior"><i class="fas fa-link"></i></a>
</h2>
<div class="inline-figure"><img src="bayesHMMcapturerecapture_files/figure-html/unnamed-chunk-8-1.png" width="672"></div>
</div>
<div id="apply-bayes-theorem" class="section level2" number="1.7">
<h2>
<span class="header-section-number">1.7</span> Apply Bayes theorem<a class="anchor" aria-label="anchor" href="#apply-bayes-theorem"><i class="fas fa-link"></i></a>
</h2>
<p>Likelihood times the prior: <span class="math inline">\(\Pr(\text{data} \mid \theta) \; \Pr(\theta)\)</span></p>
<div class="sourceCode" id="cb3"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">numerator</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">p</span><span class="op">)</span> <span class="fu"><a href="https://rdrr.io/r/stats/Binomial.html">dbinom</a></span><span class="op">(</span><span class="va">y</span>,<span class="va">n</span>,<span class="va">p</span><span class="op">)</span> <span class="op">*</span> <span class="fu"><a href="https://rdrr.io/r/stats/Beta.html">dbeta</a></span><span class="op">(</span><span class="va">p</span>,<span class="va">a</span>,<span class="va">b</span><span class="op">)</span></code></pre></div>
<p>Averaged likelihood: <span class="math inline">\(\Pr(\text{data}) = \int{L(\theta \mid \text{data}) \; \Pr(\theta) d\theta}\)</span></p>
<div class="sourceCode" id="cb4"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">denominator</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/integrate.html">integrate</a></span><span class="op">(</span><span class="va">numerator</span>,<span class="fl">0</span>,<span class="fl">1</span><span class="op">)</span><span class="op">$</span><span class="va">value</span></code></pre></div>
</div>
<div id="posterior-via-numerical-integration" class="section level2" number="1.8">
<h2>
<span class="header-section-number">1.8</span> Posterior via numerical integration<a class="anchor" aria-label="anchor" href="#posterior-via-numerical-integration"><i class="fas fa-link"></i></a>
</h2>
<div class="inline-figure"><img src="bayesHMMcapturerecapture_files/figure-html/unnamed-chunk-11-1.png" width="672"></div>
</div>
<div id="superimpose-explicit-posterior" class="section level2" number="1.9">
<h2>
<span class="header-section-number">1.9</span> Superimpose explicit posterior<a class="anchor" aria-label="anchor" href="#superimpose-explicit-posterior"><i class="fas fa-link"></i></a>
</h2>
<div class="inline-figure"><img src="bayesHMMcapturerecapture_files/figure-html/unnamed-chunk-12-1.png" width="672"></div>
</div>
<div id="and-the-prior" class="section level2" number="1.10">
<h2>
<span class="header-section-number">1.10</span> And the prior<a class="anchor" aria-label="anchor" href="#and-the-prior"><i class="fas fa-link"></i></a>
</h2>
<div class="inline-figure"><img src="bayesHMMcapturerecapture_files/figure-html/unnamed-chunk-13-1.png" width="672"></div>
</div>
<div id="what-if-multiple-parameters" class="section level2" number="1.11">
<h2>
<span class="header-section-number">1.11</span> What if multiple parameters?<a class="anchor" aria-label="anchor" href="#what-if-multiple-parameters"><i class="fas fa-link"></i></a>
</h2>
<p>Example of a linear regression with parameters <span class="math inline">\(\alpha\)</span>, <span class="math inline">\(\beta\)</span> and <span class="math inline">\(\sigma\)</span> to be estimated.</p>
<p>Bayes’ theorem says:</p>
<p><span class="math display">\[ P(\alpha, \beta, \sigma \mid \text{data}) = \frac{ P(\text{data} \mid \alpha, \beta, \sigma) \, P(\alpha, \beta, \sigma)}{\iiint \, P(\text{data} \mid \alpha, \beta, \sigma) \, P(\alpha, \beta, \sigma) \,d\alpha \,d\beta \,d\sigma} \]</span></p>
<p>Do we really wish to calculate a 3D integral?</p>
</div>
<div id="bayesian-computation" class="section level2" number="1.12">
<h2>
<span class="header-section-number">1.12</span> Bayesian computation<a class="anchor" aria-label="anchor" href="#bayesian-computation"><i class="fas fa-link"></i></a>
</h2>
<p>In the early 1990s, statisticians rediscovered work from the 1950’s in physics.</p>
<div class="sourceCode" id="cb5"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu">knitr</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/knitr/man/include_graphics.html">include_graphics</a></span><span class="op">(</span><span class="st">"images/metropolis.png"</span><span class="op">)</span></code></pre></div>
<div class="inline-figure"><img src="images/metropolis.png" width="582"></div>
<p>Use stochastic simulation to draw samples from posterior distributions.</p>
<p>Avoid explicit calculation of integrals in Bayes formula.</p>
<p>Instead, approx. posterior w/ some precision by drawing large samples.</p>
<p>Markov chain Monte Carlo (MCMC) gives a boost to Bayesian statistics!</p>
</div>
<div id="why-are-mcmc-methods-so-useful" class="section level2" number="1.13">
<h2>
<span class="header-section-number">1.13</span> Why are MCMC methods so useful?<a class="anchor" aria-label="anchor" href="#why-are-mcmc-methods-so-useful"><i class="fas fa-link"></i></a>
</h2>
<ul>
<li><p>MCMC are stochastic algorithms to produce sequence of dependent random numbers from a Markov chain.</p></li>
<li><p>A Markov chain is a discrete sequence of states, in which the probability of an event depends only on the state in the previous event.</p></li>
<li><p>A Markov chain has an equilibrium (aka stationary) distribution.</p></li>
<li><p>Equilibrium distribution is the desired posterior distribution!</p></li>
<li><p>Several ways of constructing these chains: e.g., Metropolis-Hastings, Gibbs sampler.</p></li>
<li><p>How to implement them in practice?!</p></li>
</ul>
</div>
<div id="the-metropolis-algorithm" class="section level2" number="1.14">
<h2>
<span class="header-section-number">1.14</span> The Metropolis algorithm<a class="anchor" aria-label="anchor" href="#the-metropolis-algorithm"><i class="fas fa-link"></i></a>
</h2>
<ul>
<li><p>Let’s go back to animal survival estimation.</p></li>
<li><p>We illustrate sampling from the posterior distribution.</p></li>
<li><p>We write functions in <code>R</code> for the likelihood, the prior and the posterior.</p></li>
</ul>
<div class="sourceCode" id="cb6"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co"># survival data, 19 "success" out of 57 "attempts"</span>
<span class="va">survived</span> <span class="op">&lt;-</span> <span class="fl">19</span>
<span class="va">released</span> <span class="op">&lt;-</span> <span class="fl">57</span>

<span class="co"># log-likelihood function</span>
<span class="va">loglikelihood</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">x</span>, <span class="va">p</span><span class="op">)</span><span class="op">{</span>
  <span class="fu"><a href="https://rdrr.io/r/stats/Binomial.html">dbinom</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">x</span>, size <span class="op">=</span> <span class="va">released</span>, prob <span class="op">=</span> <span class="va">p</span>, log <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span>
<span class="op">}</span>

<span class="co"># prior density</span>
<span class="va">logprior</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">p</span><span class="op">)</span><span class="op">{</span>
  <span class="fu"><a href="https://rdrr.io/r/stats/Uniform.html">dunif</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">p</span>, min <span class="op">=</span> <span class="fl">0</span>, max <span class="op">=</span> <span class="fl">1</span>, log <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span>
<span class="op">}</span>

<span class="co"># posterior density function (log scale)</span>
<span class="va">posterior</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">x</span>, <span class="va">p</span><span class="op">)</span><span class="op">{</span>
  <span class="fu">loglikelihood</span><span class="op">(</span><span class="va">x</span>, <span class="va">p</span><span class="op">)</span> <span class="op">+</span> <span class="fu">logprior</span><span class="op">(</span><span class="va">p</span><span class="op">)</span> <span class="co"># - log(Pr(data))</span>
<span class="op">}</span></code></pre></div>
</div>
<div id="metropolis-algorithm" class="section level2" number="1.15">
<h2>
<span class="header-section-number">1.15</span> Metropolis algorithm<a class="anchor" aria-label="anchor" href="#metropolis-algorithm"><i class="fas fa-link"></i></a>
</h2>
<ol style="list-style-type: decimal">
<li><p>We start at any possible value of the parameter to be estimated.</p></li>
<li><p>To decide where to visit next, we propose to move away from the current value of the parameter <span>—</span> this is a <strong>candidate</strong> value. To do so, we add to the current value some random value from say a normal distribution with some variance.</p></li>
<li><p>We compute the ratio of the probabilities at the candidate and current locations <span class="math inline">\(R = \text{posterior(candidate)/posterior(current)}\)</span>. This is where the magic of MCMC happens, in that <span class="math inline">\(\Pr(\text{data})\)</span>, the denominator of the Bayes theorem, cancels out.</p></li>
<li><p>We spin a continuous spinner that lands anywhere from 0 to 1 <span>—</span> call it the random spin <span class="math inline">\(X\)</span>. If <span class="math inline">\(X\)</span> is smaller than <span class="math inline">\(R\)</span>, we move to the candidate location, otherwise we remain at the current location.</p></li>
<li><p>We repeat 2-4 a number of times <span>—</span> or <strong>steps</strong> (many steps).</p></li>
</ol>
<div class="sourceCode" id="cb7"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co"># propose candidate value</span>
<span class="va">move</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">x</span>, <span class="va">away</span> <span class="op">=</span> <span class="fl">.2</span><span class="op">)</span><span class="op">{</span> 
  <span class="va">logitx</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">x</span> <span class="op">/</span> <span class="op">(</span><span class="fl">1</span> <span class="op">-</span> <span class="va">x</span><span class="op">)</span><span class="op">)</span>
  <span class="va">logit_candidate</span> <span class="op">&lt;-</span> <span class="va">logitx</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="fl">1</span>, <span class="fl">0</span>, <span class="va">away</span><span class="op">)</span>
  <span class="va">candidate</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Logistic.html">plogis</a></span><span class="op">(</span><span class="va">logit_candidate</span><span class="op">)</span>
  <span class="kw"><a href="https://rdrr.io/r/base/function.html">return</a></span><span class="op">(</span><span class="va">candidate</span><span class="op">)</span>
<span class="op">}</span>

<span class="co"># set up the scene</span>
<span class="va">steps</span> <span class="op">&lt;-</span> <span class="fl">100</span>
<span class="va">theta.post</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/rep.html">rep</a></span><span class="op">(</span><span class="cn">NA</span>, <span class="va">steps</span><span class="op">)</span>
<span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">1234</span><span class="op">)</span>

<span class="co"># pick starting value (step 1)</span>
<span class="va">inits</span> <span class="op">&lt;-</span> <span class="fl">0.5</span>
<span class="va">theta.post</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="va">inits</span></code></pre></div>
<div class="sourceCode" id="cb8"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="kw">for</span> <span class="op">(</span><span class="va">t</span> <span class="kw">in</span> <span class="fl">2</span><span class="op">:</span><span class="va">steps</span><span class="op">)</span><span class="op">{</span> <span class="co"># repeat steps 2-4 (step 5)</span>
  
  <span class="co"># propose candidate value for prob of success (step 2)</span>
  <span class="va">theta_star</span> <span class="op">&lt;-</span> <span class="fu">move</span><span class="op">(</span><span class="va">theta.post</span><span class="op">[</span><span class="va">t</span><span class="op">-</span><span class="fl">1</span><span class="op">]</span><span class="op">)</span>
  
  <span class="co"># calculate ratio R (step 3)</span>
  <span class="va">pstar</span> <span class="op">&lt;-</span> <span class="fu">posterior</span><span class="op">(</span><span class="va">survived</span>, p <span class="op">=</span> <span class="va">theta_star</span><span class="op">)</span>  
  <span class="va">pprev</span> <span class="op">&lt;-</span> <span class="fu">posterior</span><span class="op">(</span><span class="va">survived</span>, p <span class="op">=</span> <span class="va">theta.post</span><span class="op">[</span><span class="va">t</span><span class="op">-</span><span class="fl">1</span><span class="op">]</span><span class="op">)</span>
  <span class="va">logR</span> <span class="op">&lt;-</span> <span class="va">pstar</span> <span class="op">-</span> <span class="va">pprev</span>
  <span class="va">R</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html">exp</a></span><span class="op">(</span><span class="va">logR</span><span class="op">)</span>
  
  <span class="co"># decide to accept candidate value or to keep current value (step 4)</span>
  <span class="va">accept</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Binomial.html">rbinom</a></span><span class="op">(</span><span class="fl">1</span>, <span class="fl">1</span>, prob <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/Extremes.html">min</a></span><span class="op">(</span><span class="va">R</span>, <span class="fl">1</span><span class="op">)</span><span class="op">)</span>
  <span class="va">theta.post</span><span class="op">[</span><span class="va">t</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/ifelse.html">ifelse</a></span><span class="op">(</span><span class="va">accept</span> <span class="op">==</span> <span class="fl">1</span>, <span class="va">theta_star</span>, <span class="va">theta.post</span><span class="op">[</span><span class="va">t</span><span class="op">-</span><span class="fl">1</span><span class="op">]</span><span class="op">)</span>
<span class="op">}</span></code></pre></div>
<p>Starting at the value <span class="math inline">\(0.5\)</span> and running the algorithm for <span class="math inline">\(100\)</span> iterations.</p>
<div class="sourceCode" id="cb9"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://rdrr.io/r/utils/head.html">head</a></span><span class="op">(</span><span class="va">theta.post</span><span class="op">)</span>
<span class="co">## [1] 0.5000 0.4399 0.4399 0.4577 0.4577 0.4577</span>
<span class="fu"><a href="https://rdrr.io/r/utils/head.html">tail</a></span><span class="op">(</span><span class="va">theta.post</span><span class="op">)</span>
<span class="co">## [1] 0.4146 0.3772 0.3772 0.3861 0.3899 0.3624</span></code></pre></div>
</div>
<div id="a-chain" class="section level2" number="1.16">
<h2>
<span class="header-section-number">1.16</span> A chain<a class="anchor" aria-label="anchor" href="#a-chain"><i class="fas fa-link"></i></a>
</h2>
<div class="inline-figure"><img src="bayesHMMcapturerecapture_files/figure-html/unnamed-chunk-19-1.png" width="672"></div>
</div>
<div id="another-chain" class="section level2" number="1.17">
<h2>
<span class="header-section-number">1.17</span> Another chain<a class="anchor" aria-label="anchor" href="#another-chain"><i class="fas fa-link"></i></a>
</h2>
<div class="inline-figure"><img src="bayesHMMcapturerecapture_files/figure-html/unnamed-chunk-20-1.png" width="672"></div>
</div>
<div id="with-5000-steps" class="section level2" number="1.18">
<h2>
<span class="header-section-number">1.18</span> With 5000 steps<a class="anchor" aria-label="anchor" href="#with-5000-steps"><i class="fas fa-link"></i></a>
</h2>
<div class="inline-figure"><img src="bayesHMMcapturerecapture_files/figure-html/unnamed-chunk-21-1.png" width="672"></div>
<p>In yellow: posterior mean; in red: maximum likelihood estimate.</p>
<div id="animating-mcmc---1d-example-code-here" class="section level3" number="1.18.1">
<h3>
<span class="header-section-number">1.18.1</span> Animating MCMC - 1D example (code <a href="https://gist.github.com/oliviergimenez/5ee33af9c8d947b72a39ed1764040bf3">here</a>)<a class="anchor" aria-label="anchor" href="#animating-mcmc---1d-example-code-here"><i class="fas fa-link"></i></a>
</h3>
<div class="sourceCode" id="cb10"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu">knitr</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/knitr/man/include_graphics.html">include_graphics</a></span><span class="op">(</span><span class="st">"images/112546886-56862f00-8dba-11eb-81a0-465434672bdd.gif"</span><span class="op">)</span></code></pre></div>
<div class="inline-figure">
<img src="images/112546886-56862f00-8dba-11eb-81a0-465434672bdd.gif"><!-- -->
</div>
</div>
<div id="animating-mcmc---2d-example" class="section level3" number="1.18.2">
<h3>
<span class="header-section-number">1.18.2</span> Animating MCMC - 2D example<a class="anchor" aria-label="anchor" href="#animating-mcmc---2d-example"><i class="fas fa-link"></i></a>
</h3>
<p>Code <a href="https://mbjoseph.github.io/posts/2018-12-25-animating-the-metropolis-algorithm/">here</a>.</p>
<div class="sourceCode" id="cb11"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu">knitr</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/knitr/man/include_graphics.html">include_graphics</a></span><span class="op">(</span><span class="st">"images/create-gif.gif"</span><span class="op">)</span></code></pre></div>
<div class="inline-figure">
<img src="images/create-gif.gif"><!-- -->
</div>
</div>
<div id="the-mcmc-interactive-gallery-more-here" class="section level3" number="1.18.3">
<h3>
<span class="header-section-number">1.18.3</span> The MCMC Interactive Gallery (more <a href="https://chi-feng.github.io/mcmc-demo/">here</a>)<a class="anchor" aria-label="anchor" href="#the-mcmc-interactive-gallery-more-here"><i class="fas fa-link"></i></a>
</h3>
<div class="sourceCode" id="cb12"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu">knitr</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/knitr/man/include_graphics.html">include_graphics</a></span><span class="op">(</span><span class="st">"images/galery.png"</span><span class="op">)</span></code></pre></div>
<div class="inline-figure"><img src="images/galery.png" width="992"></div>
</div>
</div>
<div id="assessing-convergence" class="section level2" number="1.19">
<h2>
<span class="header-section-number">1.19</span> Assessing convergence<a class="anchor" aria-label="anchor" href="#assessing-convergence"><i class="fas fa-link"></i></a>
</h2>
<ul>
<li><p>MCMC algorithms can be used to construct a Markov chain with a given stationary distribution (set to be the posterior distribution).</p></li>
<li><p>For the MCMC algorithm, the posterior distribution is only needed to be known up to proportionality.</p></li>
<li><p>Once the stationary distribution is reached, we can regard the realisations of the chain as a (dependent) sample from the posterior distribution (and obtain Monte Carlo estimates).</p></li>
<li><p>We consider some important implementation issues.</p></li>
</ul>
</div>
<div id="mixing-and-autocorrelation" class="section level2" number="1.20">
<h2>
<span class="header-section-number">1.20</span> Mixing and autocorrelation<a class="anchor" aria-label="anchor" href="#mixing-and-autocorrelation"><i class="fas fa-link"></i></a>
</h2>
<div class="inline-figure"><img src="bayesHMMcapturerecapture_files/figure-html/unnamed-chunk-25-1.png" width="672"></div>
<ul>
<li><p>The movement around the parameter space is often referred to as <strong>mixing</strong>.</p></li>
<li><p>Traceplots of for small and big moves provide (relatively) high correlations (known as autocorrelations) between successive observations of the Markov chain.</p></li>
<li><p>Strongly correlated observations require large sample sizes and therefore longer simulations.</p></li>
<li><p>Autocorrelation function (ACF) plots are a convenient way of displaying the strength of autocorrelation in the given sample values.</p></li>
<li><p>ACF plots provide the autocorrelation between successively sampled values separated by <span class="math inline">\(k\)</span> iterations, referred to as lag, (i.e. <span class="math inline">\(\text{cor}(\theta_t, \theta_{t+k})\)</span>) for increasing values of <span class="math inline">\(k\)</span>.</p></li>
</ul>
</div>
<div id="how-do-good-chains-behave" class="section level2" number="1.21">
<h2>
<span class="header-section-number">1.21</span> How do good chains behave?<a class="anchor" aria-label="anchor" href="#how-do-good-chains-behave"><i class="fas fa-link"></i></a>
</h2>
<ul>
<li><p>Converge to same target distribution; discard some realisations of Markov chain before convergence is achieved.</p></li>
<li><p>Once there, explore efficiently: The post-convergence sample size required for suitable numerical summaries.</p></li>
<li><p>Therefore, we are looking to determine how long it takes for the Markov chain to converge to the stationary distribution.</p></li>
<li><p>In practice, we must discard observations from the start of the chain and just use observations from the chain once it has converged.</p></li>
<li><p>The initial observations that we discard are referred to as the <strong>burn-in</strong>.</p></li>
<li><p>Simplest method to determine length of burn-in period is to look at trace plots.</p></li>
</ul>
</div>
<div id="burn-in" class="section level2" number="1.22">
<h2>
<span class="header-section-number">1.22</span> Burn-in<a class="anchor" aria-label="anchor" href="#burn-in"><i class="fas fa-link"></i></a>
</h2>
<div class="inline-figure"><img src="bayesHMMcapturerecapture_files/figure-html/unnamed-chunk-26-1.png" width="672"></div>
<p>If simulations cheap, be conservative.</p>
</div>
<div id="effective-sample-size-n.eff" class="section level2" number="1.23">
<h2>
<span class="header-section-number">1.23</span> Effective sample size <code>n.eff</code><a class="anchor" aria-label="anchor" href="#effective-sample-size-n.eff"><i class="fas fa-link"></i></a>
</h2>
<ul>
<li>How long of a chain is needed to produce stable estimates ?</li>
<li>Most MCMC chains are strongly autocorrelated.</li>
<li>Successive steps are near each other, and are not independent.</li>
<li>The effective sample size (<code>n.eff</code>) measures chain length while taking into account the autocorrelation of the chain.</li>
<li>
<code>n.eff</code> is less than the number of MCMC iterations.</li>
<li>Check the <code>n.eff</code> of every parameter of interest.</li>
<li>Check the <code>n.eff</code> of any interesting parameter combinations.</li>
<li>We need <span class="math inline">\(\text{n.eff} \geq 100\)</span> independent steps.</li>
</ul>
</div>
<div id="potential-scale-reduction-factor" class="section level2" number="1.24">
<h2>
<span class="header-section-number">1.24</span> Potential scale reduction factor<a class="anchor" aria-label="anchor" href="#potential-scale-reduction-factor"><i class="fas fa-link"></i></a>
</h2>
<ul>
<li>Gelman-Rubin statistic <span class="math inline">\(\hat{R}\)</span>
</li>
<li>Measures the ratio of the total variability combining multiple chains (between-chain plus within-chain) to the within-chain variability.</li>
<li>Asks the question is there a chain effect? Very much alike the <span class="math inline">\(F\)</span> test in an ANOVA.</li>
<li>Values near <span class="math inline">\(1\)</span> indicates likely convergence, a value of <span class="math inline">\(\leq 1.1\)</span> is considered acceptable.</li>
<li>Necessary condition, not sufficient; In other words, these diagnostics cannot tell you that you have converged for sure, only that you have not.</li>
</ul>
</div>
<div id="to-sum-up" class="section level2" number="1.25">
<h2>
<span class="header-section-number">1.25</span> To sum up<a class="anchor" aria-label="anchor" href="#to-sum-up"><i class="fas fa-link"></i></a>
</h2>
<ul>
<li>Run multiple chains from arbitrary starting places (initial values).</li>
<li>Assume convergence when all chains reach same regime</li>
<li>Discard initial burn-in phase.</li>
<li>Proceed with posterior inference.</li>
<li>Use traceplot, effective sample size and <span class="math inline">\(\hat{R}\)</span>.</li>
</ul>
</div>
<div id="what-if-you-have-issues-of-convergence" class="section level2" number="1.26">
<h2>
<span class="header-section-number">1.26</span> What if you have issues of convergence?<a class="anchor" aria-label="anchor" href="#what-if-you-have-issues-of-convergence"><i class="fas fa-link"></i></a>
</h2>
<ul>
<li>Increase burn-in, sample more.</li>
<li>Use more informative priors.</li>
<li>Pick better initial values (good guess), using e.g. estimates from simpler models.</li>
<li>Reparameterize:</li>
<li>Standardize covariates.</li>
<li>Non-centering: <span class="math inline">\(\alpha \sim N(0,\sigma)\)</span> becomes <span class="math inline">\(\alpha = z \sigma\)</span> with <span class="math inline">\(z \sim N(0,1)\)</span>.</li>
<li>Something wrong with your model?</li>
<li>Start with a simpler model (remove complexities).</li>
<li>Use simulations.</li>
<li>Change your sampler. More later on.</li>
</ul>
</div>
<div id="further-reading" class="section level2" number="1.27">
<h2>
<span class="header-section-number">1.27</span> Further reading<a class="anchor" aria-label="anchor" href="#further-reading"><i class="fas fa-link"></i></a>
</h2>
<ul>
<li><p>McCarthy, M. (2007). <a href="https://www.cambridge.org/core/books/bayesian-methods-for-ecology/9225F65B8A25D69B0B6C50B5A9A78201">Bayesian Methods for Ecology</a>. Cambridge: Cambridge University Press.</p></li>
<li><p>McElreath, R. (2020). <a href="https://xcelab.net/rm/statistical-rethinking/">Statistical Rethinking: A Bayesian Course with Examples in R and Stan (2nd ed.)</a>. CRC Press.</p></li>
<li><p>Gelman, A. and Hill, J. (2006). <a href="https://www.cambridge.org/core/books/data-analysis-using-regression-and-multilevelhierarchical-models/32A29531C7FD730C3A68951A17C9D983">Data Analysis Using Regression and Multilevel/Hierarchical Models (Analytical Methods for Social Research)</a>. Cambridge: Cambridge University Press.</p></li>
</ul>
</div>
</div>
  <div class="chapter-nav">
<div class="prev"><a href="about-the-author.html">About the Author</a></div>
<div class="next"><a href="intronimble.html"><span class="header-section-number">2</span> Introduction to Nimble</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#crashcourse"><span class="header-section-number">1</span> Bayesian statistics &amp; MCMC</a></li>
<li><a class="nav-link" href="#bayes-theorem"><span class="header-section-number">1.1</span> Bayes’ theorem</a></li>
<li><a class="nav-link" href="#frequentist-versus-bayesian"><span class="header-section-number">1.2</span> Frequentist versus Bayesian</a></li>
<li><a class="nav-link" href="#what-is-the-bayesian-approach"><span class="header-section-number">1.3</span> What is the Bayesian approach?</a></li>
<li><a class="nav-link" href="#bayes-theorem-1"><span class="header-section-number">1.4</span> Bayes’ theorem</a></li>
<li><a class="nav-link" href="#brute-force-via-numerical-integration"><span class="header-section-number">1.5</span> Brute force via numerical integration</a></li>
<li><a class="nav-link" href="#beta-prior"><span class="header-section-number">1.6</span> Beta prior</a></li>
<li><a class="nav-link" href="#apply-bayes-theorem"><span class="header-section-number">1.7</span> Apply Bayes theorem</a></li>
<li><a class="nav-link" href="#posterior-via-numerical-integration"><span class="header-section-number">1.8</span> Posterior via numerical integration</a></li>
<li><a class="nav-link" href="#superimpose-explicit-posterior"><span class="header-section-number">1.9</span> Superimpose explicit posterior</a></li>
<li><a class="nav-link" href="#and-the-prior"><span class="header-section-number">1.10</span> And the prior</a></li>
<li><a class="nav-link" href="#what-if-multiple-parameters"><span class="header-section-number">1.11</span> What if multiple parameters?</a></li>
<li><a class="nav-link" href="#bayesian-computation"><span class="header-section-number">1.12</span> Bayesian computation</a></li>
<li><a class="nav-link" href="#why-are-mcmc-methods-so-useful"><span class="header-section-number">1.13</span> Why are MCMC methods so useful?</a></li>
<li><a class="nav-link" href="#the-metropolis-algorithm"><span class="header-section-number">1.14</span> The Metropolis algorithm</a></li>
<li><a class="nav-link" href="#metropolis-algorithm"><span class="header-section-number">1.15</span> Metropolis algorithm</a></li>
<li><a class="nav-link" href="#a-chain"><span class="header-section-number">1.16</span> A chain</a></li>
<li><a class="nav-link" href="#another-chain"><span class="header-section-number">1.17</span> Another chain</a></li>
<li>
<a class="nav-link" href="#with-5000-steps"><span class="header-section-number">1.18</span> With 5000 steps</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#animating-mcmc---1d-example-code-here"><span class="header-section-number">1.18.1</span> Animating MCMC - 1D example (code here)</a></li>
<li><a class="nav-link" href="#animating-mcmc---2d-example"><span class="header-section-number">1.18.2</span> Animating MCMC - 2D example</a></li>
<li><a class="nav-link" href="#the-mcmc-interactive-gallery-more-here"><span class="header-section-number">1.18.3</span> The MCMC Interactive Gallery (more here)</a></li>
</ul>
</li>
<li><a class="nav-link" href="#assessing-convergence"><span class="header-section-number">1.19</span> Assessing convergence</a></li>
<li><a class="nav-link" href="#mixing-and-autocorrelation"><span class="header-section-number">1.20</span> Mixing and autocorrelation</a></li>
<li><a class="nav-link" href="#how-do-good-chains-behave"><span class="header-section-number">1.21</span> How do good chains behave?</a></li>
<li><a class="nav-link" href="#burn-in"><span class="header-section-number">1.22</span> Burn-in</a></li>
<li><a class="nav-link" href="#effective-sample-size-n.eff"><span class="header-section-number">1.23</span> Effective sample size n.eff</a></li>
<li><a class="nav-link" href="#potential-scale-reduction-factor"><span class="header-section-number">1.24</span> Potential scale reduction factor</a></li>
<li><a class="nav-link" href="#to-sum-up"><span class="header-section-number">1.25</span> To sum up</a></li>
<li><a class="nav-link" href="#what-if-you-have-issues-of-convergence"><span class="header-section-number">1.26</span> What if you have issues of convergence?</a></li>
<li><a class="nav-link" href="#further-reading"><span class="header-section-number">1.27</span> Further reading</a></li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
<li><a id="book-source" href="https://github.com/oliviergimenez/banana-book/blob/master/01-bayesMCMC.Rmd">View source <i class="fab fa-github"></i></a></li>
          <li><a id="book-edit" href="https://github.com/oliviergimenez/banana-book/edit/master/01-bayesMCMC.Rmd">Edit this page <i class="fab fa-github"></i></a></li>
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>Bayesian Analysis of Capture-Recapture Data with Hidden Markov Models – Theory and Case Studies in R</strong>" was written by Olivier Gimenez. It was last built on 2021-08-27.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script><script type="text/x-mathjax-config">const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
for (let popover of popovers) {
  const div = document.createElement('div');
  div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
  div.innerHTML = popover.getAttribute('data-content');

  var has_math = div.querySelector("span.math");
  if (has_math) {
    document.body.appendChild(div);
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
    MathJax.Hub.Queue(function() {
      popover.setAttribute('data-content', div.innerHTML);
      document.body.removeChild(div);
    })
  }
}
</script>
</body>
</html>
