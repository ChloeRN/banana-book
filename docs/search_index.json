[["index.html", "Bayesian Analysis of Capture-Recapture Data with Hidden Markov Models – Theory and Case Studies in R Welcome", " Bayesian Analysis of Capture-Recapture Data with Hidden Markov Models – Theory and Case Studies in R Olivier Gimenez 2021-08-26 Welcome Welcome to the website of the book Bayesian Analysis of Capture-Recapture Data with Hidden Markov Models – Theory and Case Studies in R by Olivier Gimenez. Note that the book is also available in PDF format. I’m currently writing this book, and I welcome any feedback or requests for content here. Many thanks! Last updated: August 26, 2021 "],["preface.html", "Preface Why read this book Structure of the book Software information and conventions Acknowledgments", " Preface The HMM framework has gained much attention in the ecological literature over the last decade, and has been suggested as a general modelling framework for the demography of plant and animal populations. In particular, HMMs are increasingly used to analyse capture-recapture data and estimate key population parameters (e.g., survival, dispersal, recruitment or abundance) with applications all fields of ecology. In parallel, Bayesian statistics is relatively well established and fast growing in ecology and related disciplines, because it resonates with scientific reasoning and allows accommodating uncertainty smoothly. The popularity of Bayesian statistics also comes from the availability of free pieces of software (WinBUGS, OpenBUGS, JAGS, Stan, nimble) that allow practitioners to code their own analyses. However, to my knowledge, a full Bayesian treatment of HMMs applied to capture-recapture data is yet to be proposed in a book. This is what I propose with this book. Besides, the popular software solutions come with computational limitations when ecologists have to deal with complex models and/or big data. I will use Nimble that is seen by many as the future of ecological data modelling because it extends the BUGS language for writing new functions and distributions, and provides samplers that can deal with discrete latent states in contrast with Stan. In this book, I will cover both the theory of HMMs for capture-recapture data, and the applications of these models to empower practitioners to fit their models with confidence. An important part of the book will consist in case studies presented in a tutorial style to abide by the “learning by doing” philosophy. The online version of this book is licensed under the Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. Why read this book Structure of the book Blabla. Software information and conventions This book uses primarily the R package nimble, so you need to at least install R and the nimble package. The R session information when compiling this book is shown below: sessionInfo() ## R version 4.1.0 (2021-05-18) ## Platform: x86_64-apple-darwin17.0 (64-bit) ## Running under: macOS Catalina 10.15.7 ## ## Matrix products: default ## BLAS: /Library/Frameworks/R.framework/Versions/4.1/Resources/lib/libRblas.dylib ## LAPACK: /Library/Frameworks/R.framework/Versions/4.1/Resources/lib/libRlapack.dylib ## ## locale: ## [1] fr_FR.UTF-8/fr_FR.UTF-8/fr_FR.UTF-8/C/C/fr_FR.UTF-8 ## ## attached base packages: ## [1] stats graphics grDevices utils datasets ## [6] methods base ## ## other attached packages: ## [1] pdftools_3.0.1 magick_2.7.3 MCMCvis_0.15.3 ## [4] nimble_0.11.1 forcats_0.5.1 stringr_1.4.0 ## [7] dplyr_1.0.7 purrr_0.3.4.9000 readr_2.0.0 ## [10] tidyr_1.1.3 tibble_3.1.3 ggplot2_3.3.5 ## [13] tidyverse_1.3.1 ## ## loaded via a namespace (and not attached): ## [1] Rcpp_1.0.7 lubridate_1.7.10 ## [3] lattice_0.20-44 assertthat_0.2.1 ## [5] digest_0.6.27 utf8_1.2.2 ## [7] R6_2.5.0 cellranger_1.1.0 ## [9] backports_1.2.1 reprex_2.0.1 ## [11] evaluate_0.14 coda_0.19-4 ## [13] httr_1.4.2 pillar_1.6.2 ## [15] rlang_0.4.11 readxl_1.3.1 ## [17] rstudioapi_0.13 jquerylib_0.1.4 ## [19] qpdf_1.1 rmarkdown_2.10 ## [21] igraph_1.2.6 munsell_0.5.0 ## [23] broom_0.7.9 compiler_4.1.0 ## [25] modelr_0.1.8 xfun_0.25 ## [27] askpass_1.1 pkgconfig_2.0.3 ## [29] htmltools_0.5.1.1 tidyselect_1.1.1 ## [31] bookdown_0.23 fansi_0.5.0 ## [33] crayon_1.4.1 tzdb_0.1.2 ## [35] dbplyr_2.1.1 withr_2.4.2 ## [37] grid_4.1.0 jsonlite_1.7.2 ## [39] gtable_0.3.0 lifecycle_1.0.0 ## [41] DBI_1.1.1 magrittr_2.0.1 ## [43] scales_1.1.1 cli_3.0.1 ## [45] stringi_1.7.3 fs_1.5.0 ## [47] xml2_1.3.2 bslib_0.2.5.1 ## [49] ellipsis_0.3.2 generics_0.1.0 ## [51] vctrs_0.3.8 tools_4.1.0 ## [53] glue_1.4.2 hms_1.1.0 ## [55] parallel_4.1.0 yaml_2.2.1 ## [57] colorspace_2.0-2 rvest_1.0.1 ## [59] knitr_1.33 haven_2.4.3 ## [61] sass_0.4.0 We do not add prompts (&gt; and +) to R source code in this book, and we comment out the text output with two hashes ## by default, as you can see from the R session information above. This is for your convenience when you want to copy and run the code (the text output will be ignored since it is commented out). Package names are in bold text (e.g., nimble), and inline code and filenames are formatted in a typewriter font (e.g., knitr::knit('foo.Rmd')). Function names are followed by parentheses (e.g., nimble::nimbleCode()). The double-colon operator :: means accessing an object from a package. Acknowledgments CNRS. Jean-Do. Roger. Rémi. My students. Chloé, Sarah, Perry, Daniel. Rob Chapman &amp; Hall/CRC. Workshop attendees. Feedback from. FIP radio. Marc Kéry for his support and advice on how to write a book. Proofreading by. My family. Olivier Gimenez Montpellier, France "],["about-the-author.html", "About the Author", " About the Author Je m’appelle Olivier Gimenez (https://oliviergimenez.github.io/). Je suis directeur de recherche au CNRS. Après des études universitaires en mathématiques, j’ai fait une thèse en statistiques pour l’écologie. J’ai passé mon Habilitation à Diriger des Recherches en écologie et évolution. Récemment, je suis retourné sur les bancs de l’université pour m’initier à la sociologie. J’ai écrit des articles scientifiques faisant appel à la statistique bayésienne, et co-écrit avec des collègues britanniques un livre sur les analyses bayésiennes pour l’écologie des populations. Vous pouvez me retrouver sur Twitter (https://twitter.com/oaggimenez), ou bien me contacter via mon adresse email qui s’écrit olivier suivi d’un point puis gimenez, ensuite arobase, puis cefe, suivi d’un point, puis cnrs, suivi d’un point et pour terminer fr. Tombé dedans quand j’étais petit. Obélix Roger et Astérix JD. "],["crashcourse.html", "Chapter 1 Bayesian statistics &amp; MCMC 1.1 Bayes’ theorem 1.2 Frequentist versus Bayesian 1.3 What is the Bayesian approach? 1.4 Bayes’ theorem 1.5 Brute force via numerical integration 1.6 Beta prior 1.7 Apply Bayes theorem 1.8 Posterior via numerical integration 1.9 Superimpose explicit posterior 1.10 And the prior 1.11 What if multiple parameters? 1.12 Bayesian computation 1.13 Why are MCMC methods so useful? 1.14 The Metropolis algorithm 1.15 Metropolis algorithm 1.16 A chain 1.17 Another chain 1.18 With 5000 steps 1.19 Assessing convergence 1.20 Mixing and autocorrelation 1.21 How do good chains behave? 1.22 Burn-in 1.23 Effective sample size n.eff 1.24 Potential scale reduction factor 1.25 To sum up 1.26 What if you have issues of convergence? 1.27 Further reading", " Chapter 1 Bayesian statistics &amp; MCMC 1.1 Bayes’ theorem A theorem about conditional probabilities. \\(\\Pr(B \\mid A) = \\displaystyle{\\frac{ \\Pr(A \\mid B) \\; \\Pr(B)}{\\Pr(A)}}\\) Figure 1.1: Bayes’ theorem spelt out in blue neon at the offices of Autonomy in Cambridge. Source: Wikipedia I always forget what the letters mean. Might be easier to remember when written like this: \\[ \\Pr(\\text{hypothesis} \\mid \\text{data}) = \\frac{ \\Pr(\\text{data} \\mid \\text{hypothesis}) \\; \\Pr(\\text{hypothesis})}{\\Pr(\\text{data})} \\] The “hypothesis” is typically something unobserved or unknown. It’s what you want to learn about using the data. For regression models, the “hypothesis” is a parameter (intercept, slopes or error terms). Bayes theorem tells you the probability of the hypothesis given the data. Cool because what is doing science after all? How plausible is some hypothesis given the data? \\[ \\Pr(\\text{hypothesis} \\mid \\text{data}) = \\frac{ \\Pr(\\text{data} \\mid \\text{hypothesis}) \\; \\Pr(\\text{hypothesis})}{\\Pr(\\text{data})} \\] The Bayesian reasoning echoes the scientific reasoning. You might ask then, why is Bayesian statistics not the default? You may ask: Why is Bayesian statistics not the default? Due to practical problems of implementing the Bayesian approach, and futile wars between (male) statisticians, little progress was made for over two centuries. Recent advances in computational power coupled with the development of new methodology have led to a great increase in the application of Bayesian methods within the last two decades. 1.2 Frequentist versus Bayesian Typical stats problems involve estimating parameter \\(\\theta\\) with available data. The frequentist approach (maximum likelihood estimation – MLE) assumes that the parameters are fixed, but have unknown values to be estimated. Classical estimates are generally point estimates of the parameters of interest. The Bayesian approach assumes that the parameters are not fixed but have some fixed unknown distribution - a distribution for the parameter. 1.3 What is the Bayesian approach? The approach is based upon the idea that the experimenter begins with some prior beliefs about the system. You never start from scratch. And then updates these beliefs on the basis of observed data. This updating procedure is based upon the Bayes’ Theorem: \\[\\Pr(A \\mid B) = \\frac{\\Pr(B \\mid A) \\; \\Pr(A)}{\\Pr(B)}\\] Schematically if \\(A = \\theta\\) and \\(B = \\text{data}\\), then The Bayes’ theorem \\[\\Pr(A \\mid B) = \\frac{\\Pr(B \\mid A) \\; \\Pr(A)}{\\Pr(B)}\\] Translates into: \\[\\Pr(\\theta \\mid \\text{data}) = \\frac{\\Pr(\\text{data} \\mid \\theta) \\; \\Pr(\\theta)}{\\Pr(\\text{data})}\\] 1.4 Bayes’ theorem \\[{\\color{red}{\\Pr(\\theta \\mid \\text{data})}} = \\frac{\\color{blue}{\\Pr(\\text{data} \\mid \\theta)} \\; \\color{green}{\\Pr(\\theta)}}{\\color{orange}{\\Pr(\\text{data})}}\\] \\(\\color{red}{\\text{Posterior distribution}}\\): Represents what you know after having seen the data. The basis for inference, a distribution, possibly multivariate if more than one parameter. \\(\\color{blue}{\\text{Likelihood}}\\): This quantity is the same as in the MLE approach. \\(\\color{green}{\\text{Prior distribution}}\\): Represents what you know before seeing the data. The source of much discussion about the Bayesian approach. \\(\\color{orange}{\\Pr(\\text{data}) = \\int{L(\\text{data} \\mid \\theta)\\Pr(\\theta) d\\theta}}\\) is a \\(N\\)-dimensional integral if \\(\\theta = \\theta_1, \\ldots, \\theta_N\\). Difficult if not impossible to calculate. This is one of the reasons why we need simulation (MCMC) methods. 1.5 Brute force via numerical integration Say we release \\(n\\) animals at the beginning of the winter, out of which \\(y\\) survive, and we’d like to estimate winter survival \\(\\theta\\). y &lt;- 19 # nb of success n &lt;- 57 # nb of attempts Our model: \\[\\begin{align*} y &amp;\\sim \\text{Binomial}(n, \\theta) &amp;\\text{[likelihood]} \\\\ \\theta &amp;\\sim \\text{Beta}(1, 1) &amp;\\text{[prior for }\\theta \\text{]} \\\\ \\end{align*}\\] 1.6 Beta prior 1.7 Apply Bayes theorem Likelihood times the prior: \\(\\Pr(\\text{data} \\mid \\theta) \\; \\Pr(\\theta)\\) numerator &lt;- function(p) dbinom(y,n,p) * dbeta(p,a,b) Averaged likelihood: \\(\\Pr(\\text{data}) = \\int{L(\\theta \\mid \\text{data}) \\; \\Pr(\\theta) d\\theta}\\) denominator &lt;- integrate(numerator,0,1)$value 1.8 Posterior via numerical integration 1.9 Superimpose explicit posterior 1.10 And the prior 1.11 What if multiple parameters? Example of a linear regression with parameters \\(\\alpha\\), \\(\\beta\\) and \\(\\sigma\\) to be estimated. Bayes’ theorem says: \\[ P(\\alpha, \\beta, \\sigma \\mid \\text{data}) = \\frac{ P(\\text{data} \\mid \\alpha, \\beta, \\sigma) \\, P(\\alpha, \\beta, \\sigma)}{\\iiint \\, P(\\text{data} \\mid \\alpha, \\beta, \\sigma) \\, P(\\alpha, \\beta, \\sigma) \\,d\\alpha \\,d\\beta \\,d\\sigma} \\] Do we really wish to calculate a 3D integral? 1.12 Bayesian computation In the early 1990s, statisticians rediscovered work from the 1950’s in physics. knitr::include_graphics(&quot;images/metropolis.png&quot;) Use stochastic simulation to draw samples from posterior distributions. Avoid explicit calculation of integrals in Bayes formula. Instead, approx. posterior w/ some precision by drawing large samples. Markov chain Monte Carlo (MCMC) gives a boost to Bayesian statistics! 1.13 Why are MCMC methods so useful? MCMC are stochastic algorithms to produce sequence of dependent random numbers from a Markov chain. A Markov chain is a discrete sequence of states, in which the probability of an event depends only on the state in the previous event. A Markov chain has an equilibrium (aka stationary) distribution. Equilibrium distribution is the desired posterior distribution! Several ways of constructing these chains: e.g., Metropolis-Hastings, Gibbs sampler. How to implement them in practice?! 1.14 The Metropolis algorithm Let’s go back to animal survival estimation. We illustrate sampling from the posterior distribution. We write functions in R for the likelihood, the prior and the posterior. # survival data, 19 &quot;success&quot; out of 57 &quot;attempts&quot; survived &lt;- 19 released &lt;- 57 # log-likelihood function loglikelihood &lt;- function(x, p){ dbinom(x = x, size = released, prob = p, log = TRUE) } # prior density logprior &lt;- function(p){ dunif(x = p, min = 0, max = 1, log = TRUE) } # posterior density function (log scale) posterior &lt;- function(x, p){ loglikelihood(x, p) + logprior(p) # - log(Pr(data)) } 1.15 Metropolis algorithm We start at any possible value of the parameter to be estimated. To decide where to visit next, we propose to move away from the current value of the parameter — this is a candidate value. To do so, we add to the current value some random value from say a normal distribution with some variance. We compute the ratio of the probabilities at the candidate and current locations \\(R = \\text{posterior(candidate)/posterior(current)}\\). This is where the magic of MCMC happens, in that \\(\\Pr(\\text{data})\\), the denominator of the Bayes theorem, cancels out. We spin a continuous spinner that lands anywhere from 0 to 1 — call it the random spin \\(X\\). If \\(X\\) is smaller than \\(R\\), we move to the candidate location, otherwise we remain at the current location. We repeat 2-4 a number of times — or steps (many steps). # propose candidate value move &lt;- function(x, away = .2){ logitx &lt;- log(x / (1 - x)) logit_candidate &lt;- logitx + rnorm(1, 0, away) candidate &lt;- plogis(logit_candidate) return(candidate) } # set up the scene steps &lt;- 100 theta.post &lt;- rep(NA, steps) set.seed(1234) # pick starting value (step 1) inits &lt;- 0.5 theta.post[1] &lt;- inits for (t in 2:steps){ # repeat steps 2-4 (step 5) # propose candidate value for prob of success (step 2) theta_star &lt;- move(theta.post[t-1]) # calculate ratio R (step 3) pstar &lt;- posterior(survived, p = theta_star) pprev &lt;- posterior(survived, p = theta.post[t-1]) logR &lt;- pstar - pprev R &lt;- exp(logR) # decide to accept candidate value or to keep current value (step 4) accept &lt;- rbinom(1, 1, prob = min(R, 1)) theta.post[t] &lt;- ifelse(accept == 1, theta_star, theta.post[t-1]) } Starting at the value \\(0.5\\) and running the algorithm for \\(100\\) iterations. head(theta.post) ## [1] 0.5000 0.4399 0.4399 0.4577 0.4577 0.4577 tail(theta.post) ## [1] 0.4146 0.3772 0.3772 0.3861 0.3899 0.3624 1.16 A chain 1.17 Another chain 1.18 With 5000 steps In yellow: posterior mean; in red: maximum likelihood estimate. 1.18.1 Animating MCMC - 1D example (code here) knitr::include_graphics(&quot;images/112546886-56862f00-8dba-11eb-81a0-465434672bdd.gif&quot;) 1.18.2 Animating MCMC - 2D example Code here. knitr::include_graphics(&quot;images/create-gif.gif&quot;) 1.18.3 The MCMC Interactive Gallery (more here) knitr::include_graphics(&quot;images/galery.png&quot;) 1.19 Assessing convergence MCMC algorithms can be used to construct a Markov chain with a given stationary distribution (set to be the posterior distribution). For the MCMC algorithm, the posterior distribution is only needed to be known up to proportionality. Once the stationary distribution is reached, we can regard the realisations of the chain as a (dependent) sample from the posterior distribution (and obtain Monte Carlo estimates). We consider some important implementation issues. 1.20 Mixing and autocorrelation The movement around the parameter space is often referred to as mixing. Traceplots of for small and big moves provide (relatively) high correlations (known as autocorrelations) between successive observations of the Markov chain. Strongly correlated observations require large sample sizes and therefore longer simulations. Autocorrelation function (ACF) plots are a convenient way of displaying the strength of autocorrelation in the given sample values. ACF plots provide the autocorrelation between successively sampled values separated by \\(k\\) iterations, referred to as lag, (i.e. \\(\\text{cor}(\\theta_t, \\theta_{t+k})\\)) for increasing values of \\(k\\). 1.21 How do good chains behave? Converge to same target distribution; discard some realisations of Markov chain before convergence is achieved. Once there, explore efficiently: The post-convergence sample size required for suitable numerical summaries. Therefore, we are looking to determine how long it takes for the Markov chain to converge to the stationary distribution. In practice, we must discard observations from the start of the chain and just use observations from the chain once it has converged. The initial observations that we discard are referred to as the burn-in. Simplest method to determine length of burn-in period is to look at trace plots. 1.22 Burn-in If simulations cheap, be conservative. 1.23 Effective sample size n.eff How long of a chain is needed to produce stable estimates ? Most MCMC chains are strongly autocorrelated. Successive steps are near each other, and are not independent. The effective sample size (n.eff) measures chain length while taking into account the autocorrelation of the chain. n.eff is less than the number of MCMC iterations. Check the n.eff of every parameter of interest. Check the n.eff of any interesting parameter combinations. We need \\(\\text{n.eff} \\geq 100\\) independent steps. 1.24 Potential scale reduction factor Gelman-Rubin statistic \\(\\hat{R}\\) Measures the ratio of the total variability combining multiple chains (between-chain plus within-chain) to the within-chain variability. Asks the question is there a chain effect? Very much alike the \\(F\\) test in an ANOVA. Values near \\(1\\) indicates likely convergence, a value of \\(\\leq 1.1\\) is considered acceptable. Necessary condition, not sufficient; In other words, these diagnostics cannot tell you that you have converged for sure, only that you have not. 1.25 To sum up Run multiple chains from arbitrary starting places (initial values). Assume convergence when all chains reach same regime Discard initial burn-in phase. Proceed with posterior inference. Use traceplot, effective sample size and \\(\\hat{R}\\). 1.26 What if you have issues of convergence? Increase burn-in, sample more. Use more informative priors. Pick better initial values (good guess), using e.g. estimates from simpler models. Reparameterize: Standardize covariates. Non-centering: \\(\\alpha \\sim N(0,\\sigma)\\) becomes \\(\\alpha = z \\sigma\\) with \\(z \\sim N(0,1)\\). Something wrong with your model? Start with a simpler model (remove complexities). Use simulations. Change your sampler. More later on. 1.27 Further reading McCarthy, M. (2007). Bayesian Methods for Ecology. Cambridge: Cambridge University Press. McElreath, R. (2020). Statistical Rethinking: A Bayesian Course with Examples in R and Stan (2nd ed.). CRC Press. Gelman, A. and Hill, J. (2006). Data Analysis Using Regression and Multilevel/Hierarchical Models (Analytical Methods for Social Research). Cambridge: Cambridge University Press. "],["intronimble.html", "Chapter 2 Introduction to Nimble 2.1 What is Nimble? 2.2 Load nimble package 2.3 Build model, made of likelihood and priors 2.4 Syntax: what’s new/better/different? 2.5 Read in data 2.6 Distinguish constants and data 2.7 Specify initial values 2.8 Which parameters to save? 2.9 MCMC details 2.10 Run model, tadaa! 2.11 Explore MCMC outputs 2.12 Numerical summaries 2.13 Trace and posterior density 2.14 Our nimble workflow so far 2.15 But nimble gives full access to the MCMC engine 2.16 Useful resources", " Chapter 2 Introduction to Nimble 2.1 What is Nimble? Figure 2.1: (Meme created by Todd Arnold’s wonderful students) Figure 2.2: (Meme created by Todd Arnold’s wonderful students) Numerical Inference for statistical Models using Bayesian and Likelihood Estimation. A framework for hierarchical statistical models and algorithms. Uses almost the same model code as WinBUGS, OpenBUGS, and JAGS. An extension of the BUGS language: additional syntax, custom functions and distributions. A configurable system for MCMC. A library of other methods (SMC, MCEM). Sequential Monte Carlo (particle filtering) Monte Carlo Expectation Maximization (maximum likelihood) A model-generic programming system to write new analysis methods. 2.2 Load nimble package library(nimble) 2.3 Build model, made of likelihood and priors naive.survival.model &lt;- nimbleCode({ # prior phi ~ dunif(0, 1) # likelihood y ~ dbinom(phi, n) }) 2.4 Syntax: what’s new/better/different? Vectorization # JAGS (&amp; Nimble) for(t in 1:Tmax){ x[t] &lt;- Mu.x + epsilon[t] } # Nimble x[1:Tmax] &lt;- Mu.x + epsilon[1:Tmax] More flexible specification of distributions # JAGS (&amp; Nimble) for(t in 1:Tmax){ epsilon[t] ~ dnorm(0, tau) } tau &lt;- pow(sigma, -2) sigma ~ dunif(0, 5) # Nimble for(t in 1:Tmax){ epsilon[t] ~ dnorm(0, sd = sigma) } sigma ~ dunif(0, 5) Your own functions and distributions x[1:Tmax] &lt;- myNimbleFunction(a = Mu.x, b = epsilon[1:Tmax]) sigma ~ dCustomDistr(c = 0.5, z = 10) The end of empty indices # JAGS sum.x &lt;- sum(x[]) # Nimble sum.x &lt;- sum(x[1:Tmax]) &amp; more… 2.5 Read in data Back to our naive survival model: naive.survival.model &lt;- nimbleCode({ # prior phi ~ dunif(0, 1) # likelihood y ~ dbinom(phi, n) }) my.data &lt;- list(n = 57, y = 19) 2.6 Distinguish constants and data To Nimble, not all “data” is data… my.constants &lt;- list(n = 57) my.data &lt;- list(y = 19) Constants: + Can never be changed + Must be provided when a model is defined (part of the model structure) + E.g. vector of known index values, variables used to define for-loops, etc. To Nimble, not all “data” is data… my.constants &lt;- list(n = 57) my.data &lt;- list(y = 19) Data: + Can be changed without re-building the model + Can be (re-)simulated within a model + E.g. stuff that only appears to the left of a “~” For computational efficiency, better to specify as much as possible as constants. Nimble will help you with this! 2.7 Specify initial values initial.values &lt;- function() list(phi = runif(1,0,1)) initial.values() ## $phi ## [1] 0.5332 2.8 Which parameters to save? parameters.to.save &lt;- c(&quot;phi&quot;) 2.9 MCMC details n.iter &lt;- 5000 n.burnin &lt;- 1000 n.chains &lt;- 2 n.thin &lt;- 1 Number of posterior samples per chain: \\[n.posterior = \\frac{n.iter - n.burnin}{n.thin}\\] 2.10 Run model, tadaa! mcmc.output &lt;- nimbleMCMC(code = naive.survival.model, data = my.data, constants = my.constants, inits = initial.values, monitors = parameters.to.save, thin = n.thin, niter = n.iter, nburnin = n.burnin, nchains = n.chains) 2.11 Explore MCMC outputs str(mcmc.output) ## List of 2 ## $ chain1: num [1:4000, 1] 0.305 0.305 0.392 0.422 0.422 ... ## ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. ..$ : NULL ## .. ..$ : chr &quot;phi&quot; ## $ chain2: num [1:4000, 1] 0.352 0.416 0.416 0.416 0.416 ... ## ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. ..$ : NULL ## .. ..$ : chr &quot;phi&quot; head(mcmc.output$chain1) ## phi ## [1,] 0.3046 ## [2,] 0.3046 ## [3,] 0.3916 ## [4,] 0.4221 ## [5,] 0.4221 ## [6,] 0.4449 2.12 Numerical summaries library(MCMCvis) MCMCsummary(mcmc.output, round = 2) ## mean sd 2.5% 50% 97.5% Rhat n.eff ## phi 0.34 0.06 0.23 0.34 0.46 1 1931 2.13 Trace and posterior density MCMCtrace(mcmc.output, pdf = FALSE) MCMCtrace(mcmc.output, pdf = FALSE, ind = TRUE, Rhat = TRUE, n.eff = TRUE) 2.14 Our nimble workflow so far knitr::include_graphics(&quot;images/nimble_workflow_sofar.png&quot;) 2.15 But nimble gives full access to the MCMC engine knitr::include_graphics(&quot;images/nimble_workflow.png&quot;) knitr::include_graphics(&quot;images/I1bIY06.gif&quot;) 2.16 Useful resources Official website https://r-nimble.org User Manual https://r-nimble.org/html_manual/cha-welcome-nimble.html and cheatsheet. Users mailing list https://groups.google.com/forum/#!forum/nimble-users Training material https://github.com/nimble-training Reference to cite when using nimble in a publication: de Valpine, P., D. Turek, C. J. Paciorek, C. Anderson-Bergman, D. Temple Lang, and R. Bodik (2017). Programming With Models: Writing Statistical Algorithms for General Model Structures With NIMBLE. Journal of Computational and Graphical Statistics 26 (2): 403–13. "],["hmmcapturerecapture.html", "Chapter 3 Hidden Markov models 3.1 Back to our survival example 3.2 Longitudinal data 3.3 A model for longitudinal survival data 3.4 Markov process 3.5 Transition matrix 3.6 Initial states 3.7 Likelihood 3.8 Example 3.9 Our model 3.10 Our model 3.11 Nimble implementation 3.12 Nimble code 3.13 Note 3.14 Nimble awesomness 3.15 Converting to Nimble from Jags, OpenBUGS or WinBUGS 3.16 Constants and data 3.17 Initial values 3.18 Parameters to monitor 3.19 MCMC details 3.20 Run Nimble 3.21 Posterior distribution of survival 3.22 Unfortunately, this is the data we wish we had. 3.23 In real life 3.24 The truth is in \\(z\\) 3.25 Dead animals go undetected 3.26 Alive animals may be detected or not 3.27 Observation matrix 3.28 Markov model 3.29 Hidden Markov model 3.30 Hidden Markov model for survival 3.31 HMM likelihood 3.32 Example 3.33 Example 3.34 Estimating the latent states \\(z\\) or not? 3.35 Our model 3.36 Nimble implementation 3.37 Priors 3.38 HMM ingredients 3.39 Likelihood 3.40 Constants 3.41 Data 3.42 Initial values 3.43 Parameters to monitor 3.44 MCMC details 3.45 Run Nimble 3.46 Posterior distribution of survival 3.47 Further reading", " Chapter 3 Hidden Markov models 3.1 Back to our survival example We have \\(z\\) survivors out of \\(n\\) released animals with winter survival probability \\(\\phi\\) Let’s get back to our survival example. Our model so far: \\[\\begin{align*} z &amp;\\sim \\text{Binomial}(n, \\phi) &amp;\\text{[likelihood]} \\\\ \\phi &amp;\\sim \\text{Beta}(1, 1) &amp;\\text{[prior for }\\phi \\text{]} \\\\ \\end{align*}\\] Our model so far has been a combination Of a binomial likelihood And a Beta prior with param 1 and 1, which is a uniform between 0 and 1. This is also: \\[\\begin{align*} z_i &amp;\\sim \\text{Bernoulli}(\\phi), \\; i = 1, \\ldots, N &amp;\\text{[likelihood]} \\\\ \\phi &amp;\\sim \\text{Beta}(1, 1) &amp;\\text{[prior for }\\phi \\text{]} \\\\ \\end{align*}\\] The binomial is just a sum of Bernoulli outcomes Like flipping a coin for each individual and get a survivor with prob phi. What if we had several winters? Say \\(T = 5\\) winters. In this design, we have a single winter. But for many species, we’ll need to collect data on the long term to get a representative estimate of survival. Therefore what if we had say big T five winters? 3.2 Longitudinal data \\(z_{i,t} = 1\\) if individual \\(i\\) alive at winter \\(t\\), and \\(z_{i,t} = 2\\) if dead. id winter 1 winter 2 winter 3 winter 4 winter 5 1 1 2 2 2 2 2 1 1 1 1 1 3 1 2 2 2 2 4 1 1 1 1 1 5 1 2 2 2 2 6 1 1 1 1 1 7 1 1 1 1 2 8 1 1 1 1 1 9 1 1 1 1 1 10 1 1 2 2 2 11 1 1 1 1 1 12 1 1 1 1 1 13 1 1 2 2 2 14 1 1 1 1 1 15 1 1 1 1 1 16 1 1 1 1 1 17 1 1 2 2 2 18 1 1 1 1 1 19 1 1 1 2 2 20 1 1 1 1 1 21 1 2 2 2 2 22 1 1 2 2 2 23 1 1 2 2 2 24 1 1 1 1 2 25 1 1 1 1 1 26 1 1 1 1 2 27 1 1 1 1 1 28 1 2 2 2 2 29 1 1 2 2 2 30 1 1 1 1 1 31 1 1 2 2 2 32 1 1 1 2 2 33 1 1 1 1 1 34 1 1 1 2 2 35 1 1 1 2 2 36 1 1 2 2 2 37 1 2 2 2 2 38 1 1 2 2 2 39 1 1 1 1 1 40 1 1 2 2 2 41 1 2 2 2 2 42 1 1 1 1 2 43 1 1 2 2 2 44 1 1 1 1 1 45 1 1 1 2 2 46 1 2 2 2 2 47 1 2 2 2 2 48 1 1 2 2 2 49 1 1 1 1 1 50 1 1 1 2 2 51 1 1 1 1 1 52 1 1 1 1 1 53 1 1 1 1 1 54 1 1 1 1 2 55 1 1 1 1 1 56 1 1 1 1 1 57 1 1 1 1 1 This is what we call longitudinal data. Each row is an individual i, and columns are for winters t, or sampling occasions. z is indexed by both i and t, and takes value 1 if ind i is alive in winter t, and 2 otherwise. 3.3 A model for longitudinal survival data A model relies on assumptions. Let’s think of a model for these data. The objective remains the same, estimating survival. To build this model, we’ll make assumptions. The state of an animal at a given winter, alive or dead, is only dependent on its state the winter before. First, we assume that the state of an animal in a given winter, alive or dead, is only dependent on its state the winter before. The future depends only on the present, not the past: Markov process. In others words, he future depends only on the present, not the past This is a Markov process. If an animal is alive in a given winter, the probability it survives to the next winter is \\(\\phi\\). If an animal is alive in a given winter, the probability it survives to the next winter is \\(\\phi\\). The probability it dies is \\(1 - \\phi\\). The probability it dies is \\(1 - \\phi\\). If an animal is dead a winter, it remains dead, unless you believe in zombies. If an animal is dead a winter, it remains dead, unless you believe in zombies. 3.4 Markov process A markov process can be represented this way. The state at t+1 only depends on the state at t. In our model, going from a winter to the next is driven by survival and mortality processes. The probability of going from alive or 1 to alive or 1 is phi. Then from alive 1 to dead 2 is 1 - phi. And the probability to remain dead is 1, that is to go from state 2 dead to state 2 for dead. 3.5 Transition matrix The core of the Markov process is made of the transition probabilities. The engine of a Markov model is the transition matrix. This matrix or table gathers the probabilities of transition between states from one occasion to the next. For example, the probability of transitioning from state alive at \\(t-1\\) to state alive at \\(t\\) is \\(\\Pr(z_t = 1 | z_{t-1} = 1) = \\gamma_{1,1}\\). It is the survival probability \\(\\phi\\). For example, the probability of transitioning from state alive at \\(t-1\\) to state alive at \\(t\\) is \\(\\Pr(z_t = 1 | z_{t-1} = 1) = \\gamma_{1,1}\\). It is the survival probability \\(\\phi\\). The probability of dying over the interval \\((t-1, t)\\) is \\(\\Pr(z_t = 2 | z_{t-1} = 1) = \\gamma_{1,2} = 1 - \\phi\\). The probability of dying over the interval \\((t-1, t)\\) is \\(\\Pr(z_t = 2 | z_{t-1} = 1) = \\gamma_{1,2} = 1 - \\phi\\). Now if an animal is dead at \\(t-1\\), then \\(\\Pr(z_t = 1 | z_{t-1} = 2) = 0\\) and \\(\\Pr(z_t = 2 | z_{t-1} = 2) = 1\\). Now if an animal is dead at \\(t-1\\), then \\(\\Pr(z_t = 1 | z_{t-1} = 2) = 0\\) and \\(\\Pr(z_t = 2 | z_{t-1} = 2) = 1\\). These probabilities can be packed in a transition matrix \\(\\mathbf{\\Gamma}\\): \\[\\begin{align*} \\mathbf{\\Gamma} = \\left(\\begin{array}{cc} \\gamma_{1,1} &amp; \\gamma_{1,2}\\\\ \\gamma_{2,1} &amp; \\gamma_{2,2} \\end{array}\\right) = \\left(\\begin{array}{cc} \\phi &amp; 1 - \\phi\\\\ 0 &amp; 1 \\end{array}\\right) \\end{align*}\\] These probabilities can be packed in a transition matrix \\(\\mathbf{\\Gamma}\\): Transition matrix: \\[ \\begin{matrix} &amp; \\\\ \\mathbf{\\Gamma} = \\left ( \\vphantom{ \\begin{matrix} 12 \\\\ 12 \\end{matrix} } \\right . \\end{matrix} \\hspace{-1.2em} \\begin{matrix} z_t=A &amp; z_t=D \\\\ \\hdashline \\phi &amp; 1-\\phi \\\\ 0 &amp; 1 \\end{matrix} \\hspace{-0.2em} \\begin{matrix} &amp; \\\\ \\left . \\vphantom{ \\begin{matrix} 12 \\\\ 12 \\end{matrix} } \\right ) \\begin{matrix} z_{t-1}=A \\\\ z_{t-1}=D \\end{matrix} \\end{matrix} \\] + Take some time to navigate through this matrix. + From in rows, the origin, to in columns, the destination. + For example… 3.6 Initial states A Markov process has to start somewhere. We need the probabilities of initial states, i.e. states at \\(t = 1\\). In other words, we need the probabilities of initial states i.e. states at \\(t = 1\\). We will use \\(\\mathbf{\\delta} = \\left(\\Pr(z_1 = 1), \\Pr(z_1 = 2)\\right)\\). We will denote delta this vector. It gathers the probability of being in each initial states. Here alive 1 and dead 2. Here we assume that all animals are alive at first winter, i.e. \\(\\Pr(z_1 = 1) = 1\\) and \\(\\Pr(z_1 = 2) = 0\\). All individuals are marked and release in first winter. Therefore alive when first captured. Which means that they are all in state 1 alive for sure. 3.7 Likelihood \\[\\begin{align*} \\Pr(\\mathbf{z}) &amp;= \\Pr(z_T, z_{T-1}, z_{T-2}, \\ldots, z_1) \\color{white}{\\Pr(z_{T-1}, z_{T-2},\\ldots, z_1) \\Pr(z_{T-2}, \\ldots, z_1)} \\\\ \\end{align*}\\] OK now that we’ve defined a Markov model, we need its likelihood to apply the Bayes theorem. The likelihood is the probability of the data, given the model. Here the data are the z. \\[\\begin{align*} \\Pr(\\mathbf{z}) &amp;= \\Pr(z_T, z_{T-1}, z_{T-2}, \\ldots, z_1) \\color{white}{\\Pr(z_{T-1}, z_{T-2},\\ldots, z_1) \\Pr(z_{T-2}, \\ldots, z_1)}\\\\ &amp;= \\Pr(z_T | z_{T-1}, z_{T-2},\\ldots, z_1) \\Pr(z_{T-1}, z_{T-2},\\ldots, z_1) \\\\ \\end{align*}\\] We’re gonna work backward, starting from the last sampling occasion. Now the likelihood can be written as the product of the probability of zT ie you’re alive or not on the last occasion given your past history, that is the states at previous occasions, times the prob of your past history, y definition of cond prob. \\[\\begin{align*} \\Pr(\\mathbf{z}) &amp;= \\Pr(z_T, z_{T-1}, z_{T-2}, \\ldots, z_1) \\color{white}{\\Pr(z_{T-1}, z_{T-2},\\ldots, z_1) \\Pr(z_{T-2}, \\ldots, z_1)}\\\\ &amp;= \\Pr(z_T | z_{T-1}, z_{T-2},\\ldots, z_1) \\Pr(z_{T-1}, z_{T-2},\\ldots, z_1) \\\\ &amp;= \\Pr(z_T | z_{T-1}) \\Pr(z_{T-1}, z_{T-2},\\ldots, z_1) \\\\ \\end{align*}\\] Then because we have a Markov model, we’re memory less, that is prob of next state, here zT, depends only on the current state, that is zT-1, and not the previous states. \\[\\begin{align*} \\Pr(\\mathbf{z}) &amp;= \\Pr(z_T, z_{T-1}, z_{T-2}, \\ldots, z_1) \\color{white}{\\Pr(z_{T-1}, z_{T-2},\\ldots, z_1) \\Pr(z_{T-2}, \\ldots, z_1)}\\\\ &amp;= \\Pr(z_T | z_{T-1}, z_{T-2},\\ldots, z_1) \\Pr(z_{T-1}, z_{T-2},\\ldots, z_1) \\\\ &amp;= \\Pr(z_T | z_{T-1}) \\Pr(z_{T-1}, z_{T-2},\\ldots, z_1) \\\\ &amp;= \\Pr(z_T | z_{T-1}) \\Pr(z_{T-1} | z_{T-2}, \\ldots, z_1) \\Pr(z_{T-2}, \\ldots, z_1)\\\\ \\end{align*}\\] You can apply the same reasoning to T-1. First conditional prob. \\[\\begin{align*} \\Pr(\\mathbf{z}) &amp;= \\Pr(z_T, z_{T-1}, z_{T-2}, \\ldots, z_1) \\color{white}{\\Pr(z_{T-1}, z_{T-2},\\ldots, z_1) \\Pr(z_{T-2}, \\ldots, z_1)}\\\\ &amp;= \\Pr(z_T | z_{T-1}, z_{T-2},\\ldots, z_1) \\Pr(z_{T-1}, z_{T-2},\\ldots, z_1) \\\\ &amp;= \\Pr(z_T | z_{T-1}) \\Pr(z_{T-1}, z_{T-2},\\ldots, z_1) \\\\ &amp;= \\Pr(z_T | z_{T-1}) \\Pr(z_{T-1} | z_{T-2}, \\ldots, z_1) \\Pr(z_{T-2}, \\ldots, z_1)\\\\ &amp;= \\Pr(z_T | z_{T-1}) \\Pr(z_{T-1} | z_{T-2}) \\Pr(z_{T-2}, \\ldots, z_1)\\\\ \\end{align*}\\] Then markovian property. \\[\\begin{align*} \\Pr(\\mathbf{z}) &amp;= \\Pr(z_T, z_{T-1}, z_{T-2}, \\ldots, z_1) \\color{white}{\\Pr(z_{T-1}, z_{T-2},\\ldots, z_1) \\Pr(z_{T-2}, \\ldots, z_1)}\\\\ &amp;= \\Pr(z_T | z_{T-1}, z_{T-2},\\ldots, z_1) \\Pr(z_{T-1}, z_{T-2},\\ldots, z_1) \\\\ &amp;= \\Pr(z_T | z_{T-1}) \\Pr(z_{T-1}, z_{T-2},\\ldots, z_1) \\\\ &amp;= \\Pr(z_T | z_{T-1}) \\Pr(z_{T-1} | z_{T-2}, \\ldots, z_1) \\Pr(z_{T-2}, \\ldots, z_1)\\\\ &amp;= \\Pr(z_T | z_{T-1}) \\Pr(z_{T-1} | z_{T-2}) \\Pr(z_{T-2}, \\ldots, z_1)\\\\ &amp;= \\ldots \\\\ \\end{align*}\\] And so on. \\[\\begin{align*} \\Pr(\\mathbf{z}) &amp;= \\Pr(z_T, z_{T-1}, z_{T-2}, \\ldots, z_1) \\color{white}{\\Pr(z_{T-1}, z_{T-2},\\ldots, z_1) \\Pr(z_{T-2}, \\ldots, z_1)}\\\\ &amp;= \\Pr(z_T | z_{T-1}, z_{T-2},\\ldots, z_1) \\Pr(z_{T-1}, z_{T-2},\\ldots, z_1) \\\\ &amp;= \\Pr(z_T | z_{T-1}) \\Pr(z_{T-1}, z_{T-2},\\ldots, z_1) \\\\ &amp;= \\Pr(z_T | z_{T-1}) \\Pr(z_{T-1} | z_{T-2}, \\ldots, z_1) \\Pr(z_{T-2}, \\ldots, z_1)\\\\ &amp;= \\Pr(z_T | z_{T-1}) \\Pr(z_{T-1} | z_{T-2}) \\Pr(z_{T-2}, \\ldots, z_1)\\\\ &amp;= \\ldots \\\\ &amp;= \\Pr(z_T | z_{T-1}) \\Pr(z_{T-1} | z_{T-2}) \\ldots \\Pr(z_{2} | z_{1}) \\Pr(z_{1})\\\\ \\end{align*}\\] You end up with this expression for the likelihood. \\[\\begin{align*} \\Pr(\\mathbf{z}) &amp;= \\Pr(z_T, z_{T-1}, z_{T-2}, \\ldots, z_1) \\color{white}{\\Pr(z_{T-1}, z_{T-2},\\ldots, z_1) \\Pr(z_{T-2}, \\ldots, z_1)}\\\\ &amp;= \\Pr(z_T | z_{T-1}, z_{T-2},\\ldots, z_1) \\Pr(z_{T-1}, z_{T-2},\\ldots, z_1) \\\\ &amp;= \\Pr(z_T | z_{T-1}) \\Pr(z_{T-1}, z_{T-2},\\ldots, z_1) \\\\ &amp;= \\Pr(z_T | z_{T-1}) \\Pr(z_{T-1} | z_{T-2}, \\ldots, z_1) \\Pr(z_{T-2}, \\ldots, z_1)\\\\ &amp;= \\Pr(z_T | z_{T-1}) \\Pr(z_{T-1} | z_{T-2}) \\Pr(z_{T-2}, \\ldots, z_1)\\\\ &amp;= \\ldots \\\\ &amp;= \\Pr(z_T | z_{T-1}) \\Pr(z_{T-1} | z_{T-2}) \\ldots \\Pr(z_{2} | z_{1}) \\Pr(z_{1})\\\\ &amp;= \\Pr(z_{1}) \\prod_{t=2}^T{\\Pr(z_{t} | z_{t-1})}\\\\ \\end{align*}\\] A product of cond probabilities. And the prob of initial states Pr(z1). \\[\\begin{align*} \\Pr(\\mathbf{z}) &amp;= \\Pr(z_T, z_{T-1}, z_{T-2}, \\ldots, z_1) \\color{white}{\\Pr(z_{T-1}, z_{T-2},\\ldots, z_1) \\Pr(z_{T-2}, \\ldots, z_1)}\\\\ &amp;= \\Pr(z_T | z_{T-1}, z_{T-2},\\ldots, z_1) \\Pr(z_{T-1}, z_{T-2},\\ldots, z_1) \\\\ &amp;= \\Pr(z_T | z_{T-1}) \\Pr(z_{T-1}, z_{T-2},\\ldots, z_1) \\\\ &amp;= \\Pr(z_T | z_{T-1}) \\Pr(z_{T-1} | z_{T-2}, \\ldots, z_1) \\Pr(z_{T-2}, \\ldots, z_1)\\\\ &amp;= \\Pr(z_T | z_{T-1}) \\Pr(z_{T-1} | z_{T-2}) \\Pr(z_{T-2}, \\ldots, z_1)\\\\ &amp;= \\ldots \\\\ &amp;= \\Pr(z_T | z_{T-1}) \\Pr(z_{T-1} | z_{T-2}) \\ldots \\Pr(z_{2} | z_{1}) \\Pr(z_{1})\\\\ &amp;= \\Pr(z_{1}) \\prod_{t=2}^T{\\Pr(z_{t} | z_{t-1})}\\\\ &amp;= \\Pr(z_{1}) \\prod_{t=2}^T{\\gamma_{z_{t-1},z_{t}}}\\\\ \\end{align*}\\] We recognize the gammas we defined earlier. The transition probabilities. 3.8 Example Let’s assume an animal is alive, alive then dies. I realise these calculations are a bit difficult to follow. Let’s take an example. We have \\(\\mathbf{z} = (1, 1, 2)\\). What is the contribution of this animal to the likelihood? We have \\(\\mathbf{z} = (1, 1, 2)\\). What is the contribution of this animal to the likelihood? Let’s apply the formula we have just derived. \\[\\begin{align*} \\Pr(\\mathbf{z} = (1, 1, 2)) &amp;= \\Pr(z_1 = 1) \\; \\gamma_{z_{1} = 1, z_{2} = 1} \\; \\gamma_{z_{2} = 1, z_{3} = 2}\\\\ &amp;= 1 \\; \\phi \\; (1 - \\phi). \\end{align*}\\] The prob of having the sequence alive, alive and dead is The prob of being alive first, the to stay alive, then to die. The prob of being alive at first occasion being 1, we have that the contribution of this individual to the likelihood is phi times 1 - phi. Remember: \\[\\begin{align*} \\mathbf{\\Gamma} = \\left(\\begin{array}{cc} \\gamma_{1,1} &amp; \\gamma_{1,2}\\\\ \\gamma_{2,1} &amp; \\gamma_{2,2} \\end{array}\\right) = \\left(\\begin{array}{cc} \\phi &amp; 1 - \\phi\\\\ 0 &amp; 1 \\end{array}\\right) \\end{align*}\\] 3.9 Our model \\[\\begin{align*} z_1 &amp;\\sim \\text{Multinomial}(1, \\delta) &amp;\\text{[likelihood, }t = 1 \\text{]}\\\\ \\color{white}{z_t | z_{t-1}} &amp; \\color{white}{\\sim} \\color{white}{\\text{Multinomial}(1, \\gamma_{z_{t-1},z_{t}})} &amp; \\color{white}{\\text{[likelihood, }t &gt; 1 \\text{]}}\\\\ \\color{white}{\\phi} &amp; \\color{white}{\\sim} \\color{white}{\\text{Beta}(1, 1)} &amp; \\color{white}{\\text{[prior for }\\phi \\text{]}} \\\\ \\end{align*}\\] OK let’s wrap it up. Our model so far is that one. Initial state is multinomial with one trial, and probability delta. That is you have a dice with two faces, a coin, and you have some prob to be alive, and 1 - that prob to be dead. + Of course, it you want your Markov chain to start, you’d better say it’s alive so that delta is just (1,0). 3.10 Our model \\[\\begin{align*} z_1 &amp;\\sim \\text{Multinomial}(1, \\delta) &amp;\\text{[likelihood, }t = 1 \\text{]}\\\\ \\color{white}{z_t | z_{t-1}} &amp; \\color{white}{\\sim} \\color{white}{\\text{Multinomial}(1, \\gamma_{z_{t-1},z_{t}})} &amp; \\color{white}{\\text{[likelihood, }t &gt; 1 \\text{]}}\\\\ \\phi &amp;\\sim \\text{Beta}(1, 1) &amp;\\text{[prior for }\\phi \\text{]} \\\\ \\end{align*}\\] We also need a prior on survival. As usual we take a uniform distribution between 0 and 1, or a beta with parameters 1 and 1. \\[\\begin{align*} z_1 &amp;\\sim \\text{Multinomial}(1, \\delta) &amp;\\text{[likelihood, }t = 1 \\text{]}\\\\ z_t | z_{t-1} &amp;\\sim \\text{Multinomial}(1, \\gamma_{z_{t-1},z_{t}}) &amp;\\text{[likelihood, }t &gt; 1 \\text{]}\\\\ \\phi &amp;\\sim \\text{Beta}(1, 1) &amp;\\text{[prior for }\\phi \\text{]} \\\\ \\end{align*}\\] Now the main part is the dynamic of the states. Our state at t depends only on your state at t-1, and it is a multinomial random variable, with one trial. And the probabilities are given by the rows of the transition matrix. \\[\\begin{align*} z_1 &amp;\\sim \\text{Multinomial}(1, \\delta) &amp;\\text{[likelihood, }t = 1 \\text{]}\\\\ z_t | z_{t-1} &amp;\\sim \\text{Multinomial}(1, \\gamma_{z_{t-1},z_{t}}) &amp;\\text{[likelihood, }t &gt; 1 \\text{]}\\\\ \\phi &amp;\\sim \\text{Beta}(1, 1) &amp;\\text{[prior for }\\phi \\text{]} \\\\ \\end{align*}\\] \\[\\begin{align*} \\mathbf{\\Gamma} = \\left(\\begin{array}{cc} \\color{blue}{\\phi} &amp; \\color{blue}{1 - \\phi}\\\\ 0 &amp; 1 \\end{array}\\right) \\end{align*}\\] \\[\\color{blue}{\\gamma_{z_{t-1} = 1,z_{t}} = (\\phi, 1-\\phi)}\\] If z at t-1 is alive, it is the first row, that is phi and 1-phi. \\[\\begin{align*} z_1 &amp;\\sim \\text{Multinomial}(1, \\delta) &amp;\\text{[likelihood, }t = 1 \\text{]}\\\\ z_t | z_{t-1} &amp;\\sim \\text{Multinomial}(1, \\gamma_{z_{t-1},z_{t}}) &amp;\\text{[likelihood, }t &gt; 1 \\text{]}\\\\ \\phi &amp;\\sim \\text{Beta}(1, 1) &amp;\\text{[prior for }\\phi \\text{]} \\\\ \\end{align*}\\] \\[\\begin{align*} \\mathbf{\\Gamma} = \\left(\\begin{array}{cc} \\phi &amp; 1 - \\phi\\\\ \\color{blue}{0} &amp; \\color{blue}{1} \\end{array}\\right) \\end{align*}\\] \\[\\color{blue}{\\gamma_{z_{t-1} = 2,z_{t}} = (0, 1)}\\] Otherwise, if z at t-1 is dead that is 2, then it is the second row of gamma, 0 and 1. If dead you remain dead. 3.11 Nimble implementation In Nimble, we will use the categorical distribution dcat(). The categorical distribution is a multinomial distribution with a single draw. nimble::rcat(n = 20, prob = c(0.1, 0.3, 0.6)) ## [1] 2 3 2 1 2 2 1 3 3 3 3 2 2 2 1 3 2 1 1 3 nimble::rcat(n = 20, prob = c(0.1, 0.1, 0.4, 0.2, 0.2)) ## [1] 3 1 3 2 5 2 3 4 1 3 2 4 1 3 5 1 3 3 3 3 https://en.wikipedia.org/wiki/Categorical_distribution The categorical distribution is the generalization of the Bernoulli distribution for a categorical random variable, i.e. for a discrete variable with more than two possible outcomes, such as the roll of a dice. On the other hand, the categorical distribution is a special case of the multinomial distribution, in that it gives the probabilities of potential outcomes of a single drawing rather than multiple drawings. 3.12 Nimble code markov.survival &lt;- nimbleCode({ phi ~ dunif(0, 1) # prior gamma[1,1] &lt;- phi # Pr(alive t -&gt; alive t+1) gamma[1,2] &lt;- 1 - phi # Pr(alive t -&gt; dead t+1) gamma[2,1] &lt;- 0 # Pr(dead t -&gt; alive t+1) gamma[2,2] &lt;- 1 # Pr(dead t -&gt; dead t+1) delta[1] &lt;- 1 # Pr(alive t = 1) = 1 delta[2] &lt;- 0 # Pr(dead t = 1) = 0 # likelihood for (i in 1:N){ z[i,1] ~ dcat(delta[1:2]) for (j in 2:T){ z[i,j] ~ dcat(gamma[z[i,j-1], 1:2]) } }}) markov.survival &lt;- nimbleCode({ phi ~ dunif(0, 1) # prior #&lt;&lt; gamma[1,1] &lt;- phi # Pr(alive t -&gt; alive t+1) gamma[1,2] &lt;- 1 - phi # Pr(alive t -&gt; dead t+1) gamma[2,1] &lt;- 0 # Pr(dead t -&gt; alive t+1) gamma[2,2] &lt;- 1 # Pr(dead t -&gt; dead t+1) delta[1] &lt;- 1 # Pr(alive t = 1) = 1 delta[2] &lt;- 0 # Pr(dead t = 1) = 0 # likelihood for (i in 1:N){ z[i,1] ~ dcat(delta[1:2]) for (j in 2:T){ z[i,j] ~ dcat(gamma[z[i,j-1], 1:2]) } }}) markov.survival &lt;- nimbleCode({ phi ~ dunif(0, 1) # prior gamma[1,1] &lt;- phi # Pr(alive t -&gt; alive t+1) #&lt;&lt; gamma[1,2] &lt;- 1 - phi # Pr(alive t -&gt; dead t+1) #&lt;&lt; gamma[2,1] &lt;- 0 # Pr(dead t -&gt; alive t+1) #&lt;&lt; gamma[2,2] &lt;- 1 # Pr(dead t -&gt; dead t+1) #&lt;&lt; delta[1] &lt;- 1 # Pr(alive t = 1) = 1 delta[2] &lt;- 0 # Pr(dead t = 1) = 0 # likelihood for (i in 1:N){ z[i,1] ~ dcat(delta[1:2]) for (j in 2:T){ z[i,j] ~ dcat(gamma[z[i,j-1], 1:2]) } }}) ] markov.survival &lt;- nimbleCode({ phi ~ dunif(0, 1) # prior gamma[1,1] &lt;- phi # Pr(alive t -&gt; alive t+1) gamma[1,2] &lt;- 1 - phi # Pr(alive t -&gt; dead t+1) gamma[2,1] &lt;- 0 # Pr(dead t -&gt; alive t+1) gamma[2,2] &lt;- 1 # Pr(dead t -&gt; dead t+1) delta[1] &lt;- 1 # Pr(alive t = 1) = 1 #&lt;&lt; delta[2] &lt;- 0 # Pr(dead t = 1) = 0 #&lt;&lt; # likelihood for (i in 1:N){ z[i,1] ~ dcat(delta[1:2]) for (j in 2:T){ z[i,j] ~ dcat(gamma[z[i,j-1], 1:2]) } }}) markov.survival &lt;- nimbleCode({ phi ~ dunif(0, 1) # prior gamma[1,1] &lt;- phi # Pr(alive t -&gt; alive t+1) gamma[1,2] &lt;- 1 - phi # Pr(alive t -&gt; dead t+1) gamma[2,1] &lt;- 0 # Pr(dead t -&gt; alive t+1) gamma[2,2] &lt;- 1 # Pr(dead t -&gt; dead t+1) delta[1] &lt;- 1 # Pr(alive t = 1) = 1 delta[2] &lt;- 0 # Pr(dead t = 1) = 0 # likelihood for (i in 1:N){ #&lt;&lt; z[i,1] ~ dcat(delta[1:2]) for (j in 2:T){ z[i,j] ~ dcat(gamma[z[i,j-1], 1:2]) } } #&lt;&lt; }) markov.survival &lt;- nimbleCode({ phi ~ dunif(0, 1) # prior gamma[1,1] &lt;- phi # Pr(alive t -&gt; alive t+1) gamma[1,2] &lt;- 1 - phi # Pr(alive t -&gt; dead t+1) gamma[2,1] &lt;- 0 # Pr(dead t -&gt; alive t+1) gamma[2,2] &lt;- 1 # Pr(dead t -&gt; dead t+1) delta[1] &lt;- 1 # Pr(alive t = 1) = 1 delta[2] &lt;- 0 # Pr(dead t = 1) = 0 # likelihood for (i in 1:N){ z[i,1] ~ dcat(delta[1:2]) #&lt;&lt; for (j in 2:T){ z[i,j] ~ dcat(gamma[z[i,j-1], 1:2]) } }}) markov.survival &lt;- nimbleCode({ phi ~ dunif(0, 1) # prior gamma[1,1] &lt;- phi # Pr(alive t -&gt; alive t+1) gamma[1,2] &lt;- 1 - phi # Pr(alive t -&gt; dead t+1) gamma[2,1] &lt;- 0 # Pr(dead t -&gt; alive t+1) gamma[2,2] &lt;- 1 # Pr(dead t -&gt; dead t+1) delta[1] &lt;- 1 # Pr(alive t = 1) = 1 delta[2] &lt;- 0 # Pr(dead t = 1) = 0 # likelihood for (i in 1:N){ z[i,1] ~ dcat(delta[1:2]) for (j in 2:T){ #&lt;&lt; z[i,j] ~ dcat(gamma[z[i,j-1], 1:2]) } #&lt;&lt; }}) markov.survival &lt;- nimbleCode({ phi ~ dunif(0, 1) # prior gamma[1,1] &lt;- phi # Pr(alive t -&gt; alive t+1) gamma[1,2] &lt;- 1 - phi # Pr(alive t -&gt; dead t+1) gamma[2,1] &lt;- 0 # Pr(dead t -&gt; alive t+1) gamma[2,2] &lt;- 1 # Pr(dead t -&gt; dead t+1) delta[1] &lt;- 1 # Pr(alive t = 1) = 1 delta[2] &lt;- 0 # Pr(dead t = 1) = 0 # likelihood for (i in 1:N){ z[i,1] ~ dcat(delta[1:2]) for (j in 2:T){ z[i,j] ~ dcat(gamma[z[i,j-1], 1:2]) #&lt;&lt; } }}) 3.13 Note Vector \\(\\delta\\) is used as a placeholder for more complex models to come in Class 7. Here, you could write z[i,1] &lt;- 1. 3.14 Nimble awesomness You should be able to define vectors and matrices like you do in R. markov.survival &lt;- nimbleCode({ phi ~ dunif(0, 1) # prior gamma[1:2,1:2] &lt;- matrix( c(phi, 0, 1 - phi, 1), nrow = 2) #&lt;&lt; delta[1:2] &lt;- c(1, 0) #&lt;&lt; # likelihood for (i in 1:N){ z[i,1] ~ dcat(delta[1:2]) for (j in 2:T){ z[i,j] ~ dcat(gamma[z[i,j-1], 1:2]) } }}) 3.15 Converting to Nimble from Jags, OpenBUGS or WinBUGS Main difference is that Nimble does not guess. We need to specify dimensions of vectors and matrices. You cannot write x[] or x[i,]. Just provide index ranges x[1:n] or x[i,1:m]. More tips here. 3.16 Constants and data my.constants &lt;- list(N = 57, T = 5) my.constants ## $N ## [1] 57 ## ## $T ## [1] 5 my.data &lt;- list(z = z) 3.17 Initial values initial.values &lt;- function() list(phi = runif(1,0,1)) initial.values() ## $phi ## [1] 0.5749 3.18 Parameters to monitor parameters.to.save &lt;- c(&quot;phi&quot;) parameters.to.save ## [1] &quot;phi&quot; 3.19 MCMC details n.iter &lt;- 5000 n.burnin &lt;- 1000 n.chains &lt;- 2 3.20 Run Nimble mcmc.output &lt;- nimbleMCMC(code = markov.survival, constants = my.constants, data = my.data, inits = initial.values, monitors = parameters.to.save, niter = n.iter, nburnin = n.burnin, nchains = n.chains) ## |-------------|-------------|-------------|-------------| ## |-------------------------------------------------------| ## |-------------|-------------|-------------|-------------| ## |-------------------------------------------------------| 3.21 Posterior distribution of survival library(MCMCvis) MCMCsummary(mcmc.output, round = 2) ## mean sd 2.5% 50% 97.5% Rhat n.eff ## phi 0.81 0.03 0.74 0.81 0.86 1.01 1979 Posterior mean and median are close to \\(0.8\\). Cool! The data was simulated, with (true) survival \\(\\phi = 0.8\\). 3.22 Unfortunately, this is the data we wish we had. 3.23 In real life Animals cannot be monitored exhaustively, like humans in a medical trial. Animals are captured, marked or identified then released alive. Then, these animals may be detected again, or go undetected — capture-recapture Whenever animals go undetected, it might be that they were alive but missed, or because they were dead and therefore could not be detected — imperfect detection. https://www.youtube.com/embed/tyX79mPm2xY Whenever animals go undetected, it might be that they were alive but missed, or because they were dead and therefore could not be detected — imperfect detection. The Markov process for survival is only partially observed — hidden Markov models. 3.24 The truth is in \\(z\\) id winter 1 winter 2 winter 3 winter 4 winter 5 1 1 2 2 2 2 2 1 1 1 1 1 3 1 2 2 2 2 4 1 1 1 1 1 5 1 2 2 2 2 6 1 1 1 1 1 7 1 1 1 1 2 8 1 1 1 1 1 9 1 1 1 1 1 10 1 1 2 2 2 11 1 1 1 1 1 12 1 1 1 1 1 13 1 1 2 2 2 14 1 1 1 1 1 15 1 1 1 1 1 16 1 1 1 1 1 17 1 1 2 2 2 18 1 1 1 1 1 19 1 1 1 2 2 20 1 1 1 1 1 21 1 2 2 2 2 22 1 1 2 2 2 23 1 1 2 2 2 24 1 1 1 1 2 25 1 1 1 1 1 26 1 1 1 1 2 27 1 1 1 1 1 28 1 2 2 2 2 29 1 1 2 2 2 30 1 1 1 1 1 31 1 1 2 2 2 32 1 1 1 2 2 33 1 1 1 1 1 34 1 1 1 2 2 35 1 1 1 2 2 36 1 1 2 2 2 37 1 2 2 2 2 38 1 1 2 2 2 39 1 1 1 1 1 40 1 1 2 2 2 41 1 2 2 2 2 42 1 1 1 1 2 43 1 1 2 2 2 44 1 1 1 1 1 45 1 1 1 2 2 46 1 2 2 2 2 47 1 2 2 2 2 48 1 1 2 2 2 49 1 1 1 1 1 50 1 1 1 2 2 51 1 1 1 1 1 52 1 1 1 1 1 53 1 1 1 1 1 54 1 1 1 1 2 55 1 1 1 1 1 56 1 1 1 1 1 57 1 1 1 1 1 Unfortunately, we have only partial access to \\(z\\). We do observe \\(y\\) the detections and non-detections. How are \\(z\\) and \\(y\\) connected? 3.25 Dead animals go undetected When an animal is dead i.e. \\(z = 2\\), it cannot be detected, therefore \\(y = 0\\). id winter 1 winter 2 winter 3 winter 4 winter 5 1 1 0 0 0 0 2 1 1 1 1 1 3 1 0 0 0 0 4 1 1 1 1 1 5 1 0 0 0 0 6 1 1 1 1 1 7 1 1 1 1 0 8 1 1 1 1 1 9 1 1 1 1 1 10 1 1 0 0 0 11 1 1 1 1 1 12 1 1 1 1 1 13 1 1 0 0 0 14 1 1 1 1 1 15 1 1 1 1 1 16 1 1 1 1 1 17 1 1 0 0 0 18 1 1 1 1 1 19 1 1 1 0 0 20 1 1 1 1 1 21 1 0 0 0 0 22 1 1 0 0 0 23 1 1 0 0 0 24 1 1 1 1 0 25 1 1 1 1 1 26 1 1 1 1 0 27 1 1 1 1 1 28 1 0 0 0 0 29 1 1 0 0 0 30 1 1 1 1 1 31 1 1 0 0 0 32 1 1 1 0 0 33 1 1 1 1 1 34 1 1 1 0 0 35 1 1 1 0 0 36 1 1 0 0 0 37 1 0 0 0 0 38 1 1 0 0 0 39 1 1 1 1 1 40 1 1 0 0 0 41 1 0 0 0 0 42 1 1 1 1 0 43 1 1 0 0 0 44 1 1 1 1 1 45 1 1 1 0 0 46 1 0 0 0 0 47 1 0 0 0 0 48 1 1 0 0 0 49 1 1 1 1 1 50 1 1 1 0 0 51 1 1 1 1 1 52 1 1 1 1 1 53 1 1 1 1 1 54 1 1 1 1 0 55 1 1 1 1 1 56 1 1 1 1 1 57 1 1 1 1 1 3.26 Alive animals may be detected or not If animal is alive \\(z = 1\\), it is detected \\(y = 1\\) w/ prob \\(p\\) or not \\(y = 0\\) w/ prob \\(1-p\\). Before first detection, we know nothing, and we proceed conditional on it. id winter 1 winter 2 winter 3 winter 4 winter 5 1 NA NA 1 1 1 2 1 0 0 1 1 3 NA NA 1 1 1 4 NA NA 1 1 0 5 1 0 0 1 1 6 1 1 0 1 0 7 1 0 0 0 0 8 1 1 1 1 1 9 1 0 1 0 1 10 1 1 0 0 0 11 1 1 0 1 1 12 1 0 0 1 1 13 NA 1 0 1 0 14 NA 1 0 0 0 15 NA 1 0 1 1 16 1 1 0 0 0 17 1 1 1 0 1 18 1 1 0 0 0 19 1 1 0 1 0 20 1 1 1 0 1 21 NA 1 1 1 0 22 NA 1 1 1 0 23 1 0 0 0 0 24 1 0 0 0 0 25 1 0 0 1 1 26 1 1 0 0 0 27 1 0 1 0 0 28 1 0 0 1 1 29 NA 1 1 0 0 30 NA 1 1 0 0 31 NA 1 0 0 0 32 NA 1 0 0 0 33 1 1 0 1 0 34 1 1 0 0 0 35 1 1 0 0 0 36 NA 1 0 1 1 37 NA 1 1 0 0 38 1 0 0 0 0 39 NA 1 0 0 0 40 1 1 0 1 0 41 1 1 0 0 0 42 1 0 0 0 1 43 NA 1 0 1 1 44 NA NA 1 0 0 45 NA 1 1 0 0 46 1 1 1 1 1 47 1 1 1 1 1 48 NA NA 1 0 1 Compare with previous table Some 1s have become 0s. This table \\(y\\) is what we observe in real life. To make the connection between the observations, the y, and the true states, the z We need to describe how observations are made from the states 3.27 Observation matrix The observation probabilities can be packed in an observation matrix \\(\\mathbf{\\Omega}\\). In rows: the states alive \\(z = 1\\) and dead \\(z = 2\\). In columns: the observations non-detected \\(y = 1\\) and detected \\(y = 2\\) (previously coded 0 and 1 respectively). \\[\\begin{align*} \\mathbf{\\Omega} = \\left(\\begin{array}{cc} \\omega_{1,1} &amp; \\omega_{1,2}\\\\ \\omega_{2,1} &amp; \\omega_{2,2} \\end{array}\\right) = \\left(\\begin{array}{cc} 1 - p &amp; p\\\\ 1 &amp; 0 \\end{array}\\right) \\end{align*}\\] Observation matrix: \\[ \\begin{matrix} &amp; \\\\ \\mathbf{\\Omega} = \\left ( \\vphantom{ \\begin{matrix} 12 \\\\ 12 \\end{matrix} } \\right . \\end{matrix} \\hspace{-1.2em} \\begin{matrix} y_t=1 &amp; y_t=2 \\\\ \\hdashline 1 - p &amp; p\\\\ 1 &amp; 0\\\\ \\end{matrix} \\hspace{-0.2em} \\begin{matrix} &amp; \\\\ \\left . \\vphantom{ \\begin{matrix} 12 \\\\ 12 \\end{matrix} } \\right ) \\begin{matrix} z_{t}=A \\\\ z_{t}=D \\end{matrix} \\end{matrix} \\] 3.28 Markov model States \\(z\\) are in gray. Remember the graphical repres of a Markov model. 3.29 Hidden Markov model States \\(z\\) are in gray. Observations \\(y\\) are in white. A hidden Markov model is just two time series. One for the states with a Markovian property. The other of observations generated from the states. Run in parallel. 3.30 Hidden Markov model for survival For states (in gray), \\(z = 1\\) is alive, \\(z = 2\\) is dead. For observations (in white), \\(y = 1\\) is non-detected, \\(y = 2\\) is detected Now add the states alive and dead, 1 and 2s. The observations, non-detected and detected, 1 and 2s. And the parameters, phi for transition from 1 to 1. And p for prob of y being 2 detected given z is 1 alive. 3.31 HMM likelihood Using the formula of total probability, then the likelihood of a Markov chain: \\[\\begin{align*} \\Pr(\\mathbf{y}) &amp;= \\Pr(y_1, y_{2}, \\ldots, y_T)\\\\ &amp;= \\sum_{z_1} \\cdots \\sum_{z_T} \\Pr(y_1, y_{2}, \\ldots, y_T | z_1, z_{2}, \\ldots, z_T) \\Pr(z_1, z_{2}, \\ldots, z_T)\\\\ &amp;= \\sum_{z_1} \\cdots \\sum_{z_T} \\left(\\prod_{t=1}^T{\\omega_{z_{t}, y_t}}\\right) \\left(\\Pr(z_{1}) \\prod_{t=2}^T{\\gamma_{z_{t-1},z_{t}}}\\right)\\\\ \\end{align*}\\] What is the likelihood of a HMM. The thing here is that we don’t know the states. So we have to go through all possibilities, and sum over the possible states. Hence these sums here. Then this term is the likelihood of a Markov chain, we saw that before. And this component are the elements of the observation matrix. The likelihood has a matrix formulation that can be useful. It is delta, initial states, then observation, then transitions, and so on. There is a vector of ones at the end to get the sum all the terms. It has a matrix formulation: \\[\\begin{align*} \\Pr(\\mathbf{y}) &amp;= \\mathbf{\\delta} \\; \\mathbf{\\Omega} \\; \\mathbf{\\Gamma} \\cdots \\mathbf{\\Omega} \\; \\mathbf{\\Gamma} \\; \\mathbf{\\Omega} \\; \\mathbb{1} \\end{align*}\\] 3.32 Example Let assume an animal is detected, then missed. We have \\(\\mathbf{y} = (2, 1)\\). What is the contribution of this animal to the likelihood? \\[\\begin{align*} \\Pr(\\mathbf{y} = (2, 1)) &amp;= \\sum_{z_1 = 1}^2 \\; \\sum_{z_2 = 1}^2 w_{z_1, y_1 = 2} w_{z_2, y_2 = 1} \\Pr(z_1) \\gamma_{z_1, z_2} \\color{white}{\\Pr(z_{T-1}, z_{T-2},\\ldots, z_1, z_1, z_1, z_1)}\\\\ \\end{align*}\\] 3.33 Example Let assume an animal is detected, then missed. We have \\(\\mathbf{y} = (2, 1)\\). What is the contribution of this animal to the likelihood? \\[\\begin{align*} \\Pr(\\mathbf{y} = (2, 1)) &amp;= \\sum_{z_1 = 1}^2 \\; \\sum_{z_2 = 1}^2 w_{z_1, y_1 = 2} w_{z_2, y_2 = 1} \\Pr(z_1) \\gamma_{z_1, z_2} \\color{white}{\\Pr(z_{T-1}, z_{T-2},\\ldots, z_1, z_1, z_1, z_1)}\\\\ &amp;= \\sum_{z_1 = 1}^2 \\left( w_{z_1, y_1 = 2} w_{z_2 = 1, y_2 = 1} \\Pr(z_1) \\gamma_{z_1, z_2 = 1} + w_{z_1, y_1 = 2} w_{z_2 = 2, y_2 = 1} \\Pr(z_1) \\gamma_{z_1, z_2 = 2} \\right) \\\\ \\end{align*}\\] Let assume an animal is detected, then missed. We have \\(\\mathbf{y} = (2, 1)\\). What is the contribution of this animal to the likelihood? \\[\\begin{align*} \\Pr(\\mathbf{y} = (2, 1)) &amp;= \\sum_{z_1 = 1}^2 \\; \\sum_{z_2 = 1}^2 w_{z_1, y_1 = 2} w_{z_2, y_2 = 1} \\Pr(z_1) \\gamma_{z_1, z_2} \\color{white}{\\Pr(z_{T-1}, z_{T-2},\\ldots, z_1, z_1, z_1, z_1)}\\\\ &amp;= \\sum_{z_1 = 1}^2 \\left( w_{z_1, y_1 = 2} w_{z_2 = 1, y_2 = 1} \\Pr(z_1) \\gamma_{z_1, z_2 = 1} + w_{z_1, y_1 = 2} w_{z_2 = 2, y_2 = 1} \\Pr(z_1) \\gamma_{z_1, z_2 = 2} \\right) \\\\ &amp;= w_{z_1 = 1, y_1 = 2} w_{z_2 = 1, y_2 = 1}\\delta_1 \\gamma_{z_1 = 1, z_2 = 1} + w_{z_1 = 1, y_1 = 2} w_{z_2 = 2, y_2 = 1} \\delta_1 \\gamma_{z_1 = 1, z_2 = 2} \\end{align*}\\] Note: \\(\\Pr(z_1 = 1) = \\delta_1 = 1\\) and \\(\\Pr(z_1 = 2) = 0\\). Let assume an animal is detected, then missed. We have \\(\\mathbf{y} = (2, 1)\\). What is the contribution of this animal to the likelihood? \\[\\begin{align*} \\Pr(\\mathbf{y} = (2, 1)) &amp;= \\sum_{z_1 = 1}^2 \\; \\sum_{z_2 = 1}^2 w_{z_1, y_1 = 2} w_{z_2, y_2 = 1} \\Pr(z_1) \\gamma_{z_1, z_2} \\color{white}{\\Pr(z_{T-1}, z_{T-2},\\ldots, z_1, z_1, z_1, z_1)}\\\\ &amp;= \\sum_{z_1 = 1}^2 \\left( w_{z_1, y_1 = 2} w_{z_2 = 1, y_2 = 1} \\Pr(z_1) \\gamma_{z_1, z_2 = 1} + w_{z_1, y_1 = 2} w_{z_2 = 2, y_2 = 1} \\Pr(z_1) \\gamma_{z_1, z_2 = 2} \\right) \\\\ &amp;= w_{z_1 = 1, y_1 = 2} w_{z_2 = 1, y_2 = 1} \\delta_1 \\gamma_{z_1 = 1, z_2 = 1} + w_{z_1 = 1, y_1 = 2} w_{z_2 = 2, y_2 = 1} \\delta_1 \\gamma_{z_1 = 1, z_2 = 2}\\\\ &amp;= (1 - p) \\phi + (1-\\phi) \\end{align*}\\] Note: \\(w_{z_1 = 1, y_1 = 2} = \\Pr(y_1 = 2 | z_1 = 1) = 1\\) because we condition on first capture. 3.34 Estimating the latent states \\(z\\) or not? Next question is, shall we estimate the latent states or not? In previous example, we got rid of the states, so that likelihood was a function of \\(\\phi\\) and \\(p\\) only. This is the function we would maximize in a Frequentist approach. The Bayesian approach with MCMC methods allows treating the latent states as if they were parameters, and to be estimated as such. Infering the latent states \\(z\\) can be useful to estimate prevalence, e.g. in animal epidemiology with prevalence of a disease, in evolutionary ecology with sex ratio or in conservation biology with prevalence of hybrids. Estimating the latent states is costly though, and if not required, marginalisation may speed up computations. Actually, you can estimate the states afterwards (Viterbi). More about so-called marginalisation in Yackulic et al. (2020). The neat thing with Nimble is that it provides marginalised models through nimbleEcology, we’ll get back to it in Class 8. 3.35 Our model \\[\\begin{align*} z_{\\text{first}} &amp;\\sim \\text{Multinomial}(1, \\delta) &amp;\\text{[likelihood]}\\\\ z_t | z_{t-1} &amp;\\sim \\text{Multinomial}(1, \\gamma_{z_{t-1},z_{t}}) &amp;\\text{[likelihood]}\\\\ y_t | z_{t} &amp;\\sim \\text{Multinomial}(1, \\omega_{z_{t}}) &amp;\\text{[likelihood]}\\\\ \\phi &amp;\\sim \\text{Beta}(1, 1) &amp;\\text{[prior for }\\phi \\text{]} \\\\ p &amp;\\sim \\text{Beta}(1, 1) &amp;\\text{[prior for }p \\text{]} \\\\ \\end{align*}\\] Now our model has an observation layer for the ys, conditional on the z. And we need a prior for the detection probability. 3.36 Nimble implementation How to implement this model in Nimble? 3.37 Priors hmm.survival &lt;- nimbleCode({ phi ~ dunif(0, 1) # prior survival p ~ dunif(0, 1) # prior detection ... 3.38 HMM ingredients ... # parameters gamma[1,1] &lt;- phi # Pr(alive t -&gt; alive t+1) gamma[1,2] &lt;- 1 - phi # Pr(alive t -&gt; dead t+1) gamma[2,1] &lt;- 0 # Pr(dead t -&gt; alive t+1) gamma[2,2] &lt;- 1 # Pr(dead t -&gt; dead t+1) delta[1] &lt;- 1 # Pr(alive t = 1) = 1 delta[2] &lt;- 0 # Pr(dead t = 1) = 0 omega[1,1] &lt;- 1 - p # Pr(alive t -&gt; non-detected t) omega[1,2] &lt;- p # Pr(alive t -&gt; detected t) omega[2,1] &lt;- 1 # Pr(dead t -&gt; non-detected t) omega[2,2] &lt;- 0 # Pr(dead t -&gt; detected t) ... 3.39 Likelihood ... # likelihood for (i in 1:N){ z[i,first[i]] ~ dcat(delta[1:2]) for (j in (first[i]+1):T){ z[i,j] ~ dcat(gamma[z[i,j-1], 1:2]) y[i,j] ~ dcat(omega[z[i,j], 1:2]) } } }) 3.40 Constants first &lt;- apply(y, 1, function(x) min(which(x !=0))) my.constants &lt;- list(N = nrow(y), T = 5, first = first) my.constants ## $N ## [1] 48 ## ## $T ## [1] 5 ## ## $first ## [1] 3 1 3 3 1 1 1 1 1 1 1 1 2 2 2 1 1 1 1 1 2 2 1 1 1 1 1 1 ## [29] 2 2 2 2 1 1 1 2 2 1 2 1 1 1 2 3 2 1 1 3 3.41 Data The data are made of 0s for non-detections and 1s for detections. To use the categorical distribution, we need to code 1, 2, etc. Value 0 is not accepted. Add 1 to get the correct format \\(y=1\\) for non-detection and \\(y = 2\\) for detection. my.data &lt;- list(y = y + 1) 3.42 Initial values zinits &lt;- y + 1 # non-detection -&gt; alive zinits[zinits == 2] &lt;- 1 # dead -&gt; alive initial.values &lt;- function() list(phi = runif(1,0,1), p = runif(1,0,1), z = zinits) 3.43 Parameters to monitor parameters.to.save &lt;- c(&quot;phi&quot;, &quot;p&quot;) parameters.to.save ## [1] &quot;phi&quot; &quot;p&quot; 3.44 MCMC details n.iter &lt;- 5000 n.burnin &lt;- 1000 n.chains &lt;- 2 3.45 Run Nimble mcmc.output &lt;- nimbleMCMC(code = hmm.survival, constants = my.constants, data = my.data, inits = initial.values, monitors = parameters.to.save, niter = n.iter, nburnin = n.burnin, nchains = n.chains) ## |-------------|-------------|-------------|-------------| ## |-------------------------------------------------------| ## |-------------|-------------|-------------|-------------| ## |-------------------------------------------------------| mcmc.output &lt;- nimbleMCMC(code = hmm.survival, constants = my.constants, data = my.data, inits = initial.values, monitors = parameters.to.save, niter = n.iter, nburnin = n.burnin, nchains = n.chains, progressBar = FALSE) 3.46 Posterior distribution of survival library(MCMCvis) MCMCsummary(mcmc.output, round = 2) ## mean sd 2.5% 50% 97.5% Rhat n.eff ## p 0.56 0.06 0.43 0.56 0.67 1.11 360 ## phi 0.88 0.04 0.80 0.88 0.97 1.14 327 The data is simulated, with true survival \\(\\phi = 0.8\\) and detection \\(p = 0.6\\). 3.47 Further reading Zucchini, MacDonald and Langrock (2016) Hidden Markov Models for Time Series: An Introduction Using R (2nd ed). Chapman and Hall/CRC. McClintock, B.T., Langrock, R., Gimenez, O., Cam, E., Borchers, D.L., Glennie, R. and Patterson, T.A. (2020), Uncovering ecological state dynamics with hidden Markov models. Ecology Letters, 23: 1878-1903. Yackulic, C. B. Dodrill, M., Dzul, M., Sanderlin, J. S., and Reid, J. A.. (2020). A need for speed in Bayesian population models: a practical guide to marginalizing and recovering discrete latent states. Ecological Applications 30:e02112. L. R. Rabiner (1989). A tutorial on hidden Markov models and selected applications in speech recognition. Proceedings of the IEEE, 77:257-286. Heller and Pogaru (2021) References "],["survival.html", "Chapter 4 Survival", " Chapter 4 Survival "],["transition.html", "Chapter 5 Transition", " Chapter 5 Transition "],["covariates.html", "Chapter 6 Covariates", " Chapter 6 Covariates "],["uncertainty.html", "Chapter 7 Uncertainty in state assignment", " Chapter 7 Uncertainty in state assignment "],["abundance.html", "Chapter 8 Abundance", " Chapter 8 Abundance "],["hsmm.html", "Chapter 9 Hidden semi-Markov models", " Chapter 9 Hidden semi-Markov models "],["states.html", "Chapter 10 Hidden states", " Chapter 10 Hidden states "],["speed.html", "Chapter 11 Speed up MCMC", " Chapter 11 Speed up MCMC "],["senescence.html", "Chapter 12 Actuarial senescence", " Chapter 12 Actuarial senescence Choquet et al. (2011), Péron et al. (2016) References "],["heterogeneity.html", "Chapter 13 Individual heterogeneity", " Chapter 13 Individual heterogeneity Cubaynes et al. (2010), Gimenez and Choquet (2010), and Turek, Wehrhahn, and Gimenez (2021) References "],["tradeoffs.html", "Chapter 14 Life-history tradeoffs", " Chapter 14 Life-history tradeoffs Morano et al. (2013), Shefferson et al. (2003), and Cruz-Flores et al. (n.d.) References "],["breeding.html", "Chapter 15 Breeding dynamics", " Chapter 15 Breeding dynamics Pradel, Choquet, and Béchet (2012), Desprez et al. (2011), Desprez et al. (2013), and Pacoureau et al. (2019) References "],["rd.html", "Chapter 16 Robust design", " Chapter 16 Robust design Karamanlidis et al. (2015), Santostasi et al. (2016), Gibson et al. (2018), and Rankin et al. (2016) References "],["stopover.html", "Chapter 17 Stopover duration", " Chapter 17 Stopover duration Guérin et al. (2017) References "],["disease.html", "Chapter 18 Disease dynamics", " Chapter 18 Disease dynamics Marescot et al. (2018) and Santoro et al. (2014) References "],["sex.html", "Chapter 19 Sex uncertainty", " Chapter 19 Sex uncertainty Pradel et al. (2008) and Genovart, Pradel, and Oro (2012) References "],["dependence.html", "Chapter 20 Dependence among individuals", " Chapter 20 Dependence among individuals Culina et al. (2013) and Cubaynes et al. (2021) References "],["covariateselection.html", "Chapter 21 Individual and temporal variability", " Chapter 21 Individual and temporal variability Grosbois et al. (2008), Cubaynes et al. (2012), Gimenez et al. (2006), and Bonner, Morgan, and King (2010) References "],["mortalities.html", "Chapter 22 Cause-specific mortalities", " Chapter 22 Cause-specific mortalities Fernández-Chacón et al. (2016) and Ruette et al. (2015) References "],["prevalence.html", "Chapter 23 Prevalence", " Chapter 23 Prevalence (Santostasi et al. 2019) References "],["faq.html", "FAQ", " FAQ Below is the complete list of frequently asked questions (FAQ). Yes, there is only one question here. Personally I do not like FAQs. They often mean surprises, and surprises are not good for software users. Q: Will bookdown have the features X, Y, and Z? A: The short answer is no, but if you have asked yourself three times “do I really need them” and the answer is still “yes,” please feel free to file a feature request to https://github.com/rstudio/bookdown/issues. Users asking for more features often come from the LaTeX world. If that is the case for you, the answer to this question is yes, because Pandoc’s Markdown supports raw LaTeX code. Whenever you feel Markdown cannot do the job for you, you always have the option to apply some raw LaTeX code in your Markdown document. For example, you can create glossaries using the glossaries package, or embed a complicated LaTeX table, as long as you know the LaTeX syntax. However, please keep in mind that the LaTeX content is not portable. It will only work for LaTeX/PDF output, and will be ignored in other types of output. Depending on the request, we may port a few more LaTeX features into bookdown in the future, but our general philosophy is that Markdown should be kept as simple as possible. The most challenging thing in the world is not to learn fancy technologies, but control your own wild heart. "],["references.html", "References", " References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
